{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Phgr6rof5Z7B",
   "metadata": {
    "id": "Phgr6rof5Z7B"
   },
   "source": [
    "\"\"\"NYC Taxi Zone-Based Passenger Demand Predictor\n",
    "Predicts passenger demand by bucket (single/small/medium/large) for each zone\n",
    "Using both KNN and XGBoost models\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eBjUMsSU3hB60Dbrf0mAVbdu",
   "metadata": {
    "id": "eBjUMsSU3hB60Dbrf0mAVbdu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "from google.cloud import bigquery, storage\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CWj-M1Tq5syi",
   "metadata": {
    "id": "CWj-M1Tq5syi"
   },
   "source": [
    "# Configuration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K3luc2fG5tIL",
   "metadata": {
    "id": "K3luc2fG5tIL"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "PROJECT_ID = \"nyctaxi-467111\"\n",
    "SOURCE_DATASET = \"CleanSilver\"\n",
    "OUTPUT_DATASET = \"PostMlGold\"\n",
    "BUCKET_NAME = \"nyc_raw_data_bucket\"\n",
    "MODEL_FOLDER = \"zone_based_models_new\"\n",
    "\n",
    "# Taxi types to process\n",
    "TAXI_TYPES = [\"yellow\", \"green\"]\n",
    "\n",
    "# Minimum trips threshold for zone modeling\n",
    "MIN_TRIPS_THRESHOLD = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GQtDV2R98Cvk",
   "metadata": {
    "id": "GQtDV2R98Cvk"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Forecast parameters\n",
    "FORECAST_DAYS = 15\n",
    "CONFIDENCE_LEVEL = 0.95\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IoNnwIf15zLG",
   "metadata": {
    "id": "IoNnwIf15zLG"
   },
   "outputs": [],
   "source": [
    "\"\"\"# Setup logging\"\"\"\n",
    "\n",
    "class LoggerWrapper:\n",
    "    def __init__(self, name=None):\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        self._logger = logging.getLogger(name or __name__)\n",
    "\n",
    "    def info(self, message):\n",
    "        self._logger.info(message)\n",
    "        print(message)  # Simple print without [INFO] prefix\n",
    "\n",
    "    def error(self, message, exc_info=False):\n",
    "        self._logger.error(message, exc_info=exc_info)\n",
    "        print(f\"ERROR: {message}\")\n",
    "        if exc_info:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "    def warning(self, message):\n",
    "        self._logger.warning(message)\n",
    "        print(f\"WARNING: {message}\")\n",
    "\n",
    "# Create logger instance\n",
    "logger = LoggerWrapper(__name__)\n",
    "\n",
    "\"\"\"# Initialize clients\"\"\"\n",
    "\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R8fQy1qr87aS",
   "metadata": {
    "id": "R8fQy1qr87aS"
   },
   "outputs": [],
   "source": [
    "# logger.addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3h5zNwg684na",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1757025161784,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "3h5zNwg684na",
    "outputId": "3d974acb-2ed5-44b6-8524-e2e011c7a553"
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Your message here\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wuSR7g7O9EmD",
   "metadata": {
    "id": "wuSR7g7O9EmD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24yrpcD850lt",
   "metadata": {
    "id": "24yrpcD850lt"
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WyPfQHk_531r",
   "metadata": {
    "id": "WyPfQHk_531r"
   },
   "outputs": [],
   "source": [
    "def calculate_smape(y_true, y_pred):\n",
    "    \"\"\"Calculate Symmetric Mean Absolute Percentage Error\"\"\"\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    mask = denominator != 0\n",
    "    smape = np.zeros_like(y_true)\n",
    "    smape[mask] = np.abs(y_true[mask] - y_pred[mask]) / denominator[mask]\n",
    "    return 100 * np.mean(smape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Yn7mNZgO5_8U",
   "metadata": {
    "id": "Yn7mNZgO5_8U"
   },
   "outputs": [],
   "source": [
    "def extract_features_from_bigquery(taxi_type, end_date=None):\n",
    "    \"\"\"Extract and prepare features from BigQuery tables.\"\"\"\n",
    "    logger.info(f\"Extracting features for {taxi_type} taxi...\")\n",
    "\n",
    "    if end_date is None:\n",
    "        # Get the latest date from available data\n",
    "        latest_date_query = f\"\"\"\n",
    "        SELECT MAX(DATE(pickup_datetime)) as max_date\n",
    "        FROM (\n",
    "            SELECT {'tpep' if taxi_type == 'yellow' else 'lpep'}_pickup_datetime as pickup_datetime\n",
    "            FROM `{PROJECT_ID}.{SOURCE_DATASET}.{taxi_type}*`\n",
    "        )\n",
    "        \"\"\"\n",
    "        end_date = list(bq_client.query(latest_date_query).result())[0].max_date\n",
    "\n",
    "    pickup_col = 'tpep_pickup_datetime' if taxi_type == 'yellow' else 'lpep_pickup_datetime'\n",
    "\n",
    "    query = f\"\"\"\n",
    "    WITH bounds AS (\n",
    "        SELECT\n",
    "            DATE('2024-01-01') AS min_d,\n",
    "            DATE('{end_date}') AS max_d\n",
    "    ),\n",
    "    zones AS (\n",
    "        SELECT DISTINCT PULocationID AS zone_id\n",
    "        FROM `{PROJECT_ID}.{SOURCE_DATASET}.{taxi_type}*`\n",
    "        WHERE PULocationID IS NOT NULL\n",
    "    ),\n",
    "    grid AS (\n",
    "        SELECT\n",
    "            d,\n",
    "            hr,\n",
    "            zone_id\n",
    "        FROM bounds b,\n",
    "        UNNEST(GENERATE_DATE_ARRAY(b.min_d, b.max_d)) AS d,\n",
    "        UNNEST(GENERATE_ARRAY(0, 23)) AS hr\n",
    "        CROSS JOIN zones\n",
    "    ),\n",
    "    raw_trips AS (\n",
    "        SELECT\n",
    "            DATE({pickup_col}) AS d,\n",
    "            EXTRACT(HOUR FROM {pickup_col}) AS hr,\n",
    "            PULocationID AS zone_id,\n",
    "            CASE\n",
    "                WHEN passenger_count <= 1.5 THEN 'single'\n",
    "                WHEN passenger_count <= 3.5 THEN 'small'\n",
    "                WHEN passenger_count <= 5.5 THEN 'medium'\n",
    "                ELSE 'large'\n",
    "            END AS bucket\n",
    "        FROM `{PROJECT_ID}.{SOURCE_DATASET}.{taxi_type}*`\n",
    "        WHERE DATE({pickup_col}) >= '2024-01-01'\n",
    "          AND DATE({pickup_col}) <= '{end_date}'\n",
    "          AND PULocationID IS NOT NULL\n",
    "    ),\n",
    "    trip_counts AS (\n",
    "        SELECT\n",
    "            d, hr, zone_id,\n",
    "            COUNTIF(bucket = 'single') AS y_single,\n",
    "            COUNTIF(bucket = 'small') AS y_small,\n",
    "            COUNTIF(bucket = 'medium') AS y_medium,\n",
    "            COUNTIF(bucket = 'large') AS y_large\n",
    "        FROM raw_trips\n",
    "        GROUP BY d, hr, zone_id\n",
    "    ),\n",
    "    filled AS (\n",
    "        SELECT\n",
    "            g.d, g.hr, g.zone_id,\n",
    "            COALESCE(c.y_single, 0) AS y_single,\n",
    "            COALESCE(c.y_small, 0) AS y_small,\n",
    "            COALESCE(c.y_medium, 0) AS y_medium,\n",
    "            COALESCE(c.y_large, 0) AS y_large,\n",
    "            COALESCE(c.y_single + c.y_small + c.y_medium + c.y_large, 0) AS total_trips\n",
    "        FROM grid g\n",
    "        LEFT JOIN trip_counts c USING (d, hr, zone_id)\n",
    "    ),\n",
    "    with_features AS (\n",
    "        SELECT\n",
    "            f.*,\n",
    "            EXTRACT(DAYOFWEEK FROM f.d) AS dow,\n",
    "            EXTRACT(MONTH FROM f.d) AS month,\n",
    "            -- Lag features\n",
    "            LAG(y_single, 1) OVER (PARTITION BY zone_id, hr ORDER BY d) AS y_single_lag1,\n",
    "            LAG(y_single, 7) OVER (PARTITION BY zone_id, hr ORDER BY d) AS y_single_lag7,\n",
    "            LAG(y_small, 1) OVER (PARTITION BY zone_id, hr ORDER BY d) AS y_small_lag1,\n",
    "            LAG(y_small, 7) OVER (PARTITION BY zone_id, hr ORDER BY d) AS y_small_lag7,\n",
    "            LAG(y_medium, 1) OVER (PARTITION BY zone_id, hr ORDER BY d) AS y_medium_lag1,\n",
    "            LAG(y_medium, 7) OVER (PARTITION BY zone_id, hr ORDER BY d) AS y_medium_lag7,\n",
    "            LAG(y_large, 1) OVER (PARTITION BY zone_id, hr ORDER BY d) AS y_large_lag1,\n",
    "            LAG(y_large, 7) OVER (PARTITION BY zone_id, hr ORDER BY d) AS y_large_lag7,\n",
    "            LAG(total_trips, 1) OVER (PARTITION BY zone_id, hr ORDER BY d) AS total_trips_lag1,\n",
    "            LAG(total_trips, 7) OVER (PARTITION BY zone_id, hr ORDER BY d) AS total_trips_lag7,\n",
    "            -- Rolling averages\n",
    "            AVG(total_trips) OVER (\n",
    "                PARTITION BY zone_id, hr\n",
    "                ORDER BY d\n",
    "                ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING\n",
    "            ) AS total_trips_7dma,\n",
    "            AVG(total_trips) OVER (\n",
    "                PARTITION BY zone_id, hr\n",
    "                ORDER BY d\n",
    "                ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING\n",
    "            ) AS total_trips_3dma,\n",
    "            -- Zone-level statistics\n",
    "            AVG(total_trips) OVER (PARTITION BY zone_id) AS zone_avg_trips,\n",
    "            STDDEV(total_trips) OVER (PARTITION BY zone_id) AS zone_std_trips\n",
    "        FROM filled f\n",
    "    )\n",
    "    SELECT * FROM with_features\n",
    "    WHERE d >= DATE('2024-01-08')  -- Ensure we have lag features\n",
    "    ORDER BY zone_id, d, hr\n",
    "    \"\"\"\n",
    "\n",
    "    df = bq_client.query(query).to_dataframe()\n",
    "    logger.info(f\"Extracted {len(df):,} rows of feature data\")\n",
    "    print(f\"Extracted {len(df):,} rows of feature data\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EoJcCfNY6Fyr",
   "metadata": {
    "id": "EoJcCfNY6Fyr"
   },
   "outputs": [],
   "source": [
    "def prepare_features(df):\n",
    "    \"\"\"Add cyclical time features and handle missing values.\"\"\"\n",
    "    # Fill NaN values\n",
    "    lag_cols = [c for c in df.columns if '_lag' in c]\n",
    "    df[lag_cols] = df[lag_cols].fillna(0.0)\n",
    "    df['total_trips_7dma'] = df['total_trips_7dma'].fillna(0.0)\n",
    "    df['total_trips_3dma'] = df['total_trips_3dma'].fillna(0.0)\n",
    "    df['zone_avg_trips'] = df['zone_avg_trips'].fillna(0.0)\n",
    "    df['zone_std_trips'] = df['zone_std_trips'].fillna(1.0)\n",
    "\n",
    "    # Convert numeric columns to float\n",
    "    numeric_cols = ['total_trips', 'y_single', 'y_small', 'y_medium', 'y_large',\n",
    "                   'zone_avg_trips', 'zone_std_trips', 'total_trips_3dma', 'total_trips_7dma'] + lag_cols\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype('float64')\n",
    "\n",
    "    # Add cyclical time features\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hr'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hr'] / 24)\n",
    "    df['dow_sin'] = np.sin(2 * np.pi * df['dow'] / 7)\n",
    "    df['dow_cos'] = np.cos(2 * np.pi * df['dow'] / 7)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m4vO1J5k6IHT",
   "metadata": {
    "id": "m4vO1J5k6IHT"
   },
   "outputs": [],
   "source": [
    "def build_preprocessor_for_zone():\n",
    "    \"\"\"Build feature preprocessor for zone-specific models.\"\"\"\n",
    "    # No categorical features since we're modeling per zone\n",
    "    num_features = [\n",
    "        'hr', 'dow', 'month',\n",
    "        'total_trips', 'total_trips_3dma', 'total_trips_7dma',\n",
    "        'zone_avg_trips', 'zone_std_trips',\n",
    "        # All lag features\n",
    "        'y_single_lag1', 'y_single_lag7',\n",
    "        'y_small_lag1', 'y_small_lag7',\n",
    "        'y_medium_lag1', 'y_medium_lag7',\n",
    "        'y_large_lag1', 'y_large_lag7',\n",
    "        'total_trips_lag1', 'total_trips_lag7',\n",
    "        # Cyclical features\n",
    "        'hour_sin', 'hour_cos',\n",
    "        'dow_sin', 'dow_cos',\n",
    "        'month_sin', 'month_cos'\n",
    "    ]\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), num_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return preprocessor, num_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "etr0QGWG6LXs",
   "metadata": {
    "id": "etr0QGWG6LXs"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "def train_zone_models(zone_data, zone_id, test_days=7):\n",
    "    \"\"\"Train both KNN and XGBoost models for a specific zone.\"\"\"\n",
    "    logger.info(f\"Starting training for zone {zone_id}\")\n",
    "\n",
    "    # Split data\n",
    "    zone_data['d'] = pd.to_datetime(zone_data['d'])\n",
    "    cutoff_date = zone_data['d'].max() - timedelta(days=test_days)\n",
    "\n",
    "    logger.info(f\"Zone {zone_id}: Data range from {zone_data['d'].min()} to {zone_data['d'].max()}\")\n",
    "    logger.info(f\"Zone {zone_id}: Using cutoff date {cutoff_date} (last {test_days} days for testing)\")\n",
    "\n",
    "    train_df = zone_data[zone_data['d'] <= cutoff_date].copy()\n",
    "    test_df = zone_data[zone_data['d'] > cutoff_date].copy()\n",
    "\n",
    "    logger.info(f\"Zone {zone_id}: Train samples: {len(train_df)}, Test samples: {len(test_df)}\")\n",
    "\n",
    "    # Check if zone has enough data\n",
    "    if len(train_df) < MIN_TRIPS_THRESHOLD:\n",
    "        logger.info(f\"Zone {zone_id}: Insufficient data ({len(train_df)} < {MIN_TRIPS_THRESHOLD}), skipping\")\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    # Prepare features\n",
    "    logger.info(f\"Zone {zone_id}: Building preprocessor and extracting features\")\n",
    "    preprocessor, feature_cols = build_preprocessor_for_zone()\n",
    "    X_train = preprocessor.fit_transform(train_df[feature_cols])\n",
    "    X_test = preprocessor.transform(test_df[feature_cols])\n",
    "\n",
    "    logger.info(f\"Zone {zone_id}: Feature dimensions - Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "    # Target columns\n",
    "    targets = ['y_single', 'y_small', 'y_medium', 'y_large']\n",
    "    y_train = train_df[targets].values\n",
    "    y_test = test_df[targets].values\n",
    "\n",
    "    logger.info(f\"Zone {zone_id}: Target shape - Train: {y_train.shape}, Test: {y_test.shape}\")\n",
    "    logger.info(f\"Zone {zone_id}: Average trips per bucket - Single: {y_train[:, 0].mean():.2f}, Small: {y_train[:, 1].mean():.2f}, Medium: {y_train[:, 2].mean():.2f}, Large: {y_train[:, 3].mean():.2f}\")\n",
    "\n",
    "    # Train XGBoost model\n",
    "    logger.info(f\"Zone {zone_id}: Training XGBoost model\")\n",
    "    xgb_model = MultiOutputRegressor(\n",
    "        XGBRegressor(\n",
    "            objective='count:poisson',\n",
    "            max_delta_step=1,\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=4,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=1\n",
    "        )\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    xgb_pred = xgb_model.predict(X_test)\n",
    "    logger.info(f\"Zone {zone_id}: XGBoost training completed\")\n",
    "\n",
    "    # Train KNN model\n",
    "    n_neighbors = min(20, len(X_train) // 10)\n",
    "    logger.info(f\"Zone {zone_id}: Training KNN model with {n_neighbors} neighbors\")\n",
    "    knn_model = MultiOutputRegressor(\n",
    "        KNeighborsRegressor(\n",
    "            n_neighbors=n_neighbors,\n",
    "            weights='distance',\n",
    "            metric='manhattan'\n",
    "        )\n",
    "    )\n",
    "    knn_model.fit(X_train, y_train)\n",
    "    knn_pred = knn_model.predict(X_test)\n",
    "    logger.info(f\"Zone {zone_id}: KNN training completed\")\n",
    "\n",
    "    # Calculate residuals for confidence intervals\n",
    "    logger.info(f\"Zone {zone_id}: Calculating residuals for confidence intervals\")\n",
    "    xgb_residuals = y_train - xgb_model.predict(X_train)\n",
    "    knn_residuals = y_train - knn_model.predict(X_train)\n",
    "\n",
    "    xgb_residual_std = np.std(xgb_residuals, axis=0)\n",
    "    knn_residual_std = np.std(knn_residuals, axis=0)\n",
    "\n",
    "    logger.info(f\"Zone {zone_id}: XGB residual std: {xgb_residual_std}\")\n",
    "    logger.info(f\"Zone {zone_id}: KNN residual std: {knn_residual_std}\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    logger.info(f\"Zone {zone_id}: Calculating performance metrics\")\n",
    "    metrics = {\n",
    "        'xgb': {},\n",
    "        'knn': {}\n",
    "    }\n",
    "\n",
    "    test_predictions = test_df[['d', 'hr', 'dow', 'month']].copy()\n",
    "    test_predictions['zone_id'] = zone_id\n",
    "    test_predictions['prediction_type'] = 'test'  # Mark as test data\n",
    "\n",
    "    for i, target in enumerate(targets):\n",
    "        test_predictions[f'actual_{target}'] = y_test[:, i]\n",
    "        test_predictions[f'xgb_pred_{target}'] = xgb_pred[:, i]\n",
    "        test_predictions[f'knn_pred_{target}'] = knn_pred[:, i]\n",
    "        # XGBoost metrics\n",
    "        xgb_mae = mean_absolute_error(y_test[:, i], xgb_pred[:, i])\n",
    "        xgb_rmse = np.sqrt(mean_squared_error(y_test[:, i], xgb_pred[:, i]))\n",
    "        xgb_smape = calculate_smape(y_test[:, i], xgb_pred[:, i])\n",
    "\n",
    "        metrics['xgb'][target] = {\n",
    "            'mae': xgb_mae,\n",
    "            'rmse': xgb_rmse,\n",
    "            'smape': xgb_smape\n",
    "        }\n",
    "\n",
    "        # KNN metrics\n",
    "        knn_mae = mean_absolute_error(y_test[:, i], knn_pred[:, i])\n",
    "        knn_rmse = np.sqrt(mean_squared_error(y_test[:, i], knn_pred[:, i]))\n",
    "        knn_smape = calculate_smape(y_test[:, i], knn_pred[:, i])\n",
    "\n",
    "        metrics['knn'][target] = {\n",
    "            'mae': knn_mae,\n",
    "            'rmse': knn_rmse,\n",
    "            'smape': knn_smape\n",
    "        }\n",
    "\n",
    "        logger.info(f\"Zone {zone_id}: {target} - XGB (MAE: {xgb_mae:.2f}, RMSE: {xgb_rmse:.2f}, SMAPE: {xgb_smape:.2f}%) | KNN (MAE: {knn_mae:.2f}, RMSE: {knn_rmse:.2f}, SMAPE: {knn_smape:.2f}%)\")\n",
    "\n",
    "    # Create visualization\n",
    "    logger.info(f\"Zone {zone_id}: Creating performance visualization\")\n",
    "    # create_zone_performance_plot(zone_id, test_df, y_test, xgb_pred, knn_pred, metrics, targets)\n",
    "\n",
    "    logger.info(f\"Zone {zone_id}: Training completed successfully\")\n",
    "    return xgb_model, knn_model, preprocessor, metrics, feature_cols, (xgb_residual_std, knn_residual_std), test_predictions\n",
    "\n",
    "\n",
    "def create_zone_performance_plot(zone_id, test_df, y_test, xgb_pred, knn_pred, metrics, targets):\n",
    "    \"\"\"Create a comprehensive performance plot for a zone.\"\"\"\n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = GridSpec(3, 2, figure=fig, hspace=0.3, wspace=0.3)\n",
    "\n",
    "    # Main title\n",
    "    fig.suptitle(f'Zone {zone_id} - Model Performance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Colors for each passenger bucket\n",
    "    colors = {'y_single': '#1f77b4', 'y_small': '#ff7f0e', 'y_medium': '#2ca02c', 'y_large': '#d62728'}\n",
    "\n",
    "    # Create datetime index for x-axis\n",
    "    test_df['datetime'] = pd.to_datetime(test_df['d']) + pd.to_timedelta(test_df['hr'], unit='h')\n",
    "\n",
    "    # Plot 1: Time series comparison for all buckets\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "\n",
    "    # Plot actual vs predicted for total trips\n",
    "    total_actual = y_test.sum(axis=1)\n",
    "    total_xgb = xgb_pred.sum(axis=1)\n",
    "    total_knn = knn_pred.sum(axis=1)\n",
    "\n",
    "    ax1.plot(test_df['datetime'], total_actual, 'k-', label='Actual', linewidth=2)\n",
    "    ax1.plot(test_df['datetime'], total_xgb, 'b--', label='XGBoost', linewidth=1.5, alpha=0.7)\n",
    "    ax1.plot(test_df['datetime'], total_knn, 'r:', label='KNN', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "    ax1.set_title('Total Trips: Actual vs Predicted', fontsize=12)\n",
    "    ax1.set_xlabel('Date/Time')\n",
    "    ax1.set_ylabel('Total Trips')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d %H:00'))\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    # Add metrics annotation\n",
    "    total_xgb_mae = mean_absolute_error(total_actual, total_xgb)\n",
    "    total_knn_mae = mean_absolute_error(total_actual, total_knn)\n",
    "    ax1.text(0.02, 0.95, f'XGB MAE: {total_xgb_mae:.2f}\\nKNN MAE: {total_knn_mae:.2f}',\n",
    "             transform=ax1.transAxes, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "    # Plots 2-5: Individual bucket performance\n",
    "    for idx, (i, target) in enumerate(zip(range(4), targets)):\n",
    "        row = (idx // 2) + 1\n",
    "        col = idx % 2\n",
    "        ax = fig.add_subplot(gs[row, col])\n",
    "\n",
    "        actual = y_test[:, i]\n",
    "        xgb = xgb_pred[:, i]\n",
    "        knn = knn_pred[:, i]\n",
    "\n",
    "        # Plot time series\n",
    "        ax.plot(test_df['datetime'], actual, 'k-', label='Actual', linewidth=1.5)\n",
    "        ax.plot(test_df['datetime'], xgb, 'b--', label='XGBoost', linewidth=1, alpha=0.7)\n",
    "        ax.plot(test_df['datetime'], knn, 'r:', label='KNN', linewidth=1, alpha=0.7)\n",
    "\n",
    "        ax.set_title(f'{target.replace(\"y_\", \"\").capitalize()} Passengers', fontsize=10)\n",
    "        ax.set_xlabel('Date/Time', fontsize=8)\n",
    "        ax.set_ylabel('Trip Count', fontsize=8)\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "        # Add metrics box\n",
    "        xgb_metrics = metrics['xgb'][target]\n",
    "        knn_metrics = metrics['knn'][target]\n",
    "\n",
    "        metrics_text = (f\"XGBoost:\\n\"\n",
    "                       f\"  MAE: {xgb_metrics['mae']:.2f}\\n\"\n",
    "                       f\"  RMSE: {xgb_metrics['rmse']:.2f}\\n\"\n",
    "                       f\"  SMAPE: {xgb_metrics['smape']:.1f}%\\n\"\n",
    "                       f\"KNN:\\n\"\n",
    "                       f\"  MAE: {knn_metrics['mae']:.2f}\\n\"\n",
    "                       f\"  RMSE: {knn_metrics['rmse']:.2f}\\n\"\n",
    "                       f\"  SMAPE: {knn_metrics['smape']:.1f}%\")\n",
    "\n",
    "        ax.text(0.02, 0.95, metrics_text, transform=ax.transAxes,\n",
    "                verticalalignment='top', fontsize=7,\n",
    "                bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.7))\n",
    "\n",
    "    # Save plot\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Create directory if it doesn't exist\n",
    "    import os\n",
    "    plot_dir = f\"zone_performance_plots/{zone_id}\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "    # Save with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    plot_path = f\"{plot_dir}/performance_{timestamp}.png\"\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    logger.info(f\"Zone {zone_id}: Performance plot saved to {plot_path}\")\n",
    "\n",
    "    # Also save to GCS if needed\n",
    "    try:\n",
    "        bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "        blob = bucket.blob(f\"zone_performance_plots/zone_{zone_id}_performance_{timestamp}.png\")\n",
    "        blob.upload_from_filename(plot_path)\n",
    "        logger.info(f\"Zone {zone_id}: Performance plot uploaded to GCS\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Zone {zone_id}: Failed to upload plot to GCS: {str(e)}\")\n",
    "\n",
    "\n",
    "def create_summary_performance_plot(all_metrics, taxi_type):\n",
    "    \"\"\"Create a summary plot showing performance across all zones.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f'{taxi_type.capitalize()} Taxi - Model Performance Summary Across All Zones', fontsize=14)\n",
    "\n",
    "    targets = ['y_single', 'y_small', 'y_medium', 'y_large']\n",
    "\n",
    "    for idx, target in enumerate(targets):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "\n",
    "        # Collect metrics for all zones\n",
    "        zones = []\n",
    "        xgb_maes = []\n",
    "        knn_maes = []\n",
    "        xgb_smapes = []\n",
    "        knn_smapes = []\n",
    "\n",
    "        for zone_id, zone_metrics in all_metrics.items():\n",
    "            zones.append(zone_id)\n",
    "            xgb_maes.append(zone_metrics['xgb'][target]['mae'])\n",
    "            knn_maes.append(zone_metrics['knn'][target]['mae'])\n",
    "            xgb_smapes.append(zone_metrics['xgb'][target]['smape'])\n",
    "            knn_smapes.append(zone_metrics['knn'][target]['smape'])\n",
    "\n",
    "        # Create bar plot\n",
    "        x = np.arange(len(zones))\n",
    "        width = 0.35\n",
    "\n",
    "        ax.bar(x - width/2, xgb_maes, width, label='XGBoost MAE', alpha=0.7)\n",
    "        ax.bar(x + width/2, knn_maes, width, label='KNN MAE', alpha=0.7)\n",
    "\n",
    "        ax.set_title(f'{target.replace(\"y_\", \"\").capitalize()} Passengers - MAE by Zone')\n",
    "        ax.set_xlabel('Zone ID')\n",
    "        ax.set_ylabel('MAE')\n",
    "        ax.legend()\n",
    "\n",
    "        # Limit x-axis labels for readability\n",
    "        if len(zones) > 20:\n",
    "            step = len(zones) // 20\n",
    "            ax.set_xticks(x[::step])\n",
    "            ax.set_xticklabels(zones[::step], rotation=45)\n",
    "        else:\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(zones, rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save summary plot\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    summary_path = f\"zone_performance_plots/{taxi_type}_summary_{timestamp}.png\"\n",
    "    plt.savefig(summary_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    logger.info(f\"Summary performance plot saved to {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cKo5YoWp6Qeb",
   "metadata": {
    "id": "cKo5YoWp6Qeb"
   },
   "outputs": [],
   "source": [
    "def predict_future_iterative(zone_data, zone_models, zone_id, last_date, residual_stds, days_ahead=15):\n",
    "    \"\"\"Generate iterative predictions for a specific zone.\"\"\"\n",
    "    logger.info(f\"Zone {zone_id}: Starting iterative predictions for {days_ahead} days ahead\")\n",
    "\n",
    "    if zone_id not in zone_models:\n",
    "        logger.warning(f\"Zone {zone_id}: No trained models found, skipping predictions\")\n",
    "        return None\n",
    "\n",
    "    models = zone_models[zone_id]\n",
    "    xgb_residual_std, knn_residual_std = residual_stds[zone_id]\n",
    "\n",
    "    logger.info(f\"Zone {zone_id}: Residual STDs - XGB: {xgb_residual_std}, KNN: {knn_residual_std}\")\n",
    "\n",
    "    # Calculate z-score for confidence intervals\n",
    "    z_score = 1.96 if CONFIDENCE_LEVEL == 0.95 else 2.576\n",
    "    logger.info(f\"Zone {zone_id}: Using confidence level {CONFIDENCE_LEVEL} (z-score: {z_score})\")\n",
    "\n",
    "    # Prepare recent history for lag features\n",
    "    zone_data = zone_data.sort_values(['d', 'hr'])\n",
    "    recent_history = {\n",
    "        'y_single': list(zone_data['y_single'].iloc[-168:].values),\n",
    "        'y_small': list(zone_data['y_small'].iloc[-168:].values),\n",
    "        'y_medium': list(zone_data['y_medium'].iloc[-168:].values),\n",
    "        'y_large': list(zone_data['y_large'].iloc[-168:].values),\n",
    "        'total_trips': list(zone_data['total_trips'].iloc[-168:].values)\n",
    "    }\n",
    "\n",
    "    logger.info(f\"Zone {zone_id}: Prepared recent history with {len(recent_history['total_trips'])} hours of data\")\n",
    "    logger.info(f\"Zone {zone_id}: Last historical values - Single: {recent_history['y_single'][-1]:.2f}, Small: {recent_history['y_small'][-1]:.2f}, Medium: {recent_history['y_medium'][-1]:.2f}, Large: {recent_history['y_large'][-1]:.2f}\")\n",
    "\n",
    "    # Store predictions\n",
    "    future_predictions = []\n",
    "    xgb_predictions = {'y_single': [], 'y_small': [], 'y_medium': [], 'y_large': []}\n",
    "    knn_predictions = {'y_single': [], 'y_small': [], 'y_medium': [], 'y_large': []}\n",
    "\n",
    "    # Generate predictions iteratively\n",
    "    future_hours = days_ahead * 24\n",
    "\n",
    "    # Handle the date/datetime conversion more simply\n",
    "    # Create a datetime object for the start of predictions\n",
    "    base_date = pd.to_datetime(last_date)\n",
    "    start_datetime = base_date + timedelta(days=1)\n",
    "\n",
    "    logger.info(f\"Zone {zone_id}: Generating {future_hours} hourly predictions starting from {start_datetime.date()}\")\n",
    "\n",
    "   # Log progress every 24 hours\n",
    "    for h in range(future_hours):\n",
    "        pred_time = start_datetime + timedelta(hours=h)\n",
    "\n",
    "        if h % 24 == 0:\n",
    "            logger.info(f\"Zone {zone_id}: Predicting day {h//24 + 1}/{days_ahead} - {pred_time.date()}\")\n",
    "\n",
    "        # Create feature row\n",
    "        future_features = {}\n",
    "\n",
    "        # Time features\n",
    "        future_features['hr'] = pred_time.hour\n",
    "        future_features['dow'] = pred_time.weekday() + 1  # Match BigQuery's DAYOFWEEK\n",
    "        future_features['month'] = pred_time.month\n",
    "\n",
    "        # Cyclical features\n",
    "        future_features['hour_sin'] = np.sin(2 * np.pi * pred_time.hour / 24)\n",
    "        future_features['hour_cos'] = np.cos(2 * np.pi * pred_time.hour / 24)\n",
    "        future_features['dow_sin'] = np.sin(2 * np.pi * (pred_time.weekday() + 1) / 7)\n",
    "        future_features['dow_cos'] = np.cos(2 * np.pi * (pred_time.weekday() + 1) / 7)\n",
    "        future_features['month_sin'] = np.sin(2 * np.pi * pred_time.month / 12)\n",
    "        future_features['month_cos'] = np.cos(2 * np.pi * pred_time.month / 12)\n",
    "\n",
    "        # Zone statistics (use historical values)\n",
    "        future_features['zone_avg_trips'] = zone_data['zone_avg_trips'].iloc[-1]\n",
    "        future_features['zone_std_trips'] = zone_data['zone_std_trips'].iloc[-1]\n",
    "\n",
    "        # Lag features - use recent history or predictions\n",
    "        for bucket in ['y_single', 'y_small', 'y_medium', 'y_large']:\n",
    "            if h == 0:\n",
    "                # First prediction - use historical data\n",
    "                future_features[f'{bucket}_lag1'] = recent_history[bucket][-1]\n",
    "                future_features[f'{bucket}_lag7'] = recent_history[bucket][-24*7] if len(recent_history[bucket]) >= 24*7 else recent_history[bucket][0]\n",
    "            else:\n",
    "                # Use previous predictions\n",
    "                future_features[f'{bucket}_lag1'] = xgb_predictions[bucket][-1]  # Use XGB predictions for consistency\n",
    "                if h >= 24*7:\n",
    "                    future_features[f'{bucket}_lag7'] = xgb_predictions[bucket][-24*7]\n",
    "                else:\n",
    "                    idx = -(24*7 - h)\n",
    "                    future_features[f'{bucket}_lag7'] = recent_history[bucket][idx] if abs(idx) < len(recent_history[bucket]) else recent_history[bucket][0]\n",
    "\n",
    "        # Total trips lag features\n",
    "        if h == 0:\n",
    "            future_features['total_trips_lag1'] = recent_history['total_trips'][-1]\n",
    "            future_features['total_trips_lag7'] = recent_history['total_trips'][-24*7] if len(recent_history['total_trips']) >= 24*7 else recent_history['total_trips'][0]\n",
    "        else:\n",
    "            # Sum of all bucket predictions\n",
    "            future_features['total_trips_lag1'] = sum(xgb_predictions[b][-1] for b in ['y_single', 'y_small', 'y_medium', 'y_large'])\n",
    "            if h >= 24*7:\n",
    "                future_features['total_trips_lag7'] = sum(xgb_predictions[b][-24*7] for b in ['y_single', 'y_small', 'y_medium', 'y_large'])\n",
    "            else:\n",
    "                idx = -(24*7 - h)\n",
    "                future_features['total_trips_lag7'] = recent_history['total_trips'][idx] if abs(idx) < len(recent_history['total_trips']) else recent_history['total_trips'][0]\n",
    "\n",
    "        # Calculate rolling features\n",
    "        recent_total = recent_history['total_trips'] + [sum(xgb_predictions[b][i] for b in ['y_single', 'y_small', 'y_medium', 'y_large']) for i in range(len(xgb_predictions['y_single']))]\n",
    "\n",
    "        future_features['total_trips_3dma'] = np.mean(recent_total[-3*24:]) if len(recent_total) >= 3*24 else np.mean(recent_history['total_trips'][-3*24:])\n",
    "        future_features['total_trips_7dma'] = np.mean(recent_total[-7*24:]) if len(recent_total) >= 7*24 else np.mean(recent_history['total_trips'][-7*24:])\n",
    "\n",
    "        # Current total trips (use historical average for same hour/dow)\n",
    "        hist_same_time = zone_data[(zone_data['hr'] == pred_time.hour) & (zone_data['dow'] == pred_time.weekday() + 1)]\n",
    "        future_features['total_trips'] = hist_same_time['total_trips'].mean() if len(hist_same_time) > 0 else zone_data['total_trips'].mean()\n",
    "\n",
    "        # Create DataFrame for prediction\n",
    "        future_df = pd.DataFrame([future_features])\n",
    "\n",
    "        # Make predictions\n",
    "        X_future = models['preprocessor'].transform(future_df[models['feature_cols']])\n",
    "\n",
    "        # XGBoost predictions\n",
    "        xgb_pred = models['xgb'].predict(X_future)[0]\n",
    "        xgb_pred = np.maximum(0, xgb_pred)  # Ensure non-negative\n",
    "\n",
    "        # KNN predictions\n",
    "        knn_pred = models['knn'].predict(X_future)[0]\n",
    "        knn_pred = np.maximum(0, knn_pred)  # Ensure non-negative\n",
    "\n",
    "        # Log predictions for key hours\n",
    "        # if h % 24 == 0 or h == 0:\n",
    "        #     logger.info(f\"Zone {zone_id}: Hour {h} predictions - XGB: {xgb_pred}, KNN: {knn_pred}\")\n",
    "\n",
    "        # Store predictions\n",
    "        for i, bucket in enumerate(['y_single', 'y_small', 'y_medium', 'y_large']):\n",
    "            xgb_predictions[bucket].append(xgb_pred[i])\n",
    "            knn_predictions[bucket].append(knn_pred[i])\n",
    "\n",
    "            # Update recent history\n",
    "            recent_history[bucket].append(xgb_pred[i])\n",
    "            if len(recent_history[bucket]) > 168:\n",
    "                recent_history[bucket].pop(0)\n",
    "\n",
    "        # Update total trips history\n",
    "        recent_history['total_trips'].append(sum(xgb_pred))\n",
    "        if len(recent_history['total_trips']) > 168:\n",
    "            recent_history['total_trips'].pop(0)\n",
    "\n",
    "        # Create result row\n",
    "        result = {\n",
    "            'd': pred_time.date(),\n",
    "            'hr': pred_time.hour,\n",
    "            'zone_id': zone_id,\n",
    "            'dow': pred_time.weekday() + 1,\n",
    "            'month': pred_time.month,\n",
    "            'prediction_type': 'future'\n",
    "        }\n",
    "\n",
    "        # Add predictions with confidence intervals\n",
    "        uncertainty_factor = 1 + 0.02 * h  # 2% increase per hour for future uncertainty\n",
    "\n",
    "        for i, bucket in enumerate(['y_single', 'y_small', 'y_medium', 'y_large']):\n",
    "            # XGBoost predictions with CI\n",
    "            result[f'xgb_pred_{bucket}'] = xgb_pred[i]\n",
    "            result[f'xgb_pred_{bucket}_lower'] = max(0, xgb_pred[i] - z_score * xgb_residual_std[i] * uncertainty_factor)\n",
    "            result[f'xgb_pred_{bucket}_upper'] = xgb_pred[i] + z_score * xgb_residual_std[i] * uncertainty_factor\n",
    "\n",
    "            # KNN predictions with CI\n",
    "            result[f'knn_pred_{bucket}'] = knn_pred[i]\n",
    "            result[f'knn_pred_{bucket}_lower'] = max(0, knn_pred[i] - z_score * knn_residual_std[i] * uncertainty_factor)\n",
    "            result[f'knn_pred_{bucket}_upper'] = knn_pred[i] + z_score * knn_residual_std[i] * uncertainty_factor\n",
    "\n",
    "        future_predictions.append(result)\n",
    "\n",
    "    # Summary statistics\n",
    "    predictions_df = pd.DataFrame(future_predictions)\n",
    "\n",
    "    logger.info(f\"Zone {zone_id}: Prediction complete. Generated {len(predictions_df)} hourly predictions\")\n",
    "    logger.info(f\"Zone {zone_id}: Average daily totals - XGB: {predictions_df[[f'xgb_pred_{b}' for b in ['y_single', 'y_small', 'y_medium', 'y_large']]].sum(axis=1).groupby(predictions_df['d']).sum().mean():.2f}\")\n",
    "    logger.info(f\"Zone {zone_id}: Average daily totals - KNN: {predictions_df[[f'knn_pred_{b}' for b in ['y_single', 'y_small', 'y_medium', 'y_large']]].sum(axis=1).groupby(predictions_df['d']).sum().mean():.2f}\")\n",
    "\n",
    "    # Log prediction ranges\n",
    "    for bucket in ['y_single', 'y_small', 'y_medium', 'y_large']:\n",
    "        xgb_min = predictions_df[f'xgb_pred_{bucket}'].min()\n",
    "        xgb_max = predictions_df[f'xgb_pred_{bucket}'].max()\n",
    "        knn_min = predictions_df[f'knn_pred_{bucket}'].min()\n",
    "        knn_max = predictions_df[f'knn_pred_{bucket}'].max()\n",
    "        logger.info(f\"Zone {zone_id}: {bucket} prediction ranges - XGB: [{xgb_min:.2f}, {xgb_max:.2f}], KNN: [{knn_min:.2f}, {knn_max:.2f}]\")\n",
    "\n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obRU7nfY6X89",
   "metadata": {
    "id": "obRU7nfY6X89"
   },
   "outputs": [],
   "source": [
    "def train_all_zones(df, taxi_type, max_workers=4):\n",
    "    \"\"\"Train models for all zones in parallel.\"\"\"\n",
    "    zones = df['zone_id'].unique()\n",
    "    zone_models = {}\n",
    "    all_metrics = {}\n",
    "    residual_stds = {}\n",
    "    test_predictions = {}\n",
    "\n",
    "    logger.info(f\"=\"*80)\n",
    "    logger.info(f\"Starting parallel training for {taxi_type} taxi\")\n",
    "    logger.info(f\"Total zones to process: {len(zones)}\")\n",
    "    logger.info(f\"Max parallel workers: {max_workers}\")\n",
    "    logger.info(f\"Data date range: {df['d'].min()} to {df['d'].max()}\")\n",
    "    logger.info(f\"=\"*80)\n",
    "\n",
    "    # Get zone statistics\n",
    "    zone_stats = df.groupby('zone_id')['total_trips'].agg(['count', 'sum', 'mean']).reset_index()\n",
    "    logger.info(f\"Zone statistics:\")\n",
    "    logger.info(f\"  - Zones with > 1000 samples: {len(zone_stats[zone_stats['count'] > 1000])}\")\n",
    "    logger.info(f\"  - Zones with > 10000 samples: {len(zone_stats[zone_stats['count'] > 10000])}\")\n",
    "    logger.info(f\"  - Total trips across all zones: {zone_stats['sum'].sum():,.0f}\")\n",
    "\n",
    "    # Track processing stats\n",
    "    successful_zones = 0\n",
    "    skipped_zones = 0\n",
    "    failed_zones = 0\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        logger.info(f\"Submitting {len(zones)} zone training tasks to thread pool...\")\n",
    "\n",
    "        future_to_zone = {\n",
    "            executor.submit(\n",
    "                train_zone_models,\n",
    "                df[df['zone_id'] == zone].copy(),\n",
    "                zone\n",
    "            ): zone\n",
    "            for zone in zones\n",
    "        }\n",
    "\n",
    "        logger.info(f\"All tasks submitted. Processing results...\")\n",
    "\n",
    "        for idx, future in enumerate(tqdm(as_completed(future_to_zone), total=len(zones))):\n",
    "            zone = future_to_zone[future]\n",
    "\n",
    "            # Log progress every 10 zones\n",
    "            if (idx + 1) % 10 == 0:\n",
    "                elapsed = (datetime.now() - start_time).total_seconds()\n",
    "                rate = (idx + 1) / elapsed if elapsed > 0 else 0\n",
    "                eta = (len(zones) - idx - 1) / rate if rate > 0 else 0\n",
    "                logger.info(f\"Progress: {idx + 1}/{len(zones)} zones completed ({(idx + 1)/len(zones)*100:.1f}%) - Rate: {rate:.2f} zones/sec - ETA: {eta/60:.1f} min\")\n",
    "\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result[0] is not None:\n",
    "                    xgb_model, knn_model, preprocessor, metrics, feature_cols, residual_std, test_preds  = result\n",
    "                    zone_models[zone] = {\n",
    "                        'xgb': xgb_model,\n",
    "                        'knn': knn_model,\n",
    "                        'preprocessor': preprocessor,\n",
    "                        'feature_cols': feature_cols\n",
    "                    }\n",
    "                    all_metrics[zone] = metrics\n",
    "                    residual_stds[zone] = residual_std\n",
    "                    test_predictions[zone] = test_preds\n",
    "                    successful_zones += 1\n",
    "\n",
    "                    # Log best performing model for this zone\n",
    "                    avg_xgb_smape = np.mean([metrics['xgb'][target]['smape'] for target in metrics['xgb']])\n",
    "                    avg_knn_smape = np.mean([metrics['knn'][target]['smape'] for target in metrics['knn']])\n",
    "                    best_model = 'XGB' if avg_xgb_smape < avg_knn_smape else 'KNN'\n",
    "\n",
    "                    logger.info(f\"Zone {zone} trained successfully - Best model: {best_model} (XGB SMAPE: {avg_xgb_smape:.1f}%, KNN SMAPE: {avg_knn_smape:.1f}%)\")\n",
    "                else:\n",
    "                    skipped_zones += 1\n",
    "                    logger.warning(f\"Zone {zone} skipped - insufficient data\")\n",
    "\n",
    "            except Exception as e:\n",
    "                failed_zones += 1\n",
    "                logger.error(f\"Error training zone {zone}: {str(e)}\")\n",
    "                logger.error(f\"Zone {zone} stack trace:\", exc_info=True)\n",
    "\n",
    "    # Final summary\n",
    "    total_time = (datetime.now() - start_time).total_seconds()\n",
    "    logger.info(f\"=\"*80)\n",
    "    logger.info(f\"Training completed for {taxi_type} taxi\")\n",
    "    logger.info(f\"Total time: {total_time/60:.2f} minutes ({total_time:.1f} seconds)\")\n",
    "    logger.info(f\"Results:\")\n",
    "    logger.info(f\"  - Successful zones: {successful_zones}/{len(zones)} ({successful_zones/len(zones)*100:.1f}%)\")\n",
    "    logger.info(f\"  - Skipped zones: {skipped_zones}/{len(zones)} ({skipped_zones/len(zones)*100:.1f}%)\")\n",
    "    logger.info(f\"  - Failed zones: {failed_zones}/{len(zones)} ({failed_zones/len(zones)*100:.1f}%)\")\n",
    "    logger.info(f\"  - Average time per zone: {total_time/len(zones):.2f} seconds\")\n",
    "\n",
    "    # Performance summary across all zones\n",
    "    if all_metrics:\n",
    "        logger.info(f\"\\nPerformance summary across all trained zones:\")\n",
    "        for target in ['y_single', 'y_small', 'y_medium', 'y_large']:\n",
    "            xgb_smapes = [metrics['xgb'][target]['smape'] for metrics in all_metrics.values()]\n",
    "            knn_smapes = [metrics['knn'][target]['smape'] for metrics in all_metrics.values()]\n",
    "\n",
    "            logger.info(f\"  {target}:\")\n",
    "            logger.info(f\"    - XGB: Mean SMAPE={np.mean(xgb_smapes):.1f}%, Median={np.median(xgb_smapes):.1f}%, Std={np.std(xgb_smapes):.1f}%\")\n",
    "            logger.info(f\"    - KNN: Mean SMAPE={np.mean(knn_smapes):.1f}%, Median={np.median(knn_smapes):.1f}%, Std={np.std(knn_smapes):.1f}%\")\n",
    "\n",
    "    # Identify best and worst performing zones\n",
    "    if all_metrics:\n",
    "        zone_avg_smapes = {}\n",
    "        for zone, metrics in all_metrics.items():\n",
    "            avg_smape = np.mean([metrics['xgb'][target]['smape'] for target in metrics['xgb']])\n",
    "            zone_avg_smapes[zone] = avg_smape\n",
    "\n",
    "        sorted_zones = sorted(zone_avg_smapes.items(), key=lambda x: x[1])\n",
    "        logger.info(f\"\\nTop 5 best performing zones (lowest SMAPE):\")\n",
    "        for zone, smape in sorted_zones[:5]:\n",
    "            logger.info(f\"  - Zone {zone}: {smape:.1f}%\")\n",
    "\n",
    "        logger.info(f\"\\nTop 5 worst performing zones (highest SMAPE):\")\n",
    "        for zone, smape in sorted_zones[-5:]:\n",
    "            logger.info(f\"  - Zone {zone}: {smape:.1f}%\")\n",
    "\n",
    "    logger.info(f\"=\"*80)\n",
    "\n",
    "    return zone_models, all_metrics, residual_stds, df['d'].max(), test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tZCSMQoL6sWn",
   "metadata": {
    "id": "tZCSMQoL6sWn"
   },
   "outputs": [],
   "source": [
    "def save_results(all_predictions, all_metrics, test_predictions, taxi_type):\n",
    "    \"\"\"Save predictions and metrics to BigQuery.\"\"\"\n",
    "    logger.info(f\"=\"*80)\n",
    "    logger.info(f\"Starting to save results for {taxi_type} taxi\")\n",
    "    logger.info(f\"=\"*80)\n",
    "\n",
    "    # Combine test and future predictions\n",
    "    all_combined_predictions = []\n",
    "\n",
    "    # Add test predictions\n",
    "    for zone_id, test_preds in test_predictions.items():\n",
    "        all_combined_predictions.append(test_preds)\n",
    "\n",
    "    # Add future predictions\n",
    "    all_combined_predictions.extend(all_predictions)\n",
    "\n",
    "    # Combine all predictions (ONLY ONCE)\n",
    "    logger.info(f\"Combining test predictions from {len(test_predictions)} zones and future predictions from {len(all_predictions)} zones\")\n",
    "    predictions_df = pd.concat(all_combined_predictions, ignore_index=True)\n",
    "\n",
    "    if 'd' in predictions_df.columns:\n",
    "        predictions_df['d'] = pd.to_datetime(predictions_df['d']).dt.date\n",
    "\n",
    "    logger.info(f\"Total prediction rows: {len(predictions_df):,}\")\n",
    "    logger.info(f\"  - Test predictions: {len(predictions_df[predictions_df['prediction_type'] == 'test']):,}\")\n",
    "    logger.info(f\"  - Future predictions: {len(predictions_df[predictions_df['prediction_type'] == 'future']):,}\")\n",
    "    logger.info(f\"Date range: {predictions_df['d'].min()} to {predictions_df['d'].max()}\")\n",
    "    logger.info(f\"Unique zones in predictions: {predictions_df['zone_id'].nunique()}\")\n",
    "\n",
    "    # Prepare output tables\n",
    "    xgb_table = f\"{PROJECT_ID}.{OUTPUT_DATASET}.fleet_recommender_{taxi_type}_predictions_new_xgb\"\n",
    "    knn_table = f\"{PROJECT_ID}.{OUTPUT_DATASET}.fleet_recommender_{taxi_type}_predictions_new_knn\"\n",
    "    metrics_table = f\"{PROJECT_ID}.{OUTPUT_DATASET}.fleet_recommender_{taxi_type}_metrics_new\"\n",
    "\n",
    "    logger.info(f\"Output tables:\")\n",
    "    logger.info(f\"  - XGBoost predictions: {xgb_table}\")\n",
    "    logger.info(f\"  - KNN predictions: {knn_table}\")\n",
    "    logger.info(f\"  - Metrics: {metrics_table}\")\n",
    "\n",
    "    # Add metadata\n",
    "    predictions_df['taxi_type'] = taxi_type\n",
    "    predictions_df['prediction_timestamp'] = datetime.now()\n",
    "    predictions_df['date'] = predictions_df['d']\n",
    "\n",
    "    logger.info(f\"Added metadata - prediction timestamp: {predictions_df['prediction_timestamp'].iloc[0]}\")\n",
    "\n",
    "    # Initialize column lists\n",
    "    xgb_cols = ['taxi_type', 'date', 'hr', 'zone_id', 'dow', 'month', 'prediction_timestamp', 'prediction_type']\n",
    "    knn_cols = ['taxi_type', 'date', 'hr', 'zone_id', 'dow', 'month', 'prediction_timestamp', 'prediction_type']\n",
    "\n",
    "    # Add columns for each bucket\n",
    "    for bucket in ['y_single', 'y_small', 'y_medium', 'y_large']:\n",
    "        # Add actual values if they exist\n",
    "        if f'actual_{bucket}' in predictions_df.columns:\n",
    "            xgb_cols.append(f'actual_{bucket}')\n",
    "            knn_cols.append(f'actual_{bucket}')\n",
    "\n",
    "        # Add XGB predictions\n",
    "        xgb_cols.extend([f'xgb_pred_{bucket}', f'xgb_pred_{bucket}_lower', f'xgb_pred_{bucket}_upper'])\n",
    "\n",
    "        # Add KNN predictions\n",
    "        knn_cols.extend([f'knn_pred_{bucket}', f'knn_pred_{bucket}_lower', f'knn_pred_{bucket}_upper'])\n",
    "\n",
    "    # Fill missing columns with appropriate values\n",
    "    for col in set(xgb_cols + knn_cols):\n",
    "        if col not in predictions_df.columns:\n",
    "            if 'actual_' in col:\n",
    "                predictions_df[col] = np.nan\n",
    "            elif '_lower' in col or '_upper' in col:\n",
    "                predictions_df[col] = np.nan\n",
    "            elif col == 'prediction_type':\n",
    "                predictions_df[col] = 'future'  # Default for predictions without this column\n",
    "\n",
    "    # Save XGBoost predictions\n",
    "    logger.info(f\"Preparing XGBoost predictions for upload...\")\n",
    "    xgb_output = predictions_df[xgb_cols].copy()\n",
    "\n",
    "    # Log XGBoost prediction statistics\n",
    "    logger.info(f\"XGBoost prediction statistics:\")\n",
    "    for bucket in ['y_single', 'y_small', 'y_medium', 'y_large']:\n",
    "        pred_col = f'xgb_pred_{bucket}'\n",
    "        if pred_col in xgb_output.columns:\n",
    "            mean_pred = xgb_output[pred_col].mean()\n",
    "            max_pred = xgb_output[pred_col].max()\n",
    "            min_pred = xgb_output[pred_col].min()\n",
    "            logger.info(f\"  - {bucket}: Mean={mean_pred:.2f}, Min={min_pred:.2f}, Max={max_pred:.2f}\")\n",
    "\n",
    "    logger.info(f\"Uploading {len(xgb_output):,} rows to XGBoost table...\")\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    xgb_output.to_gbq(\n",
    "        xgb_table,\n",
    "        project_id=PROJECT_ID,\n",
    "        if_exists='replace'\n",
    "    )\n",
    "\n",
    "    upload_time = (datetime.now() - start_time).total_seconds()\n",
    "    logger.info(f\"XGBoost predictions saved successfully in {upload_time:.1f} seconds\")\n",
    "    logger.info(f\"Upload rate: {len(xgb_output)/upload_time:.0f} rows/second\")\n",
    "\n",
    "    # Save KNN predictions\n",
    "    logger.info(f\"Preparing KNN predictions for upload...\")\n",
    "    knn_output = predictions_df[knn_cols].copy()\n",
    "\n",
    "    # Log KNN prediction statistics\n",
    "    logger.info(f\"KNN prediction statistics:\")\n",
    "    for bucket in ['y_single', 'y_small', 'y_medium', 'y_large']:\n",
    "        pred_col = f'knn_pred_{bucket}'\n",
    "        if pred_col in knn_output.columns:\n",
    "            mean_pred = knn_output[pred_col].mean()\n",
    "            max_pred = knn_output[pred_col].max()\n",
    "            min_pred = knn_output[pred_col].min()\n",
    "            logger.info(f\"  - {bucket}: Mean={mean_pred:.2f}, Min={min_pred:.2f}, Max={max_pred:.2f}\")\n",
    "\n",
    "    logger.info(f\"Uploading {len(knn_output):,} rows to KNN table...\")\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    knn_output.to_gbq(\n",
    "        knn_table,\n",
    "        project_id=PROJECT_ID,\n",
    "        if_exists='replace'\n",
    "    )\n",
    "\n",
    "    upload_time = (datetime.now() - start_time).total_seconds()\n",
    "    logger.info(f\"KNN predictions saved successfully in {upload_time:.1f} seconds\")\n",
    "    logger.info(f\"Upload rate: {len(knn_output)/upload_time:.0f} rows/second\")\n",
    "\n",
    "    # Save metrics (unchanged)\n",
    "    logger.info(f\"Preparing metrics for {len(all_metrics)} zones...\")\n",
    "    metrics_rows = []\n",
    "\n",
    "    # Track best performing zones\n",
    "    zone_performance = {}\n",
    "\n",
    "    for zone_id, zone_metrics in all_metrics.items():\n",
    "        zone_avg_smape = {'xgb': 0, 'knn': 0}\n",
    "\n",
    "        for model_type in ['xgb', 'knn']:\n",
    "            for target, metrics in zone_metrics[model_type].items():\n",
    "                metrics_rows.append({\n",
    "                    'taxi_type': taxi_type,\n",
    "                    'zone_id': zone_id,\n",
    "                    'model_type': model_type,\n",
    "                    'target': target,\n",
    "                    'mae': metrics['mae'],\n",
    "                    'rmse': metrics['rmse'],\n",
    "                    'smape': metrics['smape'],\n",
    "                    'timestamp': datetime.now()\n",
    "                })\n",
    "                zone_avg_smape[model_type] += metrics['smape']\n",
    "\n",
    "        # Calculate average SMAPE for zone\n",
    "        zone_performance[zone_id] = {\n",
    "            'xgb_avg_smape': zone_avg_smape['xgb'] / 4,\n",
    "            'knn_avg_smape': zone_avg_smape['knn'] / 4\n",
    "        }\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_rows)\n",
    "\n",
    "    logger.info(f\"Metrics summary:\")\n",
    "    logger.info(f\"  - Total metric rows: {len(metrics_df)}\")\n",
    "    logger.info(f\"  - Unique zones: {metrics_df['zone_id'].nunique()}\")\n",
    "    logger.info(f\"  - Model types: {metrics_df['model_type'].unique()}\")\n",
    "    logger.info(f\"  - Targets: {metrics_df['target'].unique()}\")\n",
    "\n",
    "    # Log overall performance statistics\n",
    "    for model_type in ['xgb', 'knn']:\n",
    "        model_metrics = metrics_df[metrics_df['model_type'] == model_type]\n",
    "        logger.info(f\"\\n{model_type.upper()} overall performance:\")\n",
    "        logger.info(f\"  - Mean MAE: {model_metrics['mae'].mean():.2f}\")\n",
    "        logger.info(f\"  - Mean RMSE: {model_metrics['rmse'].mean():.2f}\")\n",
    "        logger.info(f\"  - Mean SMAPE: {model_metrics['smape'].mean():.1f}%\")\n",
    "\n",
    "    logger.info(f\"\\nUploading {len(metrics_df)} metric rows...\")\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    metrics_df.to_gbq(\n",
    "        metrics_table,\n",
    "        project_id=PROJECT_ID,\n",
    "        if_exists='replace'\n",
    "    )\n",
    "\n",
    "    upload_time = (datetime.now() - start_time).total_seconds()\n",
    "    logger.info(f\"Metrics saved successfully in {upload_time:.1f} seconds\")\n",
    "\n",
    "    # Log top performing zones\n",
    "    sorted_zones = sorted(zone_performance.items(),\n",
    "                         key=lambda x: min(x[1]['xgb_avg_smape'], x[1]['knn_avg_smape']))\n",
    "\n",
    "    logger.info(f\"\\nTop 3 best performing zones:\")\n",
    "    for zone_id, perf in sorted_zones[:3]:\n",
    "        logger.info(f\"  - Zone {zone_id}: XGB SMAPE={perf['xgb_avg_smape']:.1f}%, KNN SMAPE={perf['knn_avg_smape']:.1f}%\")\n",
    "\n",
    "    # Data quality check\n",
    "    logger.info(f\"\\nData quality check:\")\n",
    "    logger.info(f\"  - Predictions with NaN values: {predictions_df.isna().sum().sum()}\")\n",
    "\n",
    "    # Check only prediction columns for negative values\n",
    "    pred_cols = [col for col in predictions_df.columns if 'pred_' in col]\n",
    "    if pred_cols:\n",
    "        negative_count = (predictions_df[pred_cols] < 0).sum().sum()\n",
    "        logger.info(f\"  - Negative predictions: {negative_count}\")\n",
    "    else:\n",
    "        logger.info(f\"  - No prediction columns found\")\n",
    "\n",
    "    logger.info(f\"  - Zones with complete predictions: {len(all_predictions)}/{len(all_metrics)}\")\n",
    "\n",
    "    logger.info(f\"=\"*80)\n",
    "    logger.info(f\"All results saved successfully for {taxi_type} taxi\")\n",
    "    logger.info(f\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qSvZU_1O8jix",
   "metadata": {
    "id": "qSvZU_1O8jix"
   },
   "outputs": [],
   "source": [
    "def save_models_to_gcs(zone_models, taxi_type):\n",
    "    \"\"\"Save trained models to GCS.\"\"\"\n",
    "    bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "\n",
    "    for zone_id, models in tqdm(zone_models.items()):\n",
    "        # Save XGBoost model\n",
    "        xgb_path = f\"{MODEL_FOLDER}/{taxi_type}/zone_{zone_id}/xgb_model.joblib\"\n",
    "        print(f\"saveing {xgb_path}\")\n",
    "        blob = bucket.blob(xgb_path)\n",
    "        with blob.open('wb') as f:\n",
    "            joblib.dump(models['xgb'], f)\n",
    "\n",
    "        # Save KNN model\n",
    "        knn_path = f\"{MODEL_FOLDER}/{taxi_type}/zone_{zone_id}/knn_model.joblib\"\n",
    "        print(f\"saveing {knn_path}\")\n",
    "        blob = bucket.blob(knn_path)\n",
    "        with blob.open('wb') as f:\n",
    "            joblib.dump(models['knn'], f)\n",
    "\n",
    "        # Save preprocessor\n",
    "        preprocessor_path = f\"{MODEL_FOLDER}/{taxi_type}/zone_{zone_id}/preprocessor.joblib\"\n",
    "        blob = bucket.blob(preprocessor_path)\n",
    "        with blob.open('wb') as f:\n",
    "            joblib.dump(models['preprocessor'], f)\n",
    "\n",
    "    logger.info(f\"Saved {len(zone_models)} zone models to GCS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7JqHmlri8lPf",
   "metadata": {
    "id": "7JqHmlri8lPf"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main pipeline execution.\"\"\"\n",
    "    logger.info(\"Starting NYC Taxi Zone-Based Passenger Predictor Pipeline\")\n",
    "\n",
    "    for taxi_type in tqdm(TAXI_TYPES):\n",
    "        logger.info(f\"\\nProcessing {taxi_type} taxi...\")\n",
    "\n",
    "        try:\n",
    "            # Extract features\n",
    "            df = extract_features_from_bigquery(taxi_type)\n",
    "            df = prepare_features(df)\n",
    "\n",
    "            # Train models for all zones\n",
    "            zone_models, all_metrics, residual_stds, last_date, test_predictions  = train_all_zones(df, taxi_type)\n",
    "\n",
    "            # Generate predictions for each zone\n",
    "            all_predictions = []\n",
    "            for zone_id in tqdm(zone_models.keys(), desc=\"Generating predictions\"):\n",
    "                zone_data = df[df['zone_id'] == zone_id]\n",
    "                zone_pred = predict_future_iterative(\n",
    "                    zone_data, zone_models, zone_id, last_date, residual_stds, FORECAST_DAYS\n",
    "                )\n",
    "                if zone_pred is not None:\n",
    "                    all_predictions.append(zone_pred)\n",
    "\n",
    "            # Save results\n",
    "            save_results(all_predictions, all_metrics, test_predictions, taxi_type)\n",
    "            save_models_to_gcs(zone_models, taxi_type)\n",
    "\n",
    "            logger.info(f\"Completed processing for {taxi_type} taxi\")\n",
    "            print(f\"Completed processing for {taxi_type} taxi\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {taxi_type}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    logger.info(\"\\nPipeline completed successfully!\")\n",
    "\n",
    "    # Create summary views with confidence intervals\n",
    "    for model_type in ['xgb', 'knn']:\n",
    "        summary_query = f\"\"\"\n",
    "        CREATE OR REPLACE VIEW `{PROJECT_ID}.{OUTPUT_DATASET}.fleet_recommender_summary_new_{model_type}` AS\n",
    "        WITH combined AS (\n",
    "            SELECT * FROM `{PROJECT_ID}.{OUTPUT_DATASET}.fleet_recommender_yellow_predictions_new_{model_type}`\n",
    "            UNION ALL\n",
    "            SELECT * FROM `{PROJECT_ID}.{OUTPUT_DATASET}.fleet_recommender_green_predictions_new_{model_type}`\n",
    "        )\n",
    "        SELECT\n",
    "            taxi_type,\n",
    "            date,\n",
    "            SUM({model_type}_pred_y_single) as total_single_passengers,\n",
    "            SUM({model_type}_pred_y_small) as total_small_groups,\n",
    "            SUM({model_type}_pred_y_medium) as total_medium_groups,\n",
    "            SUM({model_type}_pred_y_large) as total_large_groups,\n",
    "            SUM({model_type}_pred_y_single + {model_type}_pred_y_small +\n",
    "                {model_type}_pred_y_medium + {model_type}_pred_y_large) as total_expected_trips,\n",
    "            -- Confidence intervals\n",
    "            SUM({model_type}_pred_y_single_lower + {model_type}_pred_y_small_lower +\n",
    "                {model_type}_pred_y_medium_lower + {model_type}_pred_y_large_lower) as total_expected_trips_lower,\n",
    "            SUM({model_type}_pred_y_single_upper + {model_type}_pred_y_small_upper +\n",
    "                {model_type}_pred_y_medium_upper + {model_type}_pred_y_large_upper) as total_expected_trips_upper,\n",
    "            COUNT(DISTINCT zone_id) as active_zones\n",
    "        FROM combined\n",
    "        GROUP BY taxi_type, date\n",
    "        ORDER BY taxi_type, date\n",
    "        \"\"\"\n",
    "\n",
    "        bq_client.query(summary_query).result()\n",
    "        logger.info(f\"Created {model_type} summary view with confidence intervals\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xcpyg7qn8nlX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xcpyg7qn8nlX",
    "outputId": "b605e613-1850-4e0f-8f36-95b973c74db9"
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "04aXGBoostKNNZoneWiseFleetRecommender",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
