{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qf9_RjoBJTpb"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import TimestampType, IntegerType, StringType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 35617,
     "status": "ok",
     "timestamp": 1757013694440,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "p2NN8wwF7YRa",
    "outputId": "f5399f49-6d91-487c-a8fa-f93758cce720"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import re\n",
    "import grpc\n",
    "from google.api_core.exceptions import GoogleAPICallError, RetryError, ServiceUnavailable\n",
    "from google.cloud.dataproc_spark_connect import DataprocSparkSession\n",
    "from pyspark.errors import PySparkValueError\n",
    "\n",
    "_QUOTA_PAT = re.compile(r\"insufficient.*cpus.*quota\", re.IGNORECASE)\n",
    "\n",
    "def _is_quota_error(exc: Exception) -> bool:\n",
    "    msg = str(exc) or \"\"\n",
    "    return bool(_QUOTA_PAT.search(msg))\n",
    "\n",
    "def get_spark_with_retry(\n",
    "    gentle_delay: float = 10.0,     # fixed delay for the first few tries\n",
    "    gentle_retries: int = 3,        # how many tries use gentle delay\n",
    "    max_backoff: float = 60.0,      # cap for exponential backoff\n",
    "    quota_wait: float = 90.0,       # wait longer when we hit a QUOTA error\n",
    "    jitter: bool = True,\n",
    "    verify_ready: bool = True,\n",
    "    max_attempts: int | None = None # None = infinite\n",
    "):\n",
    "    \"\"\"\n",
    "    Keep retrying until DataprocSparkSession is usable.\n",
    "    - QUOTA errors: wait `quota_wait` seconds and retry (lets other jobs finish).\n",
    "    - Other errors: gentle linear delay for first `gentle_retries`, then exponential backoff.\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    backoff = gentle_delay\n",
    "\n",
    "    while True:\n",
    "        attempt += 1\n",
    "        try:\n",
    "            spark = DataprocSparkSession.builder.getOrCreate()\n",
    "            # Let backend settle a moment before the test query\n",
    "            time.sleep(3)\n",
    "\n",
    "            if verify_ready:\n",
    "                _ = spark.range(1).count()\n",
    "\n",
    "            print(f\"Spark connected on attempt {attempt}\")\n",
    "            return spark\n",
    "\n",
    "        except (ServiceUnavailable, GoogleAPICallError, RetryError,\n",
    "                grpc.RpcError, PySparkValueError, RuntimeError) as e:\n",
    "            # QUOTA-specific path: someone else might be using CPUs. Wait longer, then retry.\n",
    "            if _is_quota_error(e):\n",
    "                wait = quota_wait + (random.uniform(0, quota_wait/3) if jitter else 0.0)\n",
    "                print(f\"⏳ Quota limited (likely other sessions using CPUs). \"\n",
    "                      f\"Sleeping {wait:.1f}s, then retrying…\\nDetails: {e}\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                # Non-quota transient errors\n",
    "                if attempt <= gentle_retries:\n",
    "                    wait = gentle_delay\n",
    "                else:\n",
    "                    # exponential backoff capped\n",
    "                    backoff = min(backoff * 2, max_backoff)\n",
    "                    wait = backoff\n",
    "                if jitter:\n",
    "                    wait += random.uniform(0, wait/2)\n",
    "                print(f\" Attempt {attempt} failed: {e!r}\\n   Sleeping {wait:.1f}s before retry…\")\n",
    "                time.sleep(wait)\n",
    "\n",
    "            if max_attempts and attempt >= max_attempts:\n",
    "                raise RuntimeError(f\" Failed after {attempt} attempts\") from e\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\" Stopped by user.\")\n",
    "            raise\n",
    "\n",
    "# --- Usage ---\n",
    "spark = get_spark_with_retry()\n",
    "\n",
    "# Safe to use:\n",
    "df = spark.range(5)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EP4968iCHCFp"
   },
   "source": [
    "# -- config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1757013694441,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "qIqFGTsWHDmY"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"nyctaxi-467111\"\n",
    "BUCKET_NAME = \"nyc_raw_data_bucket\"\n",
    "RAW_BUCKET = f\"gs://{BUCKET_NAME}\"\n",
    "BRONZE_DATASET_NAME = \"RawBronze\"\n",
    "SILVER_DATASET_NAME = \"CleanSilver\"\n",
    "TAXI_TYPES = [\"yellow\", \"green\", \"fhv\", \"fhvhv\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKz0L1BTG20q"
   },
   "source": [
    "# -- logger --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1757013694441,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "DdEDiaKeGvXR",
    "outputId": "3a02abcf-bafd-49c2-d1af-b23efea1c5f1"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "from google.cloud import storage\n",
    "\n",
    "# -------------------------\n",
    "# Init runtime log file\n",
    "# -------------------------\n",
    "run_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_filename = f\"02aYellowRawBronzeToCleanSilver_{run_time}.log\"\n",
    "local_log_path = f\"/tmp/{log_filename}\"     # Local temporary file\n",
    "gcs_log_path = f\"logs/{log_filename}\"       # Final path in GCS\n",
    "\n",
    "logger = logging.getLogger(\"YellowRawBronzeToCleanSilver\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Prevent duplicate handlers on re-run\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "\n",
    "# File handler (local file)\n",
    "fh = logging.FileHandler(local_log_path)\n",
    "fh.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "# Console handler (notebook output)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "logger.info(f\"Initialized runtime logger: {gcs_log_path}\")\n",
    "\n",
    "# -------------------------\n",
    "# Helper to flush log to GCS\n",
    "# -------------------------\n",
    "def upload_log_to_gcs():\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(BUCKET_NAME)\n",
    "    blob = bucket.blob(gcs_log_path)\n",
    "    blob.upload_from_filename(local_log_path)\n",
    "    logger.info(f\"Uploaded log file to gs://{BUCKET_NAME}/{gcs_log_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4QwGscYHj85"
   },
   "source": [
    "# -- Data to Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1757013694442,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "0UllZZEFHnEH"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "def list_tables(dataset_name, prefix=\"yellow_\"):\n",
    "    \"\"\"\n",
    "    List tables in a dataset with a given prefix.\n",
    "    Returns normalized year_month strings (yyyy-mm).\n",
    "    \"\"\"\n",
    "    tables = bq_client.list_tables(dataset_name)\n",
    "    ym_list = []\n",
    "    for table in tables:\n",
    "        table_name = table.table_id\n",
    "        if table_name.startswith(prefix):\n",
    "            # Example: yellow_2022_01 → 2022-01\n",
    "            ym = table_name.replace(prefix, \"\").replace(\"_\", \"-\")\n",
    "            ym_list.append(ym)\n",
    "    return sorted(ym_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1285,
     "status": "ok",
     "timestamp": 1757013695725,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "Rp7_fMX_HoOY",
    "outputId": "4f360edd-eb20-41da-dde5-b43aab1a7d45"
   },
   "outputs": [],
   "source": [
    "bronze_list = list_tables(BRONZE_DATASET_NAME, prefix=\"yellow_\")\n",
    "silver_list = list_tables(SILVER_DATASET_NAME, prefix=\"yellow_\")\n",
    "\n",
    "print(\"Bronze available:\", bronze_list)\n",
    "print(\"Silver available:\", silver_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1757013695725,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "qRVb6C21H1aI",
    "outputId": "f02fd204-2a3c-4032-d094-47d48d511143"
   },
   "outputs": [],
   "source": [
    "missing_in_silver = sorted(set(bronze_list) - set(silver_list))\n",
    "\n",
    "print(\" Missing in Silver:\", missing_in_silver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvonMoY-HTfv"
   },
   "source": [
    "# -- main --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1757013695726,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "10gumrnFIy9T"
   },
   "outputs": [],
   "source": [
    "def log_and_print(msg: str, level=\"info\"):\n",
    "    if level == \"info\":\n",
    "        logger.info(msg)\n",
    "    elif level == \"error\":\n",
    "        logger.error(msg)\n",
    "    print(msg)  # ensures it always shows in notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 114207,
     "status": "ok",
     "timestamp": 1757013809930,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "VbIsNxXWHP3K",
    "outputId": "f889ce67-6586-4690-b4d8-3a8c695f14b1"
   },
   "outputs": [],
   "source": [
    "for year_month in missing_in_silver:\n",
    "    year, month = map(int, year_month.split(\"-\"))\n",
    "\n",
    "    try:\n",
    "        # logger.info\n",
    "        log_and_print(f\" Starting processing for {year_month}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Load Data FROM RawBronze\n",
    "        # -------------------------\n",
    "        bronze_table = f\"{PROJECT_ID}.{BRONZE_DATASET_NAME}.yellow_{year_month.replace('-', '_')}\"\n",
    "        df = spark.read.format(\"bigquery\").load(bronze_table)\n",
    "        # logger.info\n",
    "        log_and_print(f\"Loaded {df.count()} rows from {bronze_table}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Type casting\n",
    "        # -------------------------\n",
    "        if not isinstance(df.schema[\"tpep_pickup_datetime\"].dataType, TimestampType):\n",
    "            df = df.withColumn(\"tpep_pickup_datetime\", F.col(\"tpep_pickup_datetime\").cast(TimestampType()))\n",
    "        if not isinstance(df.schema[\"tpep_dropoff_datetime\"].dataType, TimestampType):\n",
    "            df = df.withColumn(\"tpep_dropoff_datetime\", F.col(\"tpep_dropoff_datetime\").cast(TimestampType()))\n",
    "\n",
    "        categorical_columns = ['VendorID', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type']\n",
    "        for col_name in categorical_columns:\n",
    "            if col_name in df.columns:\n",
    "                if col_name == 'store_and_fwd_flag':\n",
    "                    df = df.withColumn(col_name, F.col(col_name).cast(StringType()))\n",
    "                else:\n",
    "                    df = df.withColumn(col_name, F.col(col_name).cast(IntegerType()))\n",
    "\n",
    "        # logger.info\n",
    "        log_and_print(\"Completed type casting\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Drop null passenger_count\n",
    "        # -------------------------\n",
    "        before_count = df.count()\n",
    "        df = df.filter(F.col(\"passenger_count\").isNotNull())\n",
    "        log_and_print(f\"Dropped {before_count - df.count()} rows with null passenger_count\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Filter invalid times\n",
    "        # WHERE: pickup < dropoff\n",
    "        # -------------------------\n",
    "        before_count = df.count()\n",
    "        df = df.filter(F.col(\"tpep_pickup_datetime\") < F.col(\"tpep_dropoff_datetime\"))\n",
    "        log_and_print(f\"Dropped {before_count - df.count()} rows with invalid pickup/dropoff\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Filter invalid distances\n",
    "        # -------------------------\n",
    "        before_count = df.count()\n",
    "        df = df.filter(F.col(\"trip_distance\") > 0)\n",
    "        log_and_print(f\"Dropped {before_count - df.count()} rows with invalid distances\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Filter invalid passenger_count\n",
    "        # -------------------------\n",
    "        before_count = df.count()\n",
    "        df = df.filter(F.col(\"passenger_count\") > 0)\n",
    "        log_and_print(f\"Dropped {before_count - df.count()} rows with invalid passenger_count\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Filter invalid fare & total_amount\n",
    "        # -------------------------\n",
    "        before_count = df.count()\n",
    "        df = df.filter(F.col(\"fare_amount\") > 0)\n",
    "        log_and_print(f\"Dropped {before_count - df.count()} rows with invalid fare_amount\")\n",
    "\n",
    "        before_count = df.count()\n",
    "        df = df.filter(F.col(\"total_amount\") > 0)\n",
    "        log_and_print(f\"Dropped {before_count - df.count()} rows with invalid total_amount\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Outlier Removal (IQR for trip_distance)\n",
    "        # -------------------------\n",
    "        quantiles = df.approxQuantile(\"trip_distance\", [0.25, 0.75], 0.01)\n",
    "        Q1, Q3 = quantiles\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound, upper_bound = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "        before_count = df.count()\n",
    "        df = df.filter((F.col(\"trip_distance\") >= lower_bound) & (F.col(\"trip_distance\") <= upper_bound))\n",
    "        logger.info(f\"Dropped {before_count - df.count()} outliers from trip_distance\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Trip duration filter\n",
    "        # keep trips between 5 seconds and 2 hours.\n",
    "        # -------------------------\n",
    "        df = df.withColumn(\"trip_duration\", F.unix_timestamp(\"tpep_dropoff_datetime\") - F.unix_timestamp(\"tpep_pickup_datetime\"))\n",
    "        before_count = df.count()\n",
    "        df = df.filter((F.col(\"trip_duration\") >= 5) & (F.col(\"trip_duration\") <= 7200))\n",
    "        log_and_print(f\"Dropped {before_count - df.count()} rows with invalid trip_duration\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Filter only target year/month\n",
    "        # only rows where pickup datetime’s year/month matches the file’s expected YYYY-MM.\n",
    "        # -------------------------\n",
    "        df = df.withColumn(\"year\", F.year(\"tpep_pickup_datetime\")).withColumn(\"pickup_month\", F.month(\"tpep_pickup_datetime\"))\n",
    "        before_count = df.count()\n",
    "        df = df.filter((F.col(\"year\") == year) & (F.col(\"pickup_month\") == month))\n",
    "        log_and_print(f\"Filtered to target year/month, remaining rows: {df.count()}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Write to BigQuery Silver\n",
    "        # -------------------------\n",
    "        silver_table = f\"{PROJECT_ID}.{SILVER_DATASET_NAME}.yellow_{year_month.replace('-', '_')}\"\n",
    "        df.write.format(\"bigquery\").option(\"table\", silver_table).mode(\"overwrite\").save()\n",
    "        log_and_print(f\"Written to BigQuery table {silver_table}\")\n",
    "\n",
    "        log_and_print(f\" Finished processing for {year_month}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log_and_print(f\" ERROR processing {year_month}: {str(e)}\",\"error\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5m0SIZYFLJCi"
   },
   "source": [
    "# -------------------------\n",
    "# Final log flush\n",
    "# -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1757013809931,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "vJhheDNoJjFP",
    "outputId": "f7cc0f6e-f52e-4267-8416-4a655d17e481"
   },
   "outputs": [],
   "source": [
    "upload_log_to_gcs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 61543,
     "status": "ok",
     "timestamp": 1757013871473,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "N3nWHSOnLKnD",
    "outputId": "38f6dc7d-beca-4b9b-9dc9-bf92039b4949"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Stop the Spark session gracefully\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\" Spark session stopped\")\n",
    "except Exception as e:\n",
    "    print(f\" Error while stopping Spark session: {e}\")\n",
    "\n",
    "# Sleep for 60 seconds to allow quota/resources to free up\n",
    "print(\" Waiting 60s for resources to be released...\")\n",
    "time.sleep(60)\n",
    "print(\" Done waiting. You can safely start a new session now.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "cell_execution_strategy": "setup",
   "name": "02aYellowRawBronzeToCleanSilver",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
