{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "kdrAP4U8Q_qx",
   "metadata": {
    "id": "kdrAP4U8Q_qx"
   },
   "source": [
    "\"\"\"\n",
    "\n",
    "NYC Taxi Anomaly Detection Pipeline\n",
    "Processes each month using previous months' data for training\n",
    "Stores results in BigQuery anomaly tables\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1nxeanDtEvxqb7dH4131hAql",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "executionInfo": {
     "elapsed": 6260,
     "status": "ok",
     "timestamp": 1756461566998,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "1nxeanDtEvxqb7dH4131hAql",
    "outputId": "d89759c1-4ed9-4c82-8802-75e206606f2d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_5ZatkI1RJej",
   "metadata": {
    "id": "_5ZatkI1RJej"
   },
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l3wjeJQSRFw6",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1756461594131,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "l3wjeJQSRFw6"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "PROJECT_ID = \"nyctaxi-467111\"\n",
    "DATASET_NAME = \"PreMlGold\"\n",
    "ANOMALY_DATASET = \"PostMlGold\"  # Dataset for anomaly results\n",
    "TAXI_TYPES = [\"yellow\", \"green\", \"fhv\", \"fhvhv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "M51H-ZFnRPQo",
   "metadata": {
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1756461603207,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "M51H-ZFnRPQo"
   },
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-Hkn7hsoRRYY",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1756461609119,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "-Hkn7hsoRRYY"
   },
   "outputs": [],
   "source": [
    "# Initialize BigQuery client\n",
    "client = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YEhxnZJARS1A",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1756461633844,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "YEhxnZJARS1A"
   },
   "outputs": [],
   "source": [
    "def get_available_partitions(taxi_type: str) -> List[str]:\n",
    "    \"\"\"Get available year_month partitions for a taxi type.\"\"\"\n",
    "    query = f\"\"\"\n",
    "    SELECT table_name\n",
    "    FROM `{PROJECT_ID}.{DATASET_NAME}.INFORMATION_SCHEMA.TABLES`\n",
    "    WHERE REGEXP_CONTAINS(table_name, r'^{taxi_type}_[0-9]{{4}}_[0-9]{{2}}_hourly$')\n",
    "    \"\"\"\n",
    "\n",
    "    results = client.query(query).result()\n",
    "    partitions = []\n",
    "\n",
    "    import re\n",
    "    pattern = re.compile(rf'^{taxi_type}_(\\d{{4}})_(\\d{{2}})_hourly$')\n",
    "\n",
    "    for row in results:\n",
    "        match = pattern.match(row.table_name)\n",
    "        if match:\n",
    "            year, month = match.groups()\n",
    "            partitions.append(f\"{year}_{month}\")\n",
    "\n",
    "    return sorted(partitions)\n",
    "\n",
    "def get_processed_months(taxi_type: str) -> List[str]:\n",
    "    \"\"\"Get months already processed in anomaly table.\"\"\"\n",
    "    anomaly_table = f\"{PROJECT_ID}.{ANOMALY_DATASET}.{taxi_type}_anomalies\"\n",
    "\n",
    "    try:\n",
    "        query = f\"\"\"\n",
    "        SELECT DISTINCT\n",
    "            FORMAT_DATETIME('%Y_%m', datetime) as year_month\n",
    "        FROM `{anomaly_table}`\n",
    "        ORDER BY year_month\n",
    "        \"\"\"\n",
    "\n",
    "        results = client.query(query).result()\n",
    "        return [row.year_month for row in results]\n",
    "    except:\n",
    "        # Table doesn't exist yet\n",
    "        return []\n",
    "\n",
    "def load_monthly_data(taxi_type: str, partition: str) -> pd.DataFrame:\n",
    "    \"\"\"Load data for a specific month.\"\"\"\n",
    "    table_name = f\"{taxi_type}_{partition}_hourly\"\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        DATETIME(pickup_date, TIME(pickup_hour, 0, 0)) as datetime,\n",
    "        trips\n",
    "    FROM `{PROJECT_ID}.{DATASET_NAME}.{table_name}`\n",
    "    ORDER BY datetime\n",
    "    \"\"\"\n",
    "\n",
    "    df = client.query(query).to_dataframe()\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    return df\n",
    "\n",
    "def load_training_data(taxi_type: str, partitions: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Load and combine data from multiple partitions for training.\"\"\"\n",
    "    dfs = []\n",
    "    for partition in partitions:\n",
    "        df = load_monthly_data(taxi_type, partition)\n",
    "        dfs.append(df)\n",
    "\n",
    "    combined_df = pd.concat(dfs, axis=0).sort_index()\n",
    "    return combined_df\n",
    "\n",
    "def create_sequences(data: np.ndarray, window_size: int) -> np.ndarray:\n",
    "    \"\"\"Create sequences for autoencoder training.\"\"\"\n",
    "    sequences = []\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        sequences.append(data[i:i + window_size])\n",
    "    return np.array(sequences)\n",
    "\n",
    "def build_autoencoder(window_size: int):\n",
    "    \"\"\"Build autoencoder model for anomaly detection.\"\"\"\n",
    "    input_layer = Input(shape=(window_size,))\n",
    "\n",
    "    # Encoder\n",
    "    encoded = Dense(64, activation='relu')(input_layer)\n",
    "    encoded = Dense(32, activation='relu')(encoded)\n",
    "    encoded = Dense(16, activation='relu')(encoded)\n",
    "\n",
    "    # Decoder\n",
    "    decoded = Dense(32, activation='relu')(encoded)\n",
    "    decoded = Dense(64, activation='relu')(decoded)\n",
    "    output_layer = Dense(window_size, activation='sigmoid')(decoded)\n",
    "\n",
    "    autoencoder = Model(input_layer, output_layer)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return autoencoder\n",
    "\n",
    "def detect_anomalies_for_month(\n",
    "    train_data: pd.DataFrame,\n",
    "    test_data: pd.DataFrame,\n",
    "    window_size: int = 24 * 7  # 1 week\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Detect anomalies for a specific month using autoencoder.\"\"\"\n",
    "\n",
    "    # Prepare data\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit scaler on training data\n",
    "    train_scaled = scaler.fit_transform(train_data[['trips']].values)\n",
    "    test_scaled = scaler.transform(test_data[['trips']].values)\n",
    "\n",
    "    # Create sequences for training\n",
    "    train_sequences = create_sequences(train_scaled.flatten(), window_size)\n",
    "\n",
    "    if len(train_sequences) < 100:\n",
    "        logger.warning(\"Insufficient training data for robust anomaly detection\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Build and train model\n",
    "    autoencoder = build_autoencoder(window_size)\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
    "    history = autoencoder.fit(\n",
    "        train_sequences, train_sequences,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        verbose=0,\n",
    "        callbacks=[early_stop],\n",
    "        validation_split=0.1\n",
    "    )\n",
    "\n",
    "    # Create sequences for testing\n",
    "    test_sequences = create_sequences(test_scaled.flatten(), window_size)\n",
    "\n",
    "    # Predict and calculate reconstruction error\n",
    "    if len(test_sequences) > 0:\n",
    "        predictions = autoencoder.predict(test_sequences, verbose=0)\n",
    "        mse = np.mean((test_sequences - predictions) ** 2, axis=1)\n",
    "\n",
    "        # Set threshold (using training data statistics)\n",
    "        train_predictions = autoencoder.predict(train_sequences, verbose=0)\n",
    "        train_mse = np.mean((train_sequences - train_predictions) ** 2, axis=1)\n",
    "        threshold = np.mean(train_mse) + 2.5 * np.std(train_mse)\n",
    "\n",
    "        # Identify anomalies\n",
    "        anomalies = mse > threshold\n",
    "\n",
    "        # Create results dataframe\n",
    "        results = pd.DataFrame({\n",
    "            'datetime': test_data.index[window_size-1:],\n",
    "            'trips': test_data['trips'].iloc[window_size-1:].values,\n",
    "            'reconstruction_error': mse,\n",
    "            'threshold': threshold,\n",
    "            'is_anomaly': anomalies,\n",
    "            'anomaly_score': mse / threshold  # Normalized score\n",
    "        })\n",
    "\n",
    "        return results\n",
    "\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def save_anomalies_to_bq(results: pd.DataFrame, taxi_type: str, year_month: str):\n",
    "    \"\"\"Save anomaly detection results to BigQuery.\"\"\"\n",
    "    if results.empty:\n",
    "        logger.warning(f\"No results to save for {taxi_type} {year_month}\")\n",
    "        return\n",
    "\n",
    "    # Add metadata\n",
    "    results['taxi_type'] = taxi_type\n",
    "    results['year_month'] = year_month\n",
    "    results['processing_timestamp'] = datetime.now()\n",
    "\n",
    "    # Table name\n",
    "    table_id = f\"{PROJECT_ID}.{ANOMALY_DATASET}.{taxi_type}_anomalies\"\n",
    "\n",
    "    # Configure job\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
    "        schema=[\n",
    "            bigquery.SchemaField(\"datetime\", \"TIMESTAMP\"),\n",
    "            bigquery.SchemaField(\"trips\", \"INTEGER\"),\n",
    "            bigquery.SchemaField(\"reconstruction_error\", \"FLOAT\"),\n",
    "            bigquery.SchemaField(\"threshold\", \"FLOAT\"),\n",
    "            bigquery.SchemaField(\"is_anomaly\", \"BOOLEAN\"),\n",
    "            bigquery.SchemaField(\"anomaly_score\", \"FLOAT\"),\n",
    "            bigquery.SchemaField(\"taxi_type\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"year_month\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"processing_timestamp\", \"TIMESTAMP\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Load data\n",
    "    job = client.load_table_from_dataframe(results, table_id, job_config=job_config)\n",
    "    job.result()  # Wait for job to complete\n",
    "\n",
    "    logger.info(f\"Saved {len(results)} records to {table_id}\")\n",
    "\n",
    "def process_taxi_type(taxi_type: str, min_training_months: int = 3):\n",
    "    \"\"\"Process all unprocessed months for a taxi type.\"\"\"\n",
    "    logger.info(f\"Processing {taxi_type} taxi...\")\n",
    "\n",
    "    # Get available and processed partitions\n",
    "    available_partitions = get_available_partitions(taxi_type)\n",
    "    processed_partitions = get_processed_months(taxi_type)\n",
    "\n",
    "    logger.info(f\"Available partitions: {len(available_partitions)}\")\n",
    "    logger.info(f\"Already processed: {len(processed_partitions)}\")\n",
    "\n",
    "    # Process each month\n",
    "    for i, test_partition in enumerate(available_partitions):\n",
    "        if test_partition in processed_partitions:\n",
    "            logger.info(f\"Skipping {test_partition} - already processed\")\n",
    "            continue\n",
    "\n",
    "        # Need at least min_training_months for training\n",
    "        if i < min_training_months:\n",
    "            logger.info(f\"Skipping {test_partition} - insufficient training history\")\n",
    "            continue\n",
    "\n",
    "        # Get training partitions (all previous months)\n",
    "        train_partitions = available_partitions[:i]\n",
    "\n",
    "        logger.info(f\"Processing {test_partition} using {len(train_partitions)} months for training\")\n",
    "\n",
    "        try:\n",
    "            # Load data\n",
    "            train_data = load_training_data(taxi_type, train_partitions[-6:])  # Use last 6 months for efficiency\n",
    "            test_data = load_monthly_data(taxi_type, test_partition)\n",
    "\n",
    "            # Detect anomalies\n",
    "            results = detect_anomalies_for_month(train_data, test_data)\n",
    "\n",
    "            if not results.empty:\n",
    "                # Save to BigQuery\n",
    "                save_anomalies_to_bq(results, taxi_type, test_partition)\n",
    "                logger.info(f\"Successfully processed {test_partition}: {results['is_anomaly'].sum()} anomalies found\")\n",
    "            else:\n",
    "                logger.warning(f\"No results generated for {test_partition}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {test_partition}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "def create_summary_view(taxi_type: str):\n",
    "    \"\"\"Create a summary view of anomalies by month.\"\"\"\n",
    "    query = f\"\"\"\n",
    "    CREATE OR REPLACE VIEW `{PROJECT_ID}.{ANOMALY_DATASET}.{taxi_type}_anomaly_summary` AS\n",
    "    SELECT\n",
    "        year_month,\n",
    "        COUNT(*) as total_hours,\n",
    "        SUM(CAST(is_anomaly AS INT64)) as anomaly_count,\n",
    "        ROUND(AVG(CAST(is_anomaly AS INT64)) * 100, 2) as anomaly_percentage,\n",
    "        AVG(trips) as avg_trips,\n",
    "        MAX(trips) as max_trips,\n",
    "        AVG(CASE WHEN is_anomaly THEN anomaly_score END) as avg_anomaly_score\n",
    "    FROM `{PROJECT_ID}.{ANOMALY_DATASET}.{taxi_type}_anomalies`\n",
    "    GROUP BY year_month\n",
    "    ORDER BY year_month\n",
    "    \"\"\"\n",
    "\n",
    "    client.query(query).result()\n",
    "    logger.info(f\"Created summary view for {taxi_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_LgbfdrFRY8Q",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1756461645560,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "_LgbfdrFRY8Q"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main pipeline execution.\"\"\"\n",
    "    logger.info(\"Starting NYC Taxi Anomaly Detection Pipeline\")\n",
    "\n",
    "    for taxi_type in TAXI_TYPES:\n",
    "        logger.info(f\"\\n{'='*50}\")\n",
    "        logger.info(f\"Processing {taxi_type.upper()} taxi\")\n",
    "        logger.info(f\"{'='*50}\")\n",
    "\n",
    "        try:\n",
    "            # Process all unprocessed months\n",
    "            process_taxi_type(taxi_type)\n",
    "\n",
    "            # Create summary view\n",
    "            create_summary_view(taxi_type)\n",
    "\n",
    "            logger.info(f\"Completed processing for {taxi_type}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process {taxi_type}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    logger.info(\"\\nPipeline completed!\")\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n\\nSUMMARY REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    for taxi_type in TAXI_TYPES:\n",
    "        try:\n",
    "            query = f\"\"\"\n",
    "            SELECT\n",
    "                COUNT(DISTINCT year_month) as months_processed,\n",
    "                SUM(CAST(is_anomaly AS INT64)) as total_anomalies\n",
    "            FROM `{PROJECT_ID}.{ANOMALY_DATASET}.{taxi_type}_anomalies`\n",
    "            \"\"\"\n",
    "\n",
    "            result = list(client.query(query).result())[0]\n",
    "            print(f\"{taxi_type.upper():>6}: {result.months_processed} months, {result.total_anomalies:,} anomalies\")\n",
    "        except:\n",
    "            print(f\"{taxi_type.upper():>6}: No data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7SjtsqGARbzC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 744339,
     "status": "ok",
     "timestamp": 1756465844809,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "7SjtsqGARbzC",
    "outputId": "6d6622bf-c362-4eea-de98-65f5d826eedf"
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KMO2pTpVReN4",
   "metadata": {
    "id": "KMO2pTpVReN4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
