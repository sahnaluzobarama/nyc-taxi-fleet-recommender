{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CaLeNMLVcVY5"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StringType, IntegerType, TimestampType, StructType, StructField\n",
    ")\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "executionInfo": {
     "elapsed": 121937,
     "status": "ok",
     "timestamp": 1755828954925,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "p2NN8wwF7YRa",
    "outputId": "72ae0181-4856-46e1-b3fc-ac1897bd0b10"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import re\n",
    "import grpc\n",
    "from google.api_core.exceptions import GoogleAPICallError, RetryError, ServiceUnavailable\n",
    "from google.cloud.dataproc_spark_connect import DataprocSparkSession\n",
    "from pyspark.errors import PySparkValueError\n",
    "\n",
    "_QUOTA_PAT = re.compile(r\"insufficient.*cpus.*quota\", re.IGNORECASE)\n",
    "\n",
    "def _is_quota_error(exc: Exception) -> bool:\n",
    "    msg = str(exc) or \"\"\n",
    "    return bool(_QUOTA_PAT.search(msg))\n",
    "\n",
    "def get_spark_with_retry(\n",
    "    gentle_delay: float = 10.0,     # fixed delay for the first few tries\n",
    "    gentle_retries: int = 3,        # how many tries use gentle delay\n",
    "    max_backoff: float = 60.0,      # cap for exponential backoff\n",
    "    quota_wait: float = 90.0,       # wait longer when we hit a QUOTA error\n",
    "    jitter: bool = True,\n",
    "    verify_ready: bool = True,\n",
    "    max_attempts: int | None = None # None = infinite\n",
    "):\n",
    "    \"\"\"\n",
    "    Keep retrying until DataprocSparkSession is usable.\n",
    "    - QUOTA errors: wait `quota_wait` seconds and retry (lets other jobs finish).\n",
    "    - Other errors: gentle linear delay for first `gentle_retries`, then exponential backoff.\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    backoff = gentle_delay\n",
    "\n",
    "    while True:\n",
    "        attempt += 1\n",
    "        try:\n",
    "            spark = DataprocSparkSession.builder.getOrCreate()\n",
    "            # Let backend settle a moment before the test query\n",
    "            time.sleep(3)\n",
    "\n",
    "            if verify_ready:\n",
    "                _ = spark.range(1).count()\n",
    "\n",
    "            print(f\" Spark connected on attempt {attempt}\")\n",
    "            return spark\n",
    "\n",
    "        except (ServiceUnavailable, GoogleAPICallError, RetryError,\n",
    "                grpc.RpcError, PySparkValueError, RuntimeError) as e:\n",
    "            # QUOTA-specific path: someone else might be using CPUs. Wait longer, then retry.\n",
    "            if _is_quota_error(e):\n",
    "                wait = quota_wait + (random.uniform(0, quota_wait/3) if jitter else 0.0)\n",
    "                print(f\"⏳ Quota limited (likely other sessions using CPUs). \"\n",
    "                      f\"Sleeping {wait:.1f}s, then retrying…\\nDetails: {e}\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                # Non-quota transient errors\n",
    "                if attempt <= gentle_retries:\n",
    "                    wait = gentle_delay\n",
    "                else:\n",
    "                    # exponential backoff capped\n",
    "                    backoff = min(backoff * 2, max_backoff)\n",
    "                    wait = backoff\n",
    "                if jitter:\n",
    "                    wait += random.uniform(0, wait/2)\n",
    "                print(f\"  Attempt {attempt} failed: {e!r}\\n   Sleeping {wait:.1f}s before retry…\")\n",
    "                time.sleep(wait)\n",
    "\n",
    "            if max_attempts and attempt >= max_attempts:\n",
    "                raise RuntimeError(f\" Failed after {attempt} attempts\") from e\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\" Stopped by user.\")\n",
    "            raise\n",
    "\n",
    "# --- Usage ---\n",
    "spark = get_spark_with_retry()\n",
    "\n",
    "# Safe to use:\n",
    "df = spark.range(5)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bB8ZnmE7ccMN"
   },
   "source": [
    "# -- config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ok_yw-kCcb8j"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"nyctaxi-467111\"\n",
    "BUCKET_NAME = \"nyc_raw_data_bucket\"\n",
    "RAW_BUCKET = f\"gs://{BUCKET_NAME}\"\n",
    "BRONZE_DATASET_NAME = \"RawBronze\"\n",
    "SILVER_DATASET_NAME = \"CleanSilver\"\n",
    "TAXI_TYPES = [\"yellow\", \"green\", \"fhv\", \"fhvhv\"]\n",
    "TABLE_PREFIX = \"fhv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRn_Bsb3chqV"
   },
   "source": [
    "# -- logger --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1755829649102,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "pyZvWwAoceyH",
    "outputId": "b1600d28-a5de-4004-f0ce-6c298ae62b79"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "from google.cloud import storage\n",
    "\n",
    "# -------------------------\n",
    "# Init runtime log file\n",
    "# -------------------------\n",
    "run_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_filename = f\"02cFHVRawBronzeToCleanSilver_{run_time}.log\"\n",
    "local_log_path = f\"/tmp/{log_filename}\"     # Local temporary file\n",
    "gcs_log_path = f\"logs/{log_filename}\"       # Final path in GCS\n",
    "\n",
    "logger = logging.getLogger(\"FHVRawBronzeToCleanSilver\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Prevent duplicate handlers on re-run\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "\n",
    "# File handler (local file)\n",
    "fh = logging.FileHandler(local_log_path)\n",
    "fh.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "# Console handler (notebook output)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "logger.info(f\"Initialized runtime logger: {gcs_log_path}\")\n",
    "\n",
    "def log_and_upload(msg: str):\n",
    "    logger.info(msg)\n",
    "    # Optionally copy local_log_path to GCS\n",
    "    os.system(f\"gsutil cp {local_log_path} gs://{BUCKET_NAME}/{gcs_log_path}\")\n",
    "\n",
    "# -------------------------\n",
    "# Helper to flush log to GCS\n",
    "# -------------------------\n",
    "def upload_log_to_gcs():\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(BUCKET_NAME)\n",
    "    blob = bucket.blob(gcs_log_path)\n",
    "    blob.upload_from_filename(local_log_path)\n",
    "    logger.info(f\"Uploaded log file to gs://{BUCKET_NAME}/{gcs_log_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_53KzQuIc2oH"
   },
   "source": [
    "# -- Data to Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ODd2uRxczP2"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "def list_tables(dataset_name, prefix=\"yellow_\"):\n",
    "    \"\"\"\n",
    "    List tables in a dataset with a given prefix.\n",
    "    Returns normalized year_month strings (yyyy-mm).\n",
    "    \"\"\"\n",
    "    tables = bq_client.list_tables(dataset_name)\n",
    "    ym_list = []\n",
    "    for table in tables:\n",
    "        table_name = table.table_id\n",
    "        if table_name.startswith(prefix):\n",
    "            # Example: yellow_2022_01 → 2022-01\n",
    "            ym = table_name.replace(prefix, \"\").replace(\"_\", \"-\")\n",
    "            ym_list.append(ym)\n",
    "    return sorted(ym_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 902,
     "status": "ok",
     "timestamp": 1755829652284,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "3sF7CqO-dAaH",
    "outputId": "c719560c-9dff-4f2f-a9a9-647a1a758b3d"
   },
   "outputs": [],
   "source": [
    "bronze_list = list_tables(BRONZE_DATASET_NAME, prefix=\"fhv_\")\n",
    "silver_list = list_tables(SILVER_DATASET_NAME, prefix=\"fhv_\")\n",
    "\n",
    "print(\"Bronze available:\", bronze_list)\n",
    "print(\"Silver available:\", silver_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1755829652285,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "gS1buiT2dEmu",
    "outputId": "25fb087b-1ef1-4b72-f75a-a93ebdfe3da0"
   },
   "outputs": [],
   "source": [
    "missing_in_silver = sorted(set(bronze_list) - set(silver_list))\n",
    "\n",
    "print(\" Missing in Silver:\", missing_in_silver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcJrWdnsdIou"
   },
   "source": [
    "# -- main --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0aWnUNhdGte"
   },
   "outputs": [],
   "source": [
    "def log_and_print(msg: str, level=\"info\"):\n",
    "    if level == \"info\":\n",
    "        logger.info(msg)\n",
    "    elif level == \"error\":\n",
    "        logger.error(msg)\n",
    "    print(msg)  # ensures it always shows in notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1912935,
     "status": "ok",
     "timestamp": 1755831567023,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "4x93i35LdLSQ",
    "outputId": "8ee12e0f-afb3-470d-b7bb-dc1b86cb993b"
   },
   "outputs": [],
   "source": [
    "for year_month in missing_in_silver:\n",
    "    year, month = map(int, year_month.split(\"-\"))\n",
    "    table_name = f\"{TABLE_PREFIX}_{year_month.replace('-', '_')}\"\n",
    "\n",
    "    try:\n",
    "        log_and_upload(f\" Starting processing for {year_month}\")\n",
    "\n",
    "        # --- Load from Raw Bronze ---\n",
    "        df = spark.read.format(\"bigquery\").option(\"table\", f\"{PROJECT_ID}.{BRONZE_DATASET_NAME}.{table_name}\").load()\n",
    "        log_and_upload(f\"Loaded {df.count()} rows from {BRONZE_DATASET_NAME}.{table_name}\")\n",
    "\n",
    "        # --- Cleaning Rules ---\n",
    "        before = df.count()\n",
    "        df = df.filter(F.col(\"pickup_datetime\").isNotNull() & F.col(\"dropOff_datetime\").isNotNull())\n",
    "        log_and_upload(f\"Dropped {before - df.count()} rows with null pickup/dropoff times\")\n",
    "\n",
    "        before = df.count()\n",
    "        df = df.filter(F.col(\"pickup_datetime\") < F.col(\"dropOff_datetime\"))\n",
    "        log_and_upload(f\"Dropped {before - df.count()} rows with invalid pickup/dropoff order\")\n",
    "\n",
    "        for loc_col in [\"PUlocationID\", \"DOlocationID\"]:\n",
    "            if loc_col in df.columns:\n",
    "                before = df.count()\n",
    "                df = df.filter(F.col(loc_col).isNotNull() & (F.col(loc_col) >= 0))\n",
    "                log_and_upload(f\"Dropped {before - df.count()} rows with invalid {loc_col}\")\n",
    "\n",
    "        df = df.withColumn(\"trip_duration\", F.unix_timestamp(\"dropOff_datetime\") - F.unix_timestamp(\"pickup_datetime\"))\n",
    "        before = df.count()\n",
    "        df = df.filter((F.col(\"trip_duration\") >= 60) & (F.col(\"trip_duration\") <= 86400))\n",
    "        log_and_upload(f\"Dropped {before - df.count()} rows with invalid trip_duration\")\n",
    "\n",
    "        if \"SR_Flag\" in df.columns:\n",
    "            df = df.withColumn(\"SR_Flag\", F.when(F.col(\"SR_Flag\").isin(0, 1), F.col(\"SR_Flag\")).otherwise(None))\n",
    "\n",
    "        df = df.withColumn(\"year\", F.year(\"pickup_datetime\")).withColumn(\"pickup_month\", F.month(\"pickup_datetime\"))\n",
    "        before = df.count()\n",
    "        df = df.filter((F.col(\"year\") == year) & (F.col(\"pickup_month\") == month))\n",
    "        log_and_upload(f\"Filtered to {year_month}, remaining {df.count()} rows\")\n",
    "\n",
    "        # --- Write to Clean Silver ---\n",
    "        df.write.format(\"bigquery\").option(\"table\", f\"{PROJECT_ID}.{SILVER_DATASET_NAME}.{table_name}\").mode(\"overwrite\").save()\n",
    "        log_and_upload(f\" Written to {SILVER_DATASET_NAME}.{table_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log_and_upload(f\" ERROR processing {year_month}: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZbWWYHLeCx5"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Stop the Spark session gracefully\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\" Spark session stopped\")\n",
    "except Exception as e:\n",
    "    print(f\" Error while stopping Spark session: {e}\")\n",
    "\n",
    "# Sleep for 60 seconds to allow quota/resources to free up\n",
    "print(\" Waiting 60s for resources to be released...\")\n",
    "time.sleep(60)\n",
    "print(\" Done waiting. You can safely start a new session now.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "cell_execution_strategy": "setup",
   "name": "02cFHVRawBronzeToCleanSilver",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
