{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgB-OC1lGJZR"
   },
   "outputs": [],
   "source": [
    "from google.cloud.dataproc_spark_connect import DataprocSparkSession\n",
    "from google.cloud.dataproc_v1 import Session\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import time, datetime, os\n",
    "import logging\n",
    "import sys\n",
    "from google.cloud import storage\n",
    "import io\n",
    "from google.cloud import bigquery\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "executionInfo": {
     "elapsed": 114018,
     "status": "ok",
     "timestamp": 1756449474709,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "p2NN8wwF7YRa",
    "outputId": "b47bc09b-f527-4dc5-cfb9-9bee285ddc01"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import re\n",
    "import grpc\n",
    "from google.api_core.exceptions import GoogleAPICallError, RetryError, ServiceUnavailable\n",
    "from google.cloud.dataproc_spark_connect import DataprocSparkSession\n",
    "from pyspark.errors import PySparkValueError\n",
    "\n",
    "_QUOTA_PAT = re.compile(r\"insufficient.*cpus.*quota\", re.IGNORECASE)\n",
    "\n",
    "def _is_quota_error(exc: Exception) -> bool:\n",
    "    msg = str(exc) or \"\"\n",
    "    return bool(_QUOTA_PAT.search(msg))\n",
    "\n",
    "def get_spark_with_retry(\n",
    "    gentle_delay: float = 10.0,     # fixed delay for the first few tries\n",
    "    gentle_retries: int = 3,        # how many tries use gentle delay\n",
    "    max_backoff: float = 60.0,      # cap for exponential backoff\n",
    "    quota_wait: float = 90.0,       # wait longer when we hit a QUOTA error\n",
    "    jitter: bool = True,\n",
    "    verify_ready: bool = True,\n",
    "    max_attempts: int | None = None # None = infinite\n",
    "):\n",
    "    \"\"\"\n",
    "    Keep retrying until DataprocSparkSession is usable.\n",
    "    - QUOTA errors: wait `quota_wait` seconds and retry (lets other jobs finish).\n",
    "    - Other errors: gentle linear delay for first `gentle_retries`, then exponential backoff.\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    backoff = gentle_delay\n",
    "\n",
    "    while True:\n",
    "        attempt += 1\n",
    "        try:\n",
    "            spark = DataprocSparkSession.builder.getOrCreate()\n",
    "            # Let backend settle a moment before the test query\n",
    "            time.sleep(3)\n",
    "\n",
    "            if verify_ready:\n",
    "                _ = spark.range(1).count()\n",
    "\n",
    "            print(f\" Spark connected on attempt {attempt}\")\n",
    "            return spark\n",
    "\n",
    "        except (ServiceUnavailable, GoogleAPICallError, RetryError,\n",
    "                grpc.RpcError, PySparkValueError, RuntimeError) as e:\n",
    "            # QUOTA-specific path: someone else might be using CPUs. Wait longer, then retry.\n",
    "            if _is_quota_error(e):\n",
    "                wait = quota_wait + (random.uniform(0, quota_wait/3) if jitter else 0.0)\n",
    "                print(f\" Quota limited (likely other sessions using CPUs). \"\n",
    "                      f\"Sleeping {wait:.1f}s, then retrying…\\nDetails: {e}\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                # Non-quota transient errors\n",
    "                if attempt <= gentle_retries:\n",
    "                    wait = gentle_delay\n",
    "                else:\n",
    "                    # exponential backoff capped\n",
    "                    backoff = min(backoff * 2, max_backoff)\n",
    "                    wait = backoff\n",
    "                if jitter:\n",
    "                    wait += random.uniform(0, wait/2)\n",
    "                print(f\"  Attempt {attempt} failed: {e!r}\\n   Sleeping {wait:.1f}s before retry…\")\n",
    "                time.sleep(wait)\n",
    "\n",
    "            if max_attempts and attempt >= max_attempts:\n",
    "                raise RuntimeError(f\" Failed after {attempt} attempts\") from e\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\" Stopped by user.\")\n",
    "            raise\n",
    "\n",
    "# --- Usage ---\n",
    "spark = get_spark_with_retry()\n",
    "\n",
    "# Safe to use:\n",
    "df = spark.range(5)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "executionInfo": {
     "elapsed": 423,
     "status": "ok",
     "timestamp": 1756449510199,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "lrrd3oev4ZpB",
    "outputId": "79e17289-ab4e-4afa-c472-a290d4ffa645"
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNRrtNrKDZYa"
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Global Config\n",
    "# ---------------------------\n",
    "\n",
    "PROJECT_ID = \"nyctaxi-467111\"\n",
    "SILVER_DATASET = \"CleanSilver\"\n",
    "PREML_DATASET = \"PreMlGold\"\n",
    "BUCKET_NAME = \"nyc_raw_data_bucket\"\n",
    "RUN_ID = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "TAXI_TYPES = [\"yellow\", \"green\", \"fhv\", \"fhvhv\"]\n",
    "\n",
    "LOG_LOCAL = f\"./pipeline_run_{RUN_ID}.log\"\n",
    "LOG_GCS   = f\"gs://{BUCKET_NAME}/logs/preMLpipeline_logs/pipeline_run_{RUN_ID}.log\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTa6-ESaF05o"
   },
   "outputs": [],
   "source": [
    "def log_and_upload(msg: str):\n",
    "    ts = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    line = f\"[{ts}] {msg}\"\n",
    "    print(line)\n",
    "    with open(LOG_LOCAL, \"a\") as f: f.write(line + \"\\n\")\n",
    "    os.system(f\"gsutil cp {LOG_LOCAL} {LOG_GCS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3nfr9FPOPgT"
   },
   "source": [
    "\n",
    "# ==============================================================\n",
    "# Stage 2: Check Silver vs PreML → build missing_map\n",
    "# =============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10322,
     "status": "ok",
     "timestamp": 1756451061862,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "PooQrh-fGbw2",
    "outputId": "89b20346-6ea8-4dba-9ade-c5389d69469b"
   },
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "def list_tables(dataset_name: str):\n",
    "    return [t.table_id for t in bq_client.list_tables(dataset_name)]\n",
    "\n",
    "silver_tables = list_tables(SILVER_DATASET)\n",
    "preml_tables = list_tables(PREML_DATASET)\n",
    "\n",
    "missing_map = {}\n",
    "for tbl in tqdm(silver_tables):\n",
    "    taxi_type = tbl.split(\"_\")[0]\n",
    "    if taxi_type not in TAXI_TYPES:\n",
    "       print(f\"skipping {tbl}\")\n",
    "       continue\n",
    "    if ((tbl+\"_daily\" not in preml_tables) and (tbl+\"_hourly\" not in preml_tables) and (tbl+\"_hotspot\" not in preml_tables))  :\n",
    "        missing_map.setdefault(taxi_type, []).append(tbl)\n",
    "\n",
    "log_and_upload(f\"Identified missing_map = {missing_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 485,
     "status": "ok",
     "timestamp": 1756449536392,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "BF0eaQ3gHKSZ",
    "outputId": "51670d99-138d-495f-d828-88a8982cd9a5"
   },
   "outputs": [],
   "source": [
    "for i in missing_map:\n",
    "  print(f\"{i}: count {len(missing_map[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXIgkUXaHogt"
   },
   "source": [
    "# ---------------------------\n",
    "# Stage 2 - Aggregate Features for PreML\n",
    "# ---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ri3HMp7tHqla"
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Stage 2 - Aggregate Features for PreML\n",
    "# ---------------------------\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def process_missing_to_preml(missing_map: dict):\n",
    "    \"\"\"\n",
    "    For each taxi_type and missing CleanSilver table,\n",
    "    aggregate features and save into PreML dataset.\n",
    "    \"\"\"\n",
    "    for taxi_type, tbl_list in tqdm(missing_map.items()):\n",
    "        for tbl in tbl_list:\n",
    "            silver_table = f\"{PROJECT_ID}.{SILVER_DATASET}.{tbl}\"\n",
    "            preml_table = f\"{PROJECT_ID}.{PREML_DATASET}.{tbl}\"\n",
    "\n",
    "            log_and_upload(f\" Processing {silver_table} → {preml_table}\")\n",
    "\n",
    "            try:\n",
    "                # -------------------------\n",
    "                # Load CleanSilver table\n",
    "                # -------------------------\n",
    "                df = spark.read.format(\"bigquery\").option(\"table\", silver_table).load()\n",
    "                log_and_upload(f\"Loaded {df.count()} rows from {silver_table}\")\n",
    "\n",
    "                # -------------------------\n",
    "                # Common Time Columns\n",
    "                # -------------------------\n",
    "                # Different taxi types use different pickup/dropoff field names\n",
    "                # WHERE: choose pickup/dropoff & revenue per taxi type\n",
    "                if taxi_type == \"yellow\":\n",
    "                    pickup_col, dropoff_col = \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"\n",
    "                    revenue_col = \"total_amount\"\n",
    "                elif taxi_type == \"green\":\n",
    "                    pickup_col, dropoff_col = \"lpep_pickup_datetime\", \"lpep_dropoff_datetime\"\n",
    "                    revenue_col = \"total_amount\"\n",
    "                elif taxi_type == \"fhv\":\n",
    "                    pickup_col, dropoff_col = \"pickup_datetime\", \"dropOff_datetime\"\n",
    "                    revenue_col = F.lit(0.0)  # no fare data in fhv\n",
    "                elif taxi_type == \"fhvhv\":\n",
    "                    pickup_col, dropoff_col = \"pickup_datetime\", \"dropoff_datetime\"\n",
    "                    revenue_col = \"base_passenger_fare\"\n",
    "                else:\n",
    "                    log_and_upload(f\" Unsupported taxi_type {taxi_type}, skipping...\")\n",
    "                    continue\n",
    "                # WHERE: derive date & hour for grouping\n",
    "                df = df.withColumn(\"pickup_date\", F.to_date(F.col(pickup_col)))\n",
    "                df = df.withColumn(\"pickup_hour\", F.hour(F.col(pickup_col)))\n",
    "\n",
    "                # -------------------------\n",
    "                # Aggregations\n",
    "                # -------------------------\n",
    "                log_and_upload(f\"Aggregating  rows from {silver_table}\")\n",
    "\n",
    "                # 1. Daily Trip Counts & Revenue\n",
    "                daily_agg = (\n",
    "                    df.groupBy(\"pickup_date\")\n",
    "                      .agg(\n",
    "                          F.count(\"*\").alias(\"trips\"),\n",
    "                          F.sum(revenue_col).alias(\"revenue\")\n",
    "                      )\n",
    "                      .withColumn(\"taxi_type\", F.lit(taxi_type))\n",
    "                )\n",
    "\n",
    "                # 2. Hourly Trip Demand (for anomaly detection / forecasting)\n",
    "                hourly_agg = (\n",
    "                    df.groupBy(\"pickup_date\", \"pickup_hour\")\n",
    "                      .agg(F.count(\"*\").alias(\"trips\"),\n",
    "                           F.sum(revenue_col).alias(\"revenue\"))\n",
    "                      .withColumn(\"taxi_type\", F.lit(taxi_type))\n",
    "                )\n",
    "\n",
    "                # 3. Hotspot Clustering Prep (PU/DO location counts)\n",
    "                hotspot_agg = (\n",
    "                    df.groupBy(\"pickup_date\", \"PULocationID\", \"DOLocationID\")\n",
    "                      .agg(F.count(\"*\").alias(\"trips\"),\n",
    "                           F.sum(revenue_col).alias(\"revenue\"))\n",
    "                      .withColumn(\"taxi_type\", F.lit(taxi_type))\n",
    "                )\n",
    "\n",
    "                log_and_upload(f\"Writing  rows from aggregated {silver_table}\")\n",
    "\n",
    "                # -------------------------\n",
    "                # Write to PreML\n",
    "                # -------------------------\n",
    "                # Store partitioned by taxi_type_yyyy_mm\n",
    "                (daily_agg.write.format(\"bigquery\")\n",
    "                    .option(\"table\", preml_table + \"_daily\")\n",
    "                    .option(\"writeMethod\", \"direct\")\n",
    "                    .mode(\"overwrite\")\n",
    "                    .save())\n",
    "\n",
    "                (hourly_agg.write.format(\"bigquery\")\n",
    "                    .option(\"table\", preml_table + \"_hourly\")\n",
    "                    .option(\"writeMethod\", \"direct\")\n",
    "                    .mode(\"overwrite\")\n",
    "                    .save())\n",
    "\n",
    "                (hotspot_agg.write.format(\"bigquery\")\n",
    "                    .option(\"table\", preml_table + \"_hotspot\")\n",
    "                    .option(\"writeMethod\", \"direct\")\n",
    "                    .mode(\"overwrite\")\n",
    "                    .save())\n",
    "\n",
    "                log_and_upload(f\" Saved aggregated PreML tables for {tbl}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                log_and_upload(f\" ERROR processing {tbl}: {str(e)}\")\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6Hiu7PmIrbq"
   },
   "source": [
    "# Validation & Incremental Update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2--2nwvJHraL"
   },
   "outputs": [],
   "source": [
    "def validate_and_update_preml(taxi_type: str, year_month_list: list):\n",
    "    \"\"\"\n",
    "    Check CleanSilver vs PreML for a given taxi_type and list of yyyy_mm.\n",
    "    Process only missing PreML partitions.\n",
    "    \"\"\"\n",
    "    from google.cloud import bigquery\n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "    for year_month in tqdm(year_month_list):\n",
    "        silver_table = f\"{PROJECT_ID}.{SILVER_DATASET}.{taxi_type}_{year_month}\"\n",
    "        preml_daily   = f\"{PROJECT_ID}.{PREML_DATASET}.{taxi_type}_{year_month}_daily\"\n",
    "        preml_hourly  = f\"{PROJECT_ID}.{PREML_DATASET}.{taxi_type}_{year_month}_hourly\"\n",
    "        preml_hotspot = f\"{PROJECT_ID}.{PREML_DATASET}.{taxi_type}_{year_month}_hotspot\"\n",
    "\n",
    "        try:\n",
    "            silver_count = [r.cnt for r in client.query(f\"SELECT COUNT(*) as cnt FROM `{silver_table}`\").result()][0]\n",
    "            if silver_count == 0:\n",
    "                log_and_upload(f\" Skipping {silver_table} (empty)\")\n",
    "                continue\n",
    "\n",
    "            existing_preml = []\n",
    "            for t in [preml_daily, preml_hourly, preml_hotspot]:\n",
    "                try:\n",
    "                    client.get_table(t)\n",
    "                    existing_preml.append(t)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            if len(existing_preml) == 3:\n",
    "                log_and_upload(f\" PreML already exists for {taxi_type}_{year_month}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            log_and_upload(f\" Running PreML aggregation for {taxi_type}_{year_month}\")\n",
    "            process_missing_to_preml({taxi_type: [f\"{taxi_type}_{year_month}\"]})\n",
    "\n",
    "        except Exception as e:\n",
    "            log_and_upload(f\" ERROR validation {taxi_type}_{year_month}: {str(e)}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3a2Yj3-JL9k"
   },
   "source": [
    "# ---------------------------\n",
    "# Master Orchestration\n",
    "# ---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "executionInfo": {
     "elapsed": 47182,
     "status": "ok",
     "timestamp": 1756449602224,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "daHw1DHtiu0F",
    "outputId": "6cff5ec9-9043-4c09-aeaa-35d70731d948"
   },
   "outputs": [],
   "source": [
    "for taxi_type in tqdm(TAXI_TYPES):\n",
    "        # Only capture tables like taxiType_YYYY_MM\n",
    "        query = f\"\"\"\n",
    "            SELECT table_name\n",
    "            FROM `{PROJECT_ID}.{SILVER_DATASET}.INFORMATION_SCHEMA.TABLES`\n",
    "            WHERE REGEXP_CONTAINS(table_name, r'^{taxi_type}_[0-9]{{4}}_[0-9]{{2}}$')\n",
    "        \"\"\"\n",
    "        # WHERE: run that SQL against BigQuery via the Spark-BQ connector\n",
    "        df_partitions = (\n",
    "            spark.read.format(\"bigquery\")\n",
    "            .option(\"query\", query)\n",
    "            .load()\n",
    "        )\n",
    "\n",
    "        if df_partitions.count() == 0:\n",
    "            print(f\" No CleanSilver tables found for {taxi_type}\")\n",
    "            continue\n",
    "\n",
    "        # Sorted year_month list\n",
    "        year_month_list = sorted(\n",
    "            [row.table_name.replace(f\"{taxi_type}_\", \"\") for row in df_partitions.collect()]\n",
    "        )\n",
    "        print(f\" Found partitions for {taxi_type}: {year_month_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_eDGEOm5JLXB"
   },
   "outputs": [],
   "source": [
    "def run_master_orchestration():\n",
    "    run_id = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_and_upload(f\" Starting Master Orchestration Run ID={run_id}\")\n",
    "\n",
    "    for taxi_type in tqdm(TAXI_TYPES):# WHERE: iterate across yellow/green/fhv/fhvhv\n",
    "        try:\n",
    "            # Only capture tables like taxiType_YYYY_MM\n",
    "            query = f\"\"\"\n",
    "                SELECT table_name\n",
    "                FROM `{PROJECT_ID}.{SILVER_DATASET}.INFORMATION_SCHEMA.TABLES`\n",
    "                WHERE REGEXP_CONTAINS(table_name, r'^{taxi_type}_[0-9]{{4}}_[0-9]{{2}}$')\n",
    "            \"\"\"\n",
    "            df_partitions = (\n",
    "                spark.read.format(\"bigquery\")\n",
    "                .option(\"query\", query)\n",
    "                .load()\n",
    "            )\n",
    "\n",
    "            # WHERE: if no partitions, skip this taxi type\n",
    "            if df_partitions.count() == 0:\n",
    "                log_and_upload(f\" No CleanSilver tables found for {taxi_type}\")\n",
    "                continue\n",
    "\n",
    "            # WHERE: extract and sort YYYY_MM suffixes from table names\n",
    "            # Sorted year_month list\n",
    "            year_month_list = sorted(\n",
    "                [row.table_name.replace(f\"{taxi_type}_\", \"\") for row in df_partitions.collect()]\n",
    "            )\n",
    "            log_and_upload(f\" Found partitions for {taxi_type}: {year_month_list}\")\n",
    "\n",
    "            # Stage 3 validation/update\n",
    "            # WHERE: delegate to the PreML validator/runner\n",
    "            validate_and_update_preml(taxi_type, year_month_list)\n",
    "\n",
    "        except Exception as e:\n",
    "            log_and_upload(f\" ERROR orchestration {taxi_type}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    log_and_upload(\" Master Orchestration completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09ypECtvRm4d"
   },
   "source": [
    "# --- ML Ready Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHxIQSG-RrJs"
   },
   "outputs": [],
   "source": [
    "# def create_ml_ready_views():\n",
    "#     \"\"\"Create unified BigQuery views for ML consumption.\"\"\"\n",
    "#     client = bigquery.Client(project=PROJECT_ID)\n",
    "#     log_and_upload(\" Creating ML Ready Views...\")\n",
    "\n",
    "#     # Daily\n",
    "#     daily_query = f\"\"\"\n",
    "#     CREATE OR REPLACE VIEW `{PROJECT_ID}.{PREML_DATASET}.all_taxi_daily` AS\n",
    "#     SELECT taxi_type, pickup_date, SUM(trips) as trips, SUM(revenue) as revenue\n",
    "#     FROM `{PROJECT_ID}.{PREML_DATASET}.*_daily`\n",
    "#     GROUP BY taxi_type, pickup_date\n",
    "#     \"\"\"\n",
    "#     client.query(daily_query).result()\n",
    "#     log_and_upload(\" Created view: all_taxi_daily\")\n",
    "\n",
    "#     # Hourly\n",
    "#     hourly_query = f\"\"\"\n",
    "#     CREATE OR REPLACE VIEW `{PROJECT_ID}.{PREML_DATASET}.all_taxi_hourly` AS\n",
    "#     SELECT taxi_type, pickup_date, pickup_hour, SUM(trips) as trips\n",
    "#     FROM `{PROJECT_ID}.{PREML_DATASET}.*_hourly`\n",
    "#     GROUP BY taxi_type, pickup_date, pickup_hour\n",
    "#     \"\"\"\n",
    "#     client.query(hourly_query).result()\n",
    "#     log_and_upload(\" Created view: all_taxi_hourly\")\n",
    "\n",
    "#     # Hotspots\n",
    "#     hotspot_query = f\"\"\"\n",
    "#     CREATE OR REPLACE VIEW `{PROJECT_ID}.{PREML_DATASET}.all_taxi_hotspots` AS\n",
    "#     SELECT taxi_type, pickup_date, PULocationID, DOLocationID, SUM(trips) as trips\n",
    "#     FROM `{PROJECT_ID}.{PREML_DATASET}.*_hotspot`\n",
    "#     GROUP BY taxi_type, pickup_date, PULocationID, DOLocationID\n",
    "#     \"\"\"\n",
    "#     client.query(hotspot_query).result()\n",
    "#     log_and_upload(\" Created view: all_taxi_hotspots\")\n",
    "\n",
    "#     log_and_upload(\" ML Ready Views created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCtBn-tspgLZ"
   },
   "outputs": [],
   "source": [
    "# # prompt: delete all tables from PreMlGold\n",
    "\n",
    "# import pandas_gbq\n",
    "\n",
    "# # Construct the SQL query to get all table names in the PreMlGold dataset\n",
    "# sql_query = f\"\"\"\n",
    "# SELECT table_name\n",
    "# FROM `{PROJECT_ID}.{PREML_DATASET}.INFORMATION_SCHEMA.TABLES`\n",
    "# WHERE table_type = 'BASE TABLE'\n",
    "# \"\"\"\n",
    "\n",
    "# # Read the table names into a DataFrame\n",
    "# df_tables = pandas_gbq.read_gbq(sql_query, project_id=PROJECT_ID, dialect=\"standard\")\n",
    "\n",
    "# # Iterate through the table names and delete each table\n",
    "# for table_name in tqdm(df_tables['table_name']):\n",
    "#     try:\n",
    "#         delete_query = f\"DROP TABLE `{PROJECT_ID}.{PREML_DATASET}.{table_name}`\"\n",
    "#         pandas_gbq.read_gbq(delete_query, project_id=PROJECT_ID, dialect=\"standard\")\n",
    "#         print(f\"Successfully deleted table: {table_name}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error deleting table {table_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSFm8lLlMDOq"
   },
   "source": [
    "# -- main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 199654,
     "status": "ok",
     "timestamp": 1756450384769,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "b2oMpFvoMFlW",
    "outputId": "f5836c03-f182-4929-afd8-dd0f29540472"
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Main Entrypoint\n",
    "# ==============================================================\n",
    "\n",
    "def main():\n",
    "    run_id = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_and_upload(f\"Starting PreML Incremental Aggregation Pipeline | RunID={run_id}\")\n",
    "\n",
    "    try:\n",
    "        # Stage 2 & 5 - Orchestration (find missing + aggregate)\n",
    "        run_master_orchestration()\n",
    "\n",
    "        # # Stage 6 - Create ML-ready views\n",
    "        # create_ml_ready_views()\n",
    "\n",
    "        log_and_upload(\" Pipeline completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log_and_upload(f\" Pipeline failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        log_and_upload(\" Final log uploaded to GCS\")\n",
    "        log_and_upload(f\"Log file location: {LOG_GCS}\")\n",
    "\n",
    "\n",
    "# Trigger when running notebook\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OM4NYx33Rupm"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Stop the Spark session gracefully\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\" Spark session stopped\")\n",
    "except Exception as e:\n",
    "    print(f\" Error while stopping Spark session: {e}\")\n",
    "\n",
    "# Sleep for 60 seconds to allow quota/resources to free up\n",
    "print(\" Waiting 60s for resources to be released...\")\n",
    "time.sleep(60)\n",
    "print(\" Done waiting. You can safely start a new session now.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "cell_execution_strategy": "setup",
   "name": "03CleanSilverToPreML",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
