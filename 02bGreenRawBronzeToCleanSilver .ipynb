{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WL_OOkXwY2Xi"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import TimestampType, IntegerType, StringType\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "p2NN8wwF7YRa",
    "outputId": "dd705baa-fe14-4713-86dd-8df55617ad1b"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import re\n",
    "import grpc\n",
    "from google.api_core.exceptions import GoogleAPICallError, RetryError, ServiceUnavailable\n",
    "from google.cloud.dataproc_spark_connect import DataprocSparkSession\n",
    "from pyspark.errors import PySparkValueError\n",
    "\n",
    "_QUOTA_PAT = re.compile(r\"insufficient.*cpus.*quota\", re.IGNORECASE)\n",
    "\n",
    "def _is_quota_error(exc: Exception) -> bool:\n",
    "    msg = str(exc) or \"\"\n",
    "    return bool(_QUOTA_PAT.search(msg))\n",
    "\n",
    "def get_spark_with_retry(\n",
    "    gentle_delay: float = 10.0,     # fixed delay for the first few tries\n",
    "    gentle_retries: int = 3,        # how many tries use gentle delay\n",
    "    max_backoff: float = 60.0,      # cap for exponential backoff\n",
    "    quota_wait: float = 90.0,       # wait longer when we hit a QUOTA error\n",
    "    jitter: bool = True,\n",
    "    verify_ready: bool = True,\n",
    "    max_attempts: int | None = None # None = infinite\n",
    "):\n",
    "    \"\"\"\n",
    "    Keep retrying until DataprocSparkSession is usable.\n",
    "    - QUOTA errors: wait `quota_wait` seconds and retry (lets other jobs finish).\n",
    "    - Other errors: gentle linear delay for first `gentle_retries`, then exponential backoff.\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    backoff = gentle_delay\n",
    "\n",
    "    while True:\n",
    "        attempt += 1\n",
    "        try:\n",
    "            spark = DataprocSparkSession.builder.getOrCreate()\n",
    "            # Let backend settle a moment before the test query\n",
    "            time.sleep(3)\n",
    "\n",
    "            if verify_ready:\n",
    "                _ = spark.range(1).count()\n",
    "\n",
    "            print(f\" Spark connected on attempt {attempt}\")\n",
    "            return spark\n",
    "\n",
    "        except (ServiceUnavailable, GoogleAPICallError, RetryError,\n",
    "                grpc.RpcError, PySparkValueError, RuntimeError) as e:\n",
    "            # QUOTA-specific path: someone else might be using CPUs. Wait longer, then retry.\n",
    "            if _is_quota_error(e):\n",
    "                wait = quota_wait + (random.uniform(0, quota_wait/3) if jitter else 0.0)\n",
    "                print(f\"⏳ Quota limited (likely other sessions using CPUs). \"\n",
    "                      f\"Sleeping {wait:.1f}s, then retrying…\\nDetails: {e}\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                # Non-quota transient errors\n",
    "                if attempt <= gentle_retries:\n",
    "                    wait = gentle_delay\n",
    "                else:\n",
    "                    # exponential backoff capped\n",
    "                    backoff = min(backoff * 2, max_backoff)\n",
    "                    wait = backoff\n",
    "                if jitter:\n",
    "                    wait += random.uniform(0, wait/2)\n",
    "                print(f\"  Attempt {attempt} failed: {e!r}\\n   Sleeping {wait:.1f}s before retry…\")\n",
    "                time.sleep(wait)\n",
    "\n",
    "            if max_attempts and attempt >= max_attempts:\n",
    "                raise RuntimeError(f\" Failed after {attempt} attempts\") from e\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\" Stopped by user.\")\n",
    "            raise\n",
    "\n",
    "# --- Usage ---\n",
    "spark = get_spark_with_retry()\n",
    "\n",
    "# Safe to use:\n",
    "df = spark.range(5)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_0YSUqHZaf5"
   },
   "source": [
    "## -- config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sMGYOkcYZcCf"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"nyctaxi-467111\"\n",
    "BUCKET_NAME = \"nyc_raw_data_bucket\"\n",
    "RAW_BUCKET = f\"gs://{BUCKET_NAME}\"\n",
    "BRONZE_DATASET_NAME = \"RawBronze\"\n",
    "SILVER_DATASET_NAME = \"CleanSilver\"\n",
    "TAXI_TYPES = [\"yellow\", \"green\", \"fhv\", \"fhvhv\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94KGpthHY485"
   },
   "source": [
    "# -------------------------\n",
    "# Logger Setup (runtime log file)\n",
    "# -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1755847432365,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "5NDH0NosY3NN",
    "outputId": "5538c023-7c4c-4399-8a27-3e938a5e3952"
   },
   "outputs": [],
   "source": [
    "run_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_filename = f\"Clean_green_{run_time}.log\"\n",
    "local_log_path = f\"/tmp/{log_filename}\"\n",
    "gcs_log_path = f\"logs/{log_filename}\"\n",
    "print(f\" Logs will also be uploaded to: {gcs_log_path}\")\n",
    "\n",
    "logger = logging.getLogger(\"green_cleaning\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "\n",
    "fh = logging.FileHandler(local_log_path)\n",
    "fh.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "def log_and_upload(msg: str):\n",
    "    logger.info(msg)\n",
    "    # Optionally copy local_log_path to GCS\n",
    "    os.system(f\"gsutil cp {local_log_path} gs://{BUCKET_NAME}/{gcs_log_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLaJNwwdZlPO"
   },
   "source": [
    "## - data to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hA2-xkIoZBBf"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "def list_tables(dataset_name, prefix=\"yellow_\"):\n",
    "    \"\"\"\n",
    "    List tables in a dataset with a given prefix.\n",
    "    Returns normalized year_month strings (yyyy-mm).\n",
    "    \"\"\"\n",
    "    tables = bq_client.list_tables(dataset_name)\n",
    "    ym_list = []\n",
    "    for table in tables:\n",
    "        table_name = table.table_id\n",
    "        if table_name.startswith(prefix):\n",
    "            # Example: yellow_2022_01 → 2022-01\n",
    "            ym = table_name.replace(prefix, \"\").replace(\"_\", \"-\")\n",
    "            ym_list.append(ym)\n",
    "    return sorted(ym_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1512,
     "status": "ok",
     "timestamp": 1755848703097,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "gJ6fYfKcZnu2",
    "outputId": "be8e744a-e202-4f4d-ecc9-44782b22613c"
   },
   "outputs": [],
   "source": [
    "bronze_list = list_tables(BRONZE_DATASET_NAME, prefix=\"green_\")\n",
    "silver_list = list_tables(SILVER_DATASET_NAME, prefix=\"green_\")\n",
    "\n",
    "print(\"Bronze available:\", bronze_list)\n",
    "print(\"Silver available:\", silver_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1755848703861,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "U-yw6eYPZtXG",
    "outputId": "f99f234d-e9b1-4d69-d255-d2b831005cfb"
   },
   "outputs": [],
   "source": [
    "missing_in_silver = sorted(set(bronze_list) - set(silver_list))\n",
    "\n",
    "print(\" Missing in Silver:\", missing_in_silver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iekz5gp_ZxW2"
   },
   "source": [
    "# -- main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 19040,
     "status": "ok",
     "timestamp": 1755848453732,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "sa5kPyG2ZwT_",
    "outputId": "3c8c574e-9167-4c7a-9ef0-d7f76c0bd01f"
   },
   "outputs": [],
   "source": [
    "for year_month in missing_in_silver:\n",
    "    year, month = map(int, year_month.split(\"-\"))\n",
    "\n",
    "    try:\n",
    "        log_and_upload(f\" Starting Green processing for {year_month}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Load from RawBronze\n",
    "        # -------------------------\n",
    "        table_name = f\"green_{year_month.replace('-', '_')}\"\n",
    "        rawbronze_table = f\"{PROJECT_ID}.{BRONZE_DATASET_NAME}.{table_name}\"\n",
    "        df = spark.read.format(\"bigquery\").option(\"table\", rawbronze_table).load()\n",
    "        log_and_upload(f\"Loaded {df.count()} rows from {rawbronze_table}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Type casting\n",
    "        # -------------------------\n",
    "        if not isinstance(df.schema[\"lpep_pickup_datetime\"].dataType, TimestampType):\n",
    "            df = df.withColumn(\"lpep_pickup_datetime\", F.col(\"lpep_pickup_datetime\").cast(TimestampType()))\n",
    "        if not isinstance(df.schema[\"lpep_dropoff_datetime\"].dataType, TimestampType):\n",
    "            df = df.withColumn(\"lpep_dropoff_datetime\", F.col(\"lpep_dropoff_datetime\").cast(TimestampType()))\n",
    "\n",
    "        categorical_columns = ['VendorID','RatecodeID','store_and_fwd_flag','PULocationID','DOLocationID','payment_type','trip_type']\n",
    "        for col_name in categorical_columns:\n",
    "            if col_name in df.columns:\n",
    "                if col_name == 'store_and_fwd_flag':\n",
    "                    df = df.withColumn(col_name, F.col(col_name).cast(StringType()))\n",
    "                else:\n",
    "                    df = df.withColumn(col_name, F.col(col_name).cast(IntegerType()))\n",
    "\n",
    "        log_and_upload(\"Completed type casting\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Drop null passenger_count\n",
    "        # -------------------------\n",
    "        before = df.count()\n",
    "        df = df.filter(F.col(\"passenger_count\").isNotNull())\n",
    "        log_and_upload(f\"Dropped {before - df.count()} rows with null passenger_count\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Filter invalid times\n",
    "        # -------------------------\n",
    "        before = df.count()\n",
    "        df = df.filter(F.col(\"lpep_pickup_datetime\") < F.col(\"lpep_dropoff_datetime\"))\n",
    "        log_and_upload(f\"Dropped {before - df.count()} rows with invalid pickup/dropoff\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Filter invalid distances\n",
    "        # -------------------------\n",
    "        before = df.count()\n",
    "        df = df.filter(F.col(\"trip_distance\") > 0)\n",
    "        log_and_upload(f\"Dropped {before - df.count()} rows with invalid distances\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Passenger count filter\n",
    "        # -------------------------\n",
    "        before = df.count()\n",
    "        df = df.filter(F.col(\"passenger_count\") > 0)\n",
    "        log_and_upload(f\"Dropped {before - df.count()} rows with invalid passenger_count\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Fare & total_amount checks\n",
    "        # -------------------------\n",
    "        before = df.count()\n",
    "        df = df.filter(F.col(\"fare_amount\") > 0)\n",
    "        log_and_upload(f\"Dropped {before - df.count()} rows with invalid fare_amount\")\n",
    "\n",
    "        before = df.count()\n",
    "        df = df.filter(F.col(\"total_amount\") > 0)\n",
    "        log_and_upload(f\"Dropped {before - df.count()} rows with invalid total_amount\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Outlier Removal (IQR trip_distance)\n",
    "        # -------------------------\n",
    "        Q1, Q3 = df.approxQuantile(\"trip_distance\", [0.25, 0.75], 0.01)\n",
    "        IQR = Q3 - Q1\n",
    "        lb, ub = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
    "        before = df.count()\n",
    "        df = df.filter((F.col(\"trip_distance\") >= lb) & (F.col(\"trip_distance\") <= ub))\n",
    "        log_and_upload(f\"Dropped {before - df.count()} trip_distance outliers\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Trip duration filter\n",
    "        # -------------------------\n",
    "        df = df.withColumn(\"trip_duration\",\n",
    "                           F.unix_timestamp(\"lpep_dropoff_datetime\") - F.unix_timestamp(\"lpep_pickup_datetime\"))\n",
    "        before = df.count()\n",
    "        df = df.filter((F.col(\"trip_duration\") >= 5) & (F.col(\"trip_duration\") <= 7200))\n",
    "        log_and_upload(f\"Dropped {before - df.count()} rows with invalid trip_duration\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Ensure only correct year/month\n",
    "        # -------------------------\n",
    "        df = df.withColumn(\"year\", F.year(\"lpep_pickup_datetime\")) \\\n",
    "               .withColumn(\"pickup_month\", F.month(\"lpep_pickup_datetime\"))\n",
    "        before = df.count()\n",
    "        df = df.filter((F.col(\"year\") == year) & (F.col(\"pickup_month\") == month))\n",
    "        log_and_upload(f\"Filtered to {year_month}, rows remaining: {df.count()}\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Write Cleaned → Silver\n",
    "        # -------------------------\n",
    "        clean_table = f\"green_{year_month.replace('-', '_')}\"\n",
    "        df.write.format(\"bigquery\") \\\n",
    "          .option(\"table\", f\"{PROJECT_ID}.{SILVER_DATASET_NAME}.{clean_table}\") \\\n",
    "          .mode(\"overwrite\") \\\n",
    "          .save()\n",
    "        log_and_upload(f\" Written to Silver table {SILVER_DATASET_NAME}.{clean_table}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log_and_upload(f\" ERROR processing {year_month}: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60132,
     "status": "ok",
     "timestamp": 1756479537483,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "8q76Q8qpaC2t",
    "outputId": "1f5b9141-ceb5-4ae1-c5e7-67346c7ea3b9"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Stop the Spark session gracefully\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\" Spark session stopped\")\n",
    "except Exception as e:\n",
    "    print(f\" Error while stopping Spark session: {e}\")\n",
    "\n",
    "# Sleep for 60 seconds to allow quota/resources to free up\n",
    "print(\" Waiting 60s for resources to be released...\")\n",
    "time.sleep(60)\n",
    "print(\" Done waiting. You can safely start a new session now.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eyh-eJscVb1d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "cell_execution_strategy": "setup",
   "name": "02bGreenRawBronzeToCleanSilver",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
