{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rq0-1MmJvfKq",
   "metadata": {
    "id": "rq0-1MmJvfKq"
   },
   "source": [
    "NYC Taxi Fleet Recommender Pipeline\n",
    "Predicts optimal fleet mix (sedan/SUV/van) for next 15 days\n",
    "Based on passenger demand patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IaaDisAHFAfLZyqrC4sslS8R",
   "metadata": {
    "id": "IaaDisAHFAfLZyqrC4sslS8R",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "from google.cloud import bigquery, storage\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anJEW-e2vptx",
   "metadata": {
    "id": "anJEW-e2vptx"
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0u08o0fIvlxW",
   "metadata": {
    "id": "0u08o0fIvlxW"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "PROJECT_ID = \"nyctaxi-467111\"\n",
    "SOURCE_DATASET = \"CleanSilver\"\n",
    "OUTPUT_DATASET = \"PostMlGold\"\n",
    "BUCKET_NAME = \"nyc_raw_data_bucket\"\n",
    "MODEL_FOLDER = \"fleet_recommender_models\"\n",
    "\n",
    "# Taxi types to process\n",
    "TAXI_TYPES = [\"yellow\", \"green\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mRr4IwlOvrRx",
   "metadata": {
    "id": "mRr4IwlOvrRx"
   },
   "source": [
    "# Vehicle throughput assumptions (trips/vehicle/hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x2depN8qvqB9",
   "metadata": {
    "id": "x2depN8qvqB9"
   },
   "outputs": [],
   "source": [
    "THROUGHPUT = {\n",
    "    'sedan': 1.8,\n",
    "    'suv': 1.6,\n",
    "    'van': 1.3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pR8XKS78vzNZ",
   "metadata": {
    "id": "pR8XKS78vzNZ"
   },
   "source": [
    "# Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Uqeugsp7vuv-",
   "metadata": {
    "id": "Uqeugsp7vuv-"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mcrRxtBav14m",
   "metadata": {
    "id": "mcrRxtBav14m"
   },
   "source": [
    "# Initialize clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buU1jEKXv2Jn",
   "metadata": {
    "id": "buU1jEKXv2Jn"
   },
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7LLFCH9Ev38g",
   "metadata": {
    "id": "7LLFCH9Ev38g"
   },
   "source": [
    "# helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4xtYFXdpv3Wn",
   "metadata": {
    "id": "4xtYFXdpv3Wn"
   },
   "outputs": [],
   "source": [
    "def extract_features_from_bigquery(taxi_type, end_date=None):\n",
    "    \"\"\"Extract and prepare features from BigQuery tables.\"\"\"\n",
    "    logger.info(f\"Extracting features for {taxi_type} taxi...\")\n",
    "\n",
    "    if end_date is None:\n",
    "        # Get the latest date from available data\n",
    "        latest_date_query = f\"\"\"\n",
    "        SELECT MAX(DATE(pickup_datetime)) as max_date\n",
    "        FROM (\n",
    "            SELECT {'tpep' if taxi_type == 'yellow' else 'lpep'}_pickup_datetime as pickup_datetime\n",
    "            FROM `{PROJECT_ID}.{SOURCE_DATASET}.{taxi_type}*`\n",
    "        )\n",
    "        \"\"\"\n",
    "        end_date = list(bq_client.query(latest_date_query).result())[0].max_date\n",
    "\n",
    "    pickup_col = 'tpep_pickup_datetime' if taxi_type == 'yellow' else 'lpep_pickup_datetime'\n",
    "\n",
    "    query = f\"\"\"\n",
    "    WITH bounds AS (\n",
    "        SELECT\n",
    "            DATE('2024-01-01') AS min_d,  -- Start from 2024 for consistency\n",
    "            DATE('{end_date}') AS max_d\n",
    "    ),\n",
    "    zones AS (\n",
    "        SELECT DISTINCT PULocationID AS zone_id\n",
    "        FROM `{PROJECT_ID}.{SOURCE_DATASET}.{taxi_type}*`\n",
    "        WHERE PULocationID IS NOT NULL\n",
    "    ),\n",
    "    grid AS (\n",
    "        SELECT\n",
    "            d,\n",
    "            hr,\n",
    "            zone_id\n",
    "        FROM bounds b,\n",
    "        UNNEST(GENERATE_DATE_ARRAY(b.min_d, b.max_d)) AS d,\n",
    "        UNNEST(GENERATE_ARRAY(0, 23)) AS hr\n",
    "        CROSS JOIN zones\n",
    "    ),\n",
    "    raw_trips AS (\n",
    "        SELECT\n",
    "            DATE({pickup_col}) AS d,\n",
    "            EXTRACT(HOUR FROM {pickup_col}) AS hr,\n",
    "            PULocationID AS zone_id,\n",
    "            CASE\n",
    "                WHEN passenger_count <= 1.5 THEN 'single'\n",
    "                WHEN passenger_count <= 3.5 THEN 'small'\n",
    "                WHEN passenger_count <= 5.5 THEN 'medium'\n",
    "                ELSE 'large'\n",
    "            END AS bucket\n",
    "        FROM `{PROJECT_ID}.{SOURCE_DATASET}.{taxi_type}*`\n",
    "        WHERE DATE({pickup_col}) >= '2024-01-01'\n",
    "          AND DATE({pickup_col}) <= '{end_date}'\n",
    "          AND PULocationID IS NOT NULL\n",
    "    ),\n",
    "    trip_counts AS (\n",
    "        SELECT\n",
    "            d, hr, zone_id,\n",
    "            COUNTIF(bucket = 'single') AS y_single,\n",
    "            COUNTIF(bucket = 'small') AS y_small,\n",
    "            COUNTIF(bucket = 'medium') AS y_medium,\n",
    "            COUNTIF(bucket = 'large') AS y_large\n",
    "        FROM raw_trips\n",
    "        GROUP BY d, hr, zone_id\n",
    "    ),\n",
    "    filled AS (\n",
    "        SELECT\n",
    "            g.d, g.hr, g.zone_id,\n",
    "            COALESCE(c.y_single, 0) AS y_single,\n",
    "            COALESCE(c.y_small, 0) AS y_small,\n",
    "            COALESCE(c.y_medium, 0) AS y_medium,\n",
    "            COALESCE(c.y_large, 0) AS y_large,\n",
    "            COALESCE(c.y_single + c.y_small + c.y_medium + c.y_large, 0) AS total_trips\n",
    "        FROM grid g\n",
    "        LEFT JOIN trip_counts c USING (d, hr, zone_id)\n",
    "    ),\n",
    "    with_features AS (\n",
    "        SELECT\n",
    "            f.*,\n",
    "            EXTRACT(DAYOFWEEK FROM f.d) AS dow,\n",
    "            EXTRACT(MONTH FROM f.d) AS month,\n",
    "            -- Lag features\n",
    "            LAG(y_single, 1) OVER (PARTITION BY zone_id, hr ORDER BY d) AS y_single_lag1,\n",
    "            LAG(y_single, 7) OVER (PARTITION BY zone_id, hr ORDER BY d) AS y_single_lag7,\n",
    "            LAG(y_small, 1) OVER (PARTITION BY zone_id, hr ORDER BY d) AS y_small_lag1,\n",
    "            LAG(y_small, 7) OVER (PARTITION BY zone_id, hr ORDER BY d) AS y_small_lag7,\n",
    "            LAG(y_medium, 1) OVER (PARTITION BY zone_id, hr ORDER BY d) AS y_medium_lag1,\n",
    "            LAG(y_medium, 7) OVER (PARTITION BY zone_id, hr ORDER BY d) AS y_medium_lag7,\n",
    "            LAG(y_large, 1) OVER (PARTITION BY zone_id, hr ORDER BY d) AS y_large_lag1,\n",
    "            LAG(y_large, 7) OVER (PARTITION BY zone_id, hr ORDER BY d) AS y_large_lag7,\n",
    "            -- Rolling averages\n",
    "            AVG(total_trips) OVER (\n",
    "                PARTITION BY zone_id, hr\n",
    "                ORDER BY d\n",
    "                ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING\n",
    "            ) AS total_trips_7dma\n",
    "        FROM filled f\n",
    "    )\n",
    "    SELECT * FROM with_features\n",
    "    WHERE d >= DATE('2024-01-08')  -- Ensure we have lag features\n",
    "    ORDER BY zone_id, d, hr\n",
    "    \"\"\"\n",
    "\n",
    "    df = bq_client.query(query).to_dataframe()\n",
    "    logger.info(f\"Extracted {len(df):,} rows of feature data\")\n",
    "    print(f\"Extracted {len(df):,} rows of feature data\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U3EXALW8wJMp",
   "metadata": {
    "id": "U3EXALW8wJMp"
   },
   "outputs": [],
   "source": [
    "def prepare_features(df):\n",
    "    \"\"\"Add cyclical time features and handle missing values.\"\"\"\n",
    "    # Fill NaN values\n",
    "    lag_cols = [c for c in df.columns if '_lag' in c]\n",
    "    df[lag_cols] = df[lag_cols].fillna(0.0)\n",
    "    df['total_trips_7dma'] = df['total_trips_7dma'].fillna(0.0)\n",
    "\n",
    "    # Convert numeric columns to float to avoid pandas nullable integer issues\n",
    "    numeric_cols = ['total_trips', 'y_single', 'y_small', 'y_medium', 'y_large'] + lag_cols\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype('float64')\n",
    "\n",
    "    # Convert zone_id to string for one-hot encoding\n",
    "    df['zone_id'] = df['zone_id'].astype(str)\n",
    "\n",
    "    # Add cyclical time features\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hr'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hr'] / 24)\n",
    "    df['dow_sin'] = np.sin(2 * np.pi * df['dow'] / 7)\n",
    "    df['dow_cos'] = np.cos(2 * np.pi * df['dow'] / 7)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yPA21FuowdQE",
   "metadata": {
    "id": "yPA21FuowdQE"
   },
   "outputs": [],
   "source": [
    "def build_preprocessor(df):\n",
    "    \"\"\"Build feature preprocessor with one-hot encoding for zones.\"\"\"\n",
    "    cat_features = ['zone_id']\n",
    "    num_features = [\n",
    "        'hr', 'dow', 'month',\n",
    "        'total_trips', 'total_trips_7dma',\n",
    "        # Lag features\n",
    "        'y_single_lag1', 'y_single_lag7',\n",
    "        'y_small_lag1', 'y_small_lag7',\n",
    "        'y_medium_lag1', 'y_medium_lag7',\n",
    "        'y_large_lag1', 'y_large_lag7',\n",
    "        # Cyclical features\n",
    "        'hour_sin', 'hour_cos',\n",
    "        'dow_sin', 'dow_cos',\n",
    "        'month_sin', 'month_cos'\n",
    "    ]\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='infrequent_if_exist', min_frequency=50), cat_features),\n",
    "            ('num', 'passthrough', num_features)\n",
    "        ],\n",
    "        sparse_threshold=0.3\n",
    "    )\n",
    "\n",
    "    return preprocessor, cat_features + num_features\n",
    "\n",
    "\n",
    "def train_models(df, test_days=7):\n",
    "    \"\"\"Train XGBoost models for each passenger bucket.\"\"\"\n",
    "    # Split data\n",
    "    df['d'] = pd.to_datetime(df['d'])\n",
    "    cutoff_date = df['d'].max() - timedelta(days=test_days)\n",
    "\n",
    "    train_df = df[df['d'] <= cutoff_date].copy()\n",
    "    test_df = df[df['d'] > cutoff_date].copy()\n",
    "\n",
    "    logger.info(f\"Training period: {train_df['d'].min()} to {train_df['d'].max()}\")\n",
    "    logger.info(f\"Test period: {test_df['d'].min()} to {test_df['d'].max()}\")\n",
    "    print(f\"Training period: {train_df['d'].min()} to {train_df['d'].max()}\")\n",
    "    print(f\"Test period: {test_df['d'].min()} to {test_df['d'].max()}\")\n",
    "\n",
    "    # Prepare features\n",
    "    preprocessor, feature_cols = build_preprocessor(df)\n",
    "    X_train = preprocessor.fit_transform(train_df[feature_cols])\n",
    "    X_test = preprocessor.transform(test_df[feature_cols])\n",
    "\n",
    "    # Target columns\n",
    "    targets = ['y_single', 'y_small', 'y_medium', 'y_large']\n",
    "\n",
    "    # Train models\n",
    "    models = {}\n",
    "    metrics = {}\n",
    "\n",
    "    for target in tqdm(targets):\n",
    "        logger.info(f\"Training model for {target}...\")\n",
    "\n",
    "        y_train = train_df[target].values\n",
    "        y_test = test_df[target].values\n",
    "\n",
    "        model = XGBRegressor(\n",
    "            objective='count:poisson',\n",
    "            max_delta_step=1,\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=6,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        # Evaluate\n",
    "        y_pred = model.predict(X_test)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "        models[target] = model\n",
    "        metrics[target] = {'mae': mae, 'rmse': rmse}\n",
    "\n",
    "        logger.info(f\"  {target} - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "        print(f\"  {target} - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "    return models, preprocessor, metrics, feature_cols, cutoff_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ErJkKXmk2NEH",
   "metadata": {
    "id": "ErJkKXmk2NEH"
   },
   "outputs": [],
   "source": [
    "def predict_future(df, models, preprocessor, feature_cols, last_date, days_ahead=15):\n",
    "    \"\"\"Generate predictions for future days.\"\"\"\n",
    "    # Get unique zones\n",
    "    zones = df['zone_id'].unique()\n",
    "\n",
    "    # Generate future dates\n",
    "    future_dates = pd.date_range(\n",
    "        start=last_date + timedelta(days=1),\n",
    "        end=last_date + timedelta(days=days_ahead),\n",
    "        freq='D'\n",
    "    )\n",
    "\n",
    "    # Create future grid\n",
    "    future_data = []\n",
    "    for date in future_dates:\n",
    "        for hour in range(24):\n",
    "            for zone in zones:\n",
    "                future_data.append({\n",
    "                    'd': date,\n",
    "                    'hr': hour,\n",
    "                    'zone_id': zone\n",
    "                })\n",
    "\n",
    "    future_df = pd.DataFrame(future_data)\n",
    "\n",
    "    # Add time features\n",
    "    future_df['dow'] = future_df['d'].dt.dayofweek\n",
    "    future_df['month'] = future_df['d'].dt.month\n",
    "\n",
    "    # Add cyclical features\n",
    "    future_df['hour_sin'] = np.sin(2 * np.pi * future_df['hr'] / 24)\n",
    "    future_df['hour_cos'] = np.cos(2 * np.pi * future_df['hr'] / 24)\n",
    "    future_df['dow_sin'] = np.sin(2 * np.pi * future_df['dow'] / 7)\n",
    "    future_df['dow_cos'] = np.cos(2 * np.pi * future_df['dow'] / 7)\n",
    "    future_df['month_sin'] = np.sin(2 * np.pi * future_df['month'] / 12)\n",
    "    future_df['month_cos'] = np.cos(2 * np.pi * future_df['month'] / 12)\n",
    "\n",
    "    # Get historical features for lag values\n",
    "    # Use averages by zone, hour, and day of week\n",
    "    historical_avgs = df.groupby(['zone_id', 'hr', 'dow']).agg({\n",
    "        'y_single': 'mean',\n",
    "        'y_small': 'mean',\n",
    "        'y_medium': 'mean',\n",
    "        'y_large': 'mean',\n",
    "        'total_trips': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Merge with future data\n",
    "    future_df = future_df.merge(\n",
    "        historical_avgs,\n",
    "        on=['zone_id', 'hr', 'dow'],\n",
    "        how='left',\n",
    "        suffixes=('', '_avg')\n",
    "    )\n",
    "\n",
    "    # Fill lag features with historical averages\n",
    "    future_df['y_single_lag1'] = future_df['y_single']\n",
    "    future_df['y_single_lag7'] = future_df['y_single']\n",
    "    future_df['y_small_lag1'] = future_df['y_small']\n",
    "    future_df['y_small_lag7'] = future_df['y_small']\n",
    "    future_df['y_medium_lag1'] = future_df['y_medium']\n",
    "    future_df['y_medium_lag7'] = future_df['y_medium']\n",
    "    future_df['y_large_lag1'] = future_df['y_large']\n",
    "    future_df['y_large_lag7'] = future_df['y_large']\n",
    "    future_df['total_trips_7dma'] = future_df['total_trips']\n",
    "\n",
    "    # Make predictions\n",
    "    X_future = preprocessor.transform(future_df[feature_cols])\n",
    "\n",
    "    for target in ['y_single', 'y_small', 'y_medium', 'y_large']:\n",
    "        future_df[f'pred_{target}'] = np.maximum(0, models[target].predict(X_future))\n",
    "\n",
    "    return future_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "huYrONaS2S1l",
   "metadata": {
    "id": "huYrONaS2S1l"
   },
   "outputs": [],
   "source": [
    "def calculate_fleet_requirements(df):\n",
    "    \"\"\"Convert passenger predictions to vehicle requirements.\"\"\"\n",
    "    # Calculate vehicle needs based on throughput\n",
    "    df['veh_sedan'] = np.ceil(\n",
    "        (df['pred_y_single'] + 0.5 * df['pred_y_small']) / THROUGHPUT['sedan']\n",
    "    ).astype(int)\n",
    "\n",
    "    df['veh_suv'] = np.ceil(\n",
    "        (0.5 * df['pred_y_small'] + 0.7 * df['pred_y_medium'] + 0.1 * df['pred_y_large']) / THROUGHPUT['suv']\n",
    "    ).astype(int)\n",
    "\n",
    "    df['veh_van'] = np.ceil(\n",
    "        (0.3 * df['pred_y_medium'] + 0.9 * df['pred_y_large']) / THROUGHPUT['van']\n",
    "    ).astype(int)\n",
    "\n",
    "    df['total_vehicles'] = df['veh_sedan'] + df['veh_suv'] + df['veh_van']\n",
    "    df['total_pred_trips'] = (\n",
    "        df['pred_y_single'] + df['pred_y_small'] +\n",
    "        df['pred_y_medium'] + df['pred_y_large']\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_results(df, taxi_type, metrics):\n",
    "    \"\"\"Save predictions and model metrics to BigQuery.\"\"\"\n",
    "    # Prepare output table\n",
    "    output_table = f\"{PROJECT_ID}.{OUTPUT_DATASET}.fleet_recommender_{taxi_type}_predictions\"\n",
    "    metrics_table = f\"{PROJECT_ID}.{OUTPUT_DATASET}.fleet_recommender_{taxi_type}_metrics\"\n",
    "\n",
    "    # Add metadata\n",
    "    df['taxi_type'] = taxi_type\n",
    "    df['prediction_timestamp'] = datetime.now()\n",
    "    df['date'] = df['d'].dt.date\n",
    "\n",
    "    # Select and rename columns for output\n",
    "    output_df = df[[\n",
    "        'taxi_type', 'date', 'hr', 'zone_id',\n",
    "        'dow', 'month',\n",
    "        'pred_y_single', 'pred_y_small', 'pred_y_medium', 'pred_y_large',\n",
    "        'veh_sedan', 'veh_suv', 'veh_van',\n",
    "        'total_vehicles', 'total_pred_trips',\n",
    "        'prediction_timestamp'\n",
    "    ]].copy()\n",
    "\n",
    "    # Save predictions\n",
    "    output_df.to_gbq(\n",
    "        output_table,\n",
    "        project_id=PROJECT_ID,\n",
    "        if_exists='replace'\n",
    "    )\n",
    "    logger.info(f\"Saved predictions to {output_table}\")\n",
    "    print(f\"Saved predictions to {output_table}\")\n",
    "\n",
    "    # Save metrics\n",
    "    metrics_df = pd.DataFrame([\n",
    "        {\n",
    "            'taxi_type': taxi_type,\n",
    "            'target': target,\n",
    "            'mae': metrics[target]['mae'],\n",
    "            'rmse': metrics[target]['rmse'],\n",
    "            'timestamp': datetime.now()\n",
    "        }\n",
    "        for target in metrics\n",
    "    ])\n",
    "\n",
    "    metrics_df.to_gbq(\n",
    "        metrics_table,\n",
    "        project_id=PROJECT_ID,\n",
    "        if_exists='replace'\n",
    "    )\n",
    "    logger.info(f\"Saved metrics to {metrics_table}\")\n",
    "    print(f\"Saved metrics to {metrics_table}\")\n",
    "\n",
    "\n",
    "def save_models_to_gcs(models, preprocessor, taxi_type):\n",
    "    \"\"\"Save trained models and preprocessor to GCS.\"\"\"\n",
    "    bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "\n",
    "    # Save models\n",
    "    for target, model in models.items():\n",
    "        model_path = f\"{MODEL_FOLDER}/{taxi_type}_{target}_model.joblib\"\n",
    "        blob = bucket.blob(model_path)\n",
    "        with blob.open('wb') as f:\n",
    "            joblib.dump(model, f)\n",
    "        logger.info(f\"Saved model to gs://{BUCKET_NAME}/{model_path}\")\n",
    "\n",
    "    # Save preprocessor\n",
    "    preprocessor_path = f\"{MODEL_FOLDER}/{taxi_type}_preprocessor.joblib\"\n",
    "    blob = bucket.blob(preprocessor_path)\n",
    "    with blob.open('wb') as f:\n",
    "        joblib.dump(preprocessor, f)\n",
    "    logger.info(f\"Saved preprocessor to gs://{BUCKET_NAME}/{preprocessor_path}\")\n",
    "    print(f\"Saved preprocessor to gs://{BUCKET_NAME}/{preprocessor_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4_EP2LYLy-6P",
   "metadata": {
    "id": "4_EP2LYLy-6P"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main pipeline execution.\"\"\"\n",
    "    logger.info(\"Starting NYC Taxi Fleet Recommender Pipeline\")\n",
    "\n",
    "    for taxi_type in tqdm(TAXI_TYPES):\n",
    "        logger.info(f\"\\nProcessing {taxi_type} taxi...\")\n",
    "\n",
    "        try:\n",
    "            # Extract features\n",
    "            df = extract_features_from_bigquery(taxi_type)\n",
    "            df = prepare_features(df)\n",
    "\n",
    "            # Train models\n",
    "            models, preprocessor, metrics, feature_cols, last_date = train_models(df)\n",
    "\n",
    "            # Generate future predictions\n",
    "            future_df = predict_future(df, models, preprocessor, feature_cols, last_date)\n",
    "            future_df = calculate_fleet_requirements(future_df)\n",
    "\n",
    "            # Save results\n",
    "            save_results(future_df, taxi_type, metrics)\n",
    "            save_models_to_gcs(models, preprocessor, taxi_type)\n",
    "\n",
    "            logger.info(f\"Completed processing for {taxi_type} taxi\")\n",
    "            print((f\"Completed processing for {taxi_type} taxi\"))\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {taxi_type}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    logger.info(\"\\nPipeline completed successfully!\")\n",
    "\n",
    "    # Create summary view\n",
    "    summary_query = f\"\"\"\n",
    "    CREATE OR REPLACE VIEW `{PROJECT_ID}.{OUTPUT_DATASET}.fleet_recommender_summary` AS\n",
    "    WITH combined AS (\n",
    "        SELECT * FROM `{PROJECT_ID}.{OUTPUT_DATASET}.fleet_recommender_yellow_predictions`\n",
    "        UNION ALL\n",
    "        SELECT * FROM `{PROJECT_ID}.{OUTPUT_DATASET}.fleet_recommender_green_predictions`\n",
    "    )\n",
    "    SELECT\n",
    "        taxi_type,\n",
    "        date,\n",
    "        SUM(total_vehicles) as total_fleet_needed,\n",
    "        SUM(veh_sedan) as total_sedans,\n",
    "        SUM(veh_suv) as total_suvs,\n",
    "        SUM(veh_van) as total_vans,\n",
    "        SUM(total_pred_trips) as total_expected_trips,\n",
    "        COUNT(DISTINCT zone_id) as active_zones\n",
    "    FROM combined\n",
    "    GROUP BY taxi_type, date\n",
    "    ORDER BY taxi_type, date\n",
    "    \"\"\"\n",
    "\n",
    "    bq_client.query(summary_query).result()\n",
    "    logger.info(\"Created fleet summary view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FULVHSvvwUe2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 750824,
     "status": "ok",
     "timestamp": 1756473369180,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "FULVHSvvwUe2",
    "outputId": "e585ee64-0acb-45fc-d462-cec69dd01206"
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QEvZILby5WqZ",
   "metadata": {
    "id": "QEvZILby5WqZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "04aXGBostFleetRecommender",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
