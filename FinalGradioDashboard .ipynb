{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bR3BWkU59E97",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4093,
     "status": "ok",
     "timestamp": 1757012266352,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "bR3BWkU59E97",
    "outputId": "6f0b8c5b-54b8-492b-bc80-aeb8e88237da"
   },
   "outputs": [],
   "source": [
    "%pip install prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y4LWC6VT9T98",
   "metadata": {
    "executionInfo": {
     "elapsed": 901,
     "status": "ok",
     "timestamp": 1757012267251,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "y4LWC6VT9T98"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "# Try to import Prophet\n",
    "\n",
    "try:\n",
    "\n",
    "    from prophet import Prophet\n",
    "\n",
    "    PROPHET_AVAILABLE = True\n",
    "\n",
    "except ImportError:\n",
    "\n",
    "    PROPHET_AVAILABLE = False\n",
    "\n",
    "    print(\"Prophet not available. Install with: pip install prophet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AZN6KXLRamXNxNHOgcqQ4Q1b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "executionInfo": {
     "elapsed": 9294,
     "status": "ok",
     "timestamp": 1757012276543,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "AZN6KXLRamXNxNHOgcqQ4Q1b",
    "outputId": "cc76a0e1-f831-4c23-89ba-37e7821255d1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from google.cloud import bigquery\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1YnuDn7bx4sq",
   "metadata": {
    "id": "1YnuDn7bx4sq"
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tGSBAcKnx2zP",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1757012276543,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "tGSBAcKnx2zP"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = os.environ.get(\"PROJECT_ID\", \"nyctaxi-467111\")\n",
    "BUCKET_NAME = \"nyc_raw_data_bucket\"\n",
    "DATASET_NAME = \"PreMlGold\"\n",
    "OUTPUT_DATASET = \"PostMlGold\"\n",
    "MlDATASET_NAME = \"PostMlGold\"\n",
    "TAXI_TYPES = [\"yellow\", \"green\", \"fhv\", \"fhvhv\"]\n",
    "\n",
    "# Initialize BigQuery client\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# Global variables for data persistence across tabs\n",
    "global_df = None\n",
    "city_ts = None\n",
    "trip_scaled = None\n",
    "ml_taxi_type_value = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LjIyCBAm9FBb",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1757012276543,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "LjIyCBAm9FBb"
   },
   "outputs": [],
   "source": [
    "bq_client = client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vrJOJoE5FsHw",
   "metadata": {
    "id": "vrJOJoE5FsHw"
   },
   "source": [
    "## holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "W2sBYlrJFuaO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 422,
     "status": "ok",
     "timestamp": 1757012276963,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "W2sBYlrJFuaO",
    "outputId": "00f172cd-cfc8-4f45-ee8b-b7acf5a52075"
   },
   "outputs": [],
   "source": [
    "import holidays\n",
    "\n",
    "# US holidays for 2024\n",
    "us_holidays = holidays.US(years=[2023,2024,2025])\n",
    "\n",
    "us_holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qFnW0i7wGQ6y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1757012276963,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "qFnW0i7wGQ6y",
    "outputId": "2656efa8-9907-40d4-e391-a7ee59fedea4"
   },
   "outputs": [],
   "source": [
    "US_HOLIDAYS = [ (str(d), hname) for d,hname in us_holidays.items()]\n",
    "US_HOLIDAYS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9UsZNeohx-qx",
   "metadata": {
    "id": "9UsZNeohx-qx"
   },
   "source": [
    "# ==================== Data Access Functions ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QSNzaTPTx9_f",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1757012276963,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "QSNzaTPTx9_f"
   },
   "outputs": [],
   "source": [
    "def get_available_partitions(taxi_type: str) -> list:\n",
    "    \"\"\"Get available year_month partitions for a taxi type.\"\"\"\n",
    "    query = f\"\"\"\n",
    "    SELECT table_name\n",
    "    FROM `{PROJECT_ID}.{DATASET_NAME}.INFORMATION_SCHEMA.TABLES`\n",
    "    WHERE REGEXP_CONTAINS(table_name, r'^{taxi_type}_[0-9]{{4}}_[0-9]{{2}}_hourly$')\n",
    "    \"\"\"\n",
    "\n",
    "    results = client.query(query).result()\n",
    "    partitions = []\n",
    "\n",
    "    import re\n",
    "    pattern = re.compile(rf'^{taxi_type}_(\\d{{4}})_(\\d{{2}})_hourly$')\n",
    "\n",
    "    for row in results:\n",
    "        match = pattern.match(row.table_name)\n",
    "        if match:\n",
    "            year, month = match.groups()\n",
    "            partitions.append(f\"{year}_{month}\")\n",
    "\n",
    "    return sorted(partitions)\n",
    "\n",
    "def load_partitions(taxi_type: str, partitions: list) -> pd.DataFrame:\n",
    "    \"\"\"Load and union selected monthly partitions.\"\"\"\n",
    "    if not partitions:\n",
    "        raise ValueError(\"No partitions selected\")\n",
    "\n",
    "    union_queries = []\n",
    "    for part in partitions:\n",
    "        table_name = f\"{taxi_type}_{part}_hourly\"\n",
    "        union_queries.append(f\"\"\"\n",
    "            SELECT pickup_date, pickup_hour, trips\n",
    "            FROM `{PROJECT_ID}.{DATASET_NAME}.{table_name}`\n",
    "        \"\"\")\n",
    "\n",
    "    full_query = \" UNION ALL \".join(union_queries)\n",
    "    df = client.query(full_query).to_dataframe()\n",
    "    return df\n",
    "\n",
    "def fetch_time_series_data(taxi_type: str, partitions: list, granularity: str = \"Daily\") -> pd.DataFrame:\n",
    "    \"\"\"Fetch aggregated time series data.\"\"\"\n",
    "    if not partitions:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Build UNION query for selected partitions\n",
    "    union_queries = []\n",
    "    for part in partitions:\n",
    "        table_name = f\"{taxi_type}_{part}_hourly\"\n",
    "        union_queries.append(f\"\"\"\n",
    "            SELECT pickup_date, pickup_hour,\n",
    "                   SUM(trips) as trips,\n",
    "                   SUM(revenue) as revenue\n",
    "            FROM `{PROJECT_ID}.{DATASET_NAME}.{table_name}`\n",
    "            GROUP BY pickup_date, pickup_hour\n",
    "        \"\"\")\n",
    "\n",
    "    query = f\"\"\"\n",
    "    WITH all_data AS (\n",
    "        {\" UNION ALL \".join(union_queries)}\n",
    "    )\n",
    "    SELECT pickup_date, pickup_hour,\n",
    "           SUM(trips) as total_trips,\n",
    "           SUM(revenue) as total_revenue\n",
    "    FROM all_data\n",
    "    GROUP BY pickup_date, pickup_hour\n",
    "    ORDER BY pickup_date, pickup_hour\n",
    "    \"\"\"\n",
    "\n",
    "    df = client.query(query).to_dataframe()\n",
    "\n",
    "    # Create datetime column\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_date']) + pd.to_timedelta(df['pickup_hour'], unit='h')\n",
    "\n",
    "    if granularity == \"Daily\":\n",
    "        df = df.groupby(pd.Grouper(key='pickup_datetime', freq='D')).agg({\n",
    "            'total_trips': 'sum',\n",
    "            'total_revenue': 'sum'\n",
    "        }).reset_index()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GegGt3smyEXS",
   "metadata": {
    "id": "GegGt3smyEXS"
   },
   "source": [
    "# ==================== Feature Engineering ====================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JGucoMxOyCN2",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1757012276963,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "JGucoMxOyCN2"
   },
   "outputs": [],
   "source": [
    "def create_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create time-based features for ML models.\"\"\"\n",
    "    # Combine date and hour into datetime index\n",
    "    df['datetime'] = pd.to_datetime(df['pickup_date'].astype(str) + ' ' +\n",
    "                                   df['pickup_hour'].astype(str) + ':00:00')\n",
    "    df = df.set_index('datetime').sort_index()\n",
    "    df = df.drop(['pickup_date', 'pickup_hour'], axis=1, errors='ignore')\n",
    "\n",
    "    # Calendar features\n",
    "    df['hour'] = df.index.hour\n",
    "    df['weekday'] = df.index.dayofweek\n",
    "    df['day_of_month'] = df.index.day\n",
    "    df['week_of_year'] = df.index.isocalendar().week.astype(int)\n",
    "    df['is_weekend'] = (df['weekday'] >= 5).astype(int)\n",
    "\n",
    "    # Cyclical encoding\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['weekday_sin'] = np.sin(2 * np.pi * df['weekday'] / 7)\n",
    "    df['weekday_cos'] = np.cos(2 * np.pi * df['weekday'] / 7)\n",
    "\n",
    "    # Lag features (avoid data leakage)\n",
    "    df['lag_1h'] = df['trips'].shift(1)\n",
    "    df['lag_24h'] = df['trips'].shift(24)\n",
    "    df['lag_168h'] = df['trips'].shift(168)  # 1 week\n",
    "\n",
    "    # Rolling statistics (with shift to avoid leakage)\n",
    "    df['rolling_mean_3h'] = df['trips'].shift(1).rolling(window=3, min_periods=1).mean()\n",
    "    df['rolling_mean_24h'] = df['trips'].shift(1).rolling(window=24, min_periods=1).mean()\n",
    "    df['rolling_std_24h'] = df['trips'].shift(1).rolling(window=24, min_periods=1).std()\n",
    "\n",
    "    # Drop rows with NaN values from lag features\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7PWU7eSHGtnH",
   "metadata": {
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1757012277246,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "7PWU7eSHGtnH"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_actual_data_from_gcs(taxi_type, start_date, end_date):\n",
    "    \"\"\"Load actual data from BigQuery for the forecast period if available.\"\"\"\n",
    "    if not taxi_type:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        # Convert dates to get the partitions we need\n",
    "        start_year_month = f\"{start_date.year}_{start_date.strftime('%m')}\"\n",
    "        end_year_month = f\"{end_date.year}_{end_date.strftime('%m')}\"\n",
    "\n",
    "        # Get available partitions\n",
    "        all_partitions = get_available_partitions(taxi_type)\n",
    "\n",
    "        # Filter partitions that fall within our date range\n",
    "        relevant_partitions = []\n",
    "        for partition in all_partitions:\n",
    "            if start_year_month <= partition <= end_year_month:\n",
    "                relevant_partitions.append(partition)\n",
    "\n",
    "        if not relevant_partitions:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Build query to get hourly data for the exact date range\n",
    "        union_queries = []\n",
    "        for partition in relevant_partitions:\n",
    "            table_name = f\"{taxi_type}_{partition}_hourly\"\n",
    "            union_queries.append(f\"\"\"\n",
    "                SELECT pickup_date, pickup_hour, trips\n",
    "                FROM `{PROJECT_ID}.{DATASET_NAME}.{table_name}`\n",
    "                WHERE DATETIME(pickup_date, TIME(pickup_hour, 0, 0)) >= '{start_date.strftime('%Y-%m-%d %H:%M:%S')}'\n",
    "                  AND DATETIME(pickup_date, TIME(pickup_hour, 0, 0)) <= '{end_date.strftime('%Y-%m-%d %H:%M:%S')}'\n",
    "            \"\"\")\n",
    "\n",
    "        full_query = \" UNION ALL \".join(union_queries)\n",
    "\n",
    "        # Execute query\n",
    "        df = client.query(full_query).to_dataframe()\n",
    "\n",
    "        if df.empty:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Create datetime index\n",
    "        df['datetime'] = pd.to_datetime(df['pickup_date']) + pd.to_timedelta(df['pickup_hour'], unit='h')\n",
    "        df = df.set_index('datetime').sort_index()\n",
    "\n",
    "        return df[['trips']]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading actual future data: {str(e)}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hiy0yLqMyH8f",
   "metadata": {
    "id": "hiy0yLqMyH8f"
   },
   "source": [
    "# ==================== Visualization Functions ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8nuSX57AyG3u",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1757012277246,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "8nuSX57AyG3u"
   },
   "outputs": [],
   "source": [
    "def plot_time_series(taxi_type, partitions, metric, granularity):\n",
    "    \"\"\"Create interactive time series plot.\"\"\"\n",
    "    if not taxi_type or not partitions:\n",
    "        return px.line(title=\"Please select taxi type and partitions\")\n",
    "\n",
    "    df = fetch_time_series_data(taxi_type, partitions, granularity)\n",
    "\n",
    "    if df.empty:\n",
    "        return px.line(title=\"No data available for selected options\")\n",
    "\n",
    "    y_col = \"total_trips\" if metric == \"Trips\" else \"total_revenue\"\n",
    "    title = f\"{taxi_type.title()} Taxi - {metric} ({granularity})\"\n",
    "\n",
    "    fig = px.line(df, x=\"pickup_datetime\", y=y_col, title=title, markers=True)\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=metric,\n",
    "        hovermode=\"x unified\",\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2BRiGvRHyMqY",
   "metadata": {
    "id": "2BRiGvRHyMqY"
   },
   "source": [
    "# ==================== ML Functions ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ASliyYXykiJy",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1757012277246,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "ASliyYXykiJy"
   },
   "outputs": [],
   "source": [
    "def load_and_prepare_data_with_metric(taxi_type, start_partition, end_partition, metric=\"trips\"):\n",
    "    \"\"\"Load data and prepare for ML with selected metric.\"\"\"\n",
    "    global global_df, city_ts, trip_scaled, ml_taxi_type_value, selected_metric\n",
    "\n",
    "    selected_metric = metric  # Store selected metric globally\n",
    "\n",
    "    if not taxi_type or not start_partition or not end_partition:\n",
    "        return \"Please select taxi type and date range\"\n",
    "\n",
    "    try:\n",
    "        # Store the taxi type globally\n",
    "        ml_taxi_type_value = taxi_type\n",
    "\n",
    "        # Get all partitions in range\n",
    "        all_partitions = get_available_partitions(taxi_type)\n",
    "\n",
    "        # Filter partitions within range\n",
    "        selected_partitions = []\n",
    "        for part in all_partitions:\n",
    "            if start_partition <= part <= end_partition:\n",
    "                selected_partitions.append(part)\n",
    "\n",
    "        if not selected_partitions:\n",
    "            return \"No data available in selected range\"\n",
    "\n",
    "        # Modified query to include revenue\n",
    "        union_queries = []\n",
    "        for part in selected_partitions:\n",
    "            table_name = f\"{taxi_type}_{part}_hourly\"\n",
    "            union_queries.append(f\"\"\"\n",
    "                SELECT pickup_date, pickup_hour, trips, revenue\n",
    "                FROM `{PROJECT_ID}.{DATASET_NAME}.{table_name}`\n",
    "            \"\"\")\n",
    "\n",
    "        full_query = \" UNION ALL \".join(union_queries)\n",
    "        raw_df = client.query(full_query).to_dataframe()\n",
    "\n",
    "        # Create a copy with the selected metric\n",
    "        raw_df['metric_value'] = raw_df[metric].copy()\n",
    "\n",
    "        # Replace 'trips' column with selected metric for feature engineering\n",
    "        original_trips = raw_df['trips'].copy()\n",
    "        if metric == \"revenue\":\n",
    "            raw_df['trips'] = raw_df['revenue']\n",
    "\n",
    "        # Create features\n",
    "        global_df = create_features(raw_df)\n",
    "\n",
    "        # Restore metric name for clarity\n",
    "        global_df['metric_value'] = global_df['trips'].copy()\n",
    "\n",
    "        # Prepare time series data\n",
    "        city_ts = pd.DataFrame({\n",
    "            'date_hour': global_df.index,\n",
    "            'trips': global_df['trips'].values,\n",
    "            'metric_value': global_df['trips'].values\n",
    "        }).reset_index(drop=True)\n",
    "\n",
    "        # Scale data for anomaly detection\n",
    "        scaler = MinMaxScaler()\n",
    "        trip_scaled = scaler.fit_transform(city_ts[['trips']].values)\n",
    "\n",
    "        return f\"Successfully loaded {len(selected_partitions)} partition(s) for {taxi_type} taxi\\n\" \\\n",
    "               f\"Data shape: {global_df.shape}\\n\" \\\n",
    "               f\"Date range: {global_df.index.min()} to {global_df.index.max()}\\n\" \\\n",
    "               f\"Forecasting metric: {metric.upper()}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error loading data: {str(e)}\"\n",
    "\n",
    "\n",
    "def train_xgboost_with_ci(train_df, test_df, forecast_days, confidence_level):\n",
    "    \"\"\"Train XGBoost with confidence intervals using multiple models approach.\"\"\"\n",
    "    # Remove 'revenue' from feature_cols since it might not exist after metric selection\n",
    "    feature_cols = [col for col in train_df.columns\n",
    "                    if col not in ['trips', 'date_hour', 'metric_value', 'revenue', 'trips_original']]\n",
    "\n",
    "    # Ensure all feature columns exist in the dataframe\n",
    "    feature_cols = [col for col in feature_cols if col in train_df.columns]\n",
    "\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df['trips']\n",
    "    X_test = test_df[feature_cols]\n",
    "    y_test = test_df['trips']\n",
    "\n",
    "    # Calculate quantiles for confidence intervals\n",
    "    alpha = 1 - confidence_level\n",
    "    lower_q = alpha / 2\n",
    "    upper_q = 1 - alpha / 2\n",
    "\n",
    "    # Train main model for point predictions\n",
    "    main_model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    main_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "\n",
    "    # For confidence intervals, use prediction intervals based on residuals\n",
    "    # This is more reliable than quantile regression for XGBoost\n",
    "    test_pred_mean = main_model.predict(X_test)\n",
    "\n",
    "    # Calculate residuals on training set for variance estimation\n",
    "    train_pred = main_model.predict(X_train)\n",
    "    residuals = y_train - train_pred\n",
    "    residual_std = np.std(residuals)\n",
    "\n",
    "    # Calculate confidence intervals\n",
    "    z_score = 1.96 if confidence_level == 0.95 else 2.576\n",
    "    test_pred_lower = test_pred_mean - z_score * residual_std\n",
    "    test_pred_upper = test_pred_mean + z_score * residual_std\n",
    "\n",
    "    # Future predictions\n",
    "    last_date = test_df.index.max()\n",
    "    future_hours = forecast_days * 24\n",
    "    future_pred_mean = []\n",
    "    prediction_times = []\n",
    "\n",
    "    # Use recent trips for lag features\n",
    "    recent_trips = list(train_df['trips'].iloc[-168:].values) + list(test_df['trips'].values)\n",
    "\n",
    "    for h in range(future_hours):\n",
    "        pred_time = last_date + timedelta(hours=h+1)\n",
    "        prediction_times.append(pred_time)\n",
    "\n",
    "        future_features = pd.DataFrame(index=[pred_time])\n",
    "\n",
    "        # Time features\n",
    "        future_features['hour'] = pred_time.hour\n",
    "        future_features['weekday'] = pred_time.weekday()\n",
    "        future_features['day_of_month'] = pred_time.day\n",
    "        future_features['week_of_year'] = pred_time.isocalendar()[1]\n",
    "        future_features['is_weekend'] = int(pred_time.weekday() >= 5)\n",
    "\n",
    "        # Cyclical encoding\n",
    "        future_features['hour_sin'] = np.sin(2 * np.pi * pred_time.hour / 24)\n",
    "        future_features['hour_cos'] = np.cos(2 * np.pi * pred_time.hour / 24)\n",
    "        future_features['weekday_sin'] = np.sin(2 * np.pi * pred_time.weekday() / 7)\n",
    "        future_features['weekday_cos'] = np.cos(2 * np.pi * pred_time.weekday() / 7)\n",
    "\n",
    "        # Lag features (use mean prediction for consistency)\n",
    "        if h == 0:\n",
    "            future_features['lag_1h'] = recent_trips[-1]\n",
    "            future_features['lag_24h'] = recent_trips[-24] if len(recent_trips) >= 24 else recent_trips[0]\n",
    "            future_features['lag_168h'] = recent_trips[-168] if len(recent_trips) >= 168 else recent_trips[0]\n",
    "        else:\n",
    "            future_features['lag_1h'] = future_pred_mean[-1]\n",
    "            future_features['lag_24h'] = future_pred_mean[-24] if len(future_pred_mean) >= 24 else recent_trips[-24]\n",
    "            future_features['lag_168h'] = future_pred_mean[-168] if len(future_pred_mean) >= 168 else recent_trips[-168] if len(recent_trips) >= 168 else recent_trips[0]\n",
    "\n",
    "        # Rolling features\n",
    "        recent_for_rolling = recent_trips[-24:] + future_pred_mean\n",
    "        future_features['rolling_mean_3h'] = np.mean(recent_for_rolling[-3:]) if len(recent_for_rolling) >= 3 else np.mean(recent_trips[-3:])\n",
    "        future_features['rolling_mean_24h'] = np.mean(recent_for_rolling[-24:]) if len(recent_for_rolling) >= 24 else np.mean(recent_trips[-24:])\n",
    "        future_features['rolling_std_24h'] = np.std(recent_for_rolling[-24:]) if len(recent_for_rolling) >= 24 else np.std(recent_trips[-24:])\n",
    "\n",
    "        # Ensure we only use features that exist in our feature_cols\n",
    "        available_features = [col for col in feature_cols if col in future_features.columns]\n",
    "\n",
    "        # Make predictions\n",
    "        pred_mean = main_model.predict(future_features[available_features])[0]\n",
    "        future_pred_mean.append(pred_mean)\n",
    "\n",
    "        recent_trips.append(pred_mean)\n",
    "        if len(recent_trips) > 168:\n",
    "            recent_trips.pop(0)\n",
    "\n",
    "    # Convert to arrays\n",
    "    future_pred_mean = np.array(future_pred_mean)\n",
    "\n",
    "    # Calculate confidence intervals for future predictions\n",
    "    # Increase uncertainty for future predictions\n",
    "    future_uncertainty_factor = 1 + 0.05 * np.arange(1, future_hours + 1)  # 5% increase per hour\n",
    "    future_pred_lower = future_pred_mean - z_score * residual_std * future_uncertainty_factor\n",
    "    future_pred_upper = future_pred_mean + z_score * residual_std * future_uncertainty_factor\n",
    "\n",
    "    return (test_pred_mean, test_pred_lower, test_pred_upper,\n",
    "            future_pred_mean, future_pred_lower, future_pred_upper,\n",
    "            prediction_times)\n",
    "\n",
    "\n",
    "def train_combined_forecast_with_ci(forecast_days=7, confidence_level=0.95, show_actual=False, metric=\"trips\"):\n",
    "    \"\"\"Train models and show predictions with confidence intervals.\"\"\"\n",
    "    global global_df, selected_metric, ml_taxi_type_value\n",
    "\n",
    "    if global_df is None or global_df.empty:\n",
    "        return px.line(title=\"Please load data first\")\n",
    "\n",
    "    # Use the stored metric\n",
    "    metric_label = selected_metric.upper() if 'selected_metric' in globals() else metric.upper()\n",
    "\n",
    "    # Create train/test split\n",
    "    last_date = global_df.index.max()\n",
    "    cutoff_date = last_date - timedelta(days=30)\n",
    "\n",
    "    train_df = global_df[global_df.index <= cutoff_date].copy()\n",
    "    test_df = global_df[global_df.index > cutoff_date].copy()\n",
    "\n",
    "    if len(train_df) < 100 or len(test_df) < 10:\n",
    "        return px.line(title=\"Insufficient data for training. Please load more partitions.\")\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add historical data\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=train_df.index, y=train_df['trips'],\n",
    "        mode='lines',\n",
    "        name='Historical Data',\n",
    "        line=dict(color='gray', width=1),\n",
    "        opacity=0.7\n",
    "    ))\n",
    "\n",
    "    # Add actual test data\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=test_df.index, y=test_df['trips'],\n",
    "        mode='lines',\n",
    "        name='Actual (Test Period)',\n",
    "        line=dict(color='black', width=2)\n",
    "    ))\n",
    "\n",
    "    # XGBoost Predictions with confidence intervals\n",
    "    xgb_test_mean, xgb_test_lower, xgb_test_upper, \\\n",
    "    xgb_future_mean, xgb_future_lower, xgb_future_upper, xgb_future_times = train_xgboost_with_ci(\n",
    "        train_df, test_df, forecast_days, confidence_level\n",
    "    )\n",
    "\n",
    "    # Calculate metrics for XGBoost\n",
    "    xgb_mae = mean_absolute_error(test_df['trips'], xgb_test_mean)\n",
    "    xgb_rmse = np.sqrt(mean_squared_error(test_df['trips'], xgb_test_mean))\n",
    "    xgb_mse = mean_squared_error(test_df['trips'], xgb_test_mean)\n",
    "\n",
    "    # Add XGBoost test predictions with CI\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=test_df.index, y=xgb_test_mean,\n",
    "        mode='lines',\n",
    "        name='XGBoost (Test)',\n",
    "        line=dict(color='red', width=2)\n",
    "    ))\n",
    "\n",
    "    # XGBoost test confidence interval\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=test_df.index.tolist() + test_df.index.tolist()[::-1],\n",
    "        y=xgb_test_upper.tolist() + xgb_test_lower.tolist()[::-1],\n",
    "        fill='toself',\n",
    "        fillcolor='rgba(255,0,0,0.1)',\n",
    "        line=dict(color='rgba(255,0,0,0)'),\n",
    "        name=f'XGBoost {int(confidence_level*100)}% CI (Test)',\n",
    "        showlegend=True\n",
    "    ))\n",
    "\n",
    "    # XGBoost future predictions\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=xgb_future_times, y=xgb_future_mean,\n",
    "        mode='lines',\n",
    "        name='XGBoost (Future)',\n",
    "        line=dict(color='red', width=2, dash='dash')\n",
    "    ))\n",
    "\n",
    "    # XGBoost future confidence interval\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=xgb_future_times + xgb_future_times[::-1],\n",
    "        y=xgb_future_upper.tolist() + xgb_future_lower.tolist()[::-1],\n",
    "        fill='toself',\n",
    "        fillcolor='rgba(255,0,0,0.2)',\n",
    "        line=dict(color='rgba(255,0,0,0)'),\n",
    "        name=f'XGBoost {int(confidence_level*100)}% CI (Future)',\n",
    "        showlegend=True\n",
    "    ))\n",
    "\n",
    "    # Prophet predictions if available\n",
    "    prophet_mae, prophet_rmse, prophet_mse = None, None, None\n",
    "    if PROPHET_AVAILABLE:\n",
    "        try:\n",
    "            prophet_test, prophet_future, prophet_times, prophet_test_lower, prophet_test_upper, \\\n",
    "            prophet_future_lower, prophet_future_upper = train_prophet_with_ci(\n",
    "                train_df, test_df, forecast_days, confidence_level\n",
    "            )\n",
    "\n",
    "            prophet_mae = mean_absolute_error(test_df['trips'], prophet_test)\n",
    "            prophet_rmse = np.sqrt(mean_squared_error(test_df['trips'], prophet_test))\n",
    "            prophet_mse = mean_squared_error(test_df['trips'], prophet_test)\n",
    "\n",
    "            # Add Prophet predictions and confidence intervals\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=test_df.index, y=prophet_test,\n",
    "                mode='lines',\n",
    "                name='Prophet (Test)',\n",
    "                line=dict(color='blue', width=2)\n",
    "            ))\n",
    "\n",
    "            # Prophet confidence intervals\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=test_df.index.tolist() + test_df.index.tolist()[::-1],\n",
    "                y=prophet_test_upper.tolist() + prophet_test_lower.tolist()[::-1],\n",
    "                fill='toself',\n",
    "                fillcolor='rgba(0,0,255,0.1)',\n",
    "                line=dict(color='rgba(0,0,255,0)'),\n",
    "                name=f'Prophet {int(confidence_level*100)}% CI (Test)',\n",
    "                showlegend=True\n",
    "            ))\n",
    "\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=prophet_times, y=prophet_future,\n",
    "                mode='lines',\n",
    "                name='Prophet (Future)',\n",
    "                line=dict(color='blue', width=2, dash='dash')\n",
    "            ))\n",
    "\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=prophet_times.tolist() + prophet_times.tolist()[::-1],\n",
    "                y=prophet_future_upper.tolist() + prophet_future_lower.tolist()[::-1],\n",
    "                fill='toself',\n",
    "                fillcolor='rgba(0,0,255,0.2)',\n",
    "                line=dict(color='rgba(0,0,255,0)'),\n",
    "                name=f'Prophet {int(confidence_level*100)}% CI (Future)',\n",
    "                showlegend=True\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            print(f\"Prophet error: {e}\")\n",
    "\n",
    "    # Load actual future data if requested\n",
    "    if show_actual and ml_taxi_type_value:\n",
    "        future_start = last_date + timedelta(hours=1)\n",
    "        future_end = future_start + timedelta(days=forecast_days)\n",
    "        actual_future = load_actual_data_from_gcs(ml_taxi_type_value, future_start, future_end)\n",
    "\n",
    "        if not actual_future.empty:\n",
    "            # Apply metric selection to actual future data\n",
    "            if selected_metric == \"revenue\":\n",
    "                actual_future['trips'] = actual_future.get('revenue', actual_future['trips'])\n",
    "\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=actual_future.index, y=actual_future['trips'],\n",
    "                mode='markers',\n",
    "                name='Actual (Future)',\n",
    "                marker=dict(color='green', size=8, symbol='star')\n",
    "            ))\n",
    "\n",
    "    # Add metric annotations\n",
    "    fig.add_annotation(\n",
    "        x=cutoff_date, y=train_df['trips'].max(),\n",
    "        text=\"Test Start\", showarrow=True,\n",
    "        arrowhead=2, arrowsize=1, arrowwidth=2,\n",
    "        arrowcolor=\"gray\", ax=20, ay=-30\n",
    "    )\n",
    "\n",
    "    fig.add_annotation(\n",
    "        x=last_date, y=test_df['trips'].max(),\n",
    "        text=\"Forecast Start\", showarrow=True,\n",
    "        arrowhead=2, arrowsize=1, arrowwidth=2,\n",
    "        arrowcolor=\"purple\", ax=20, ay=-30\n",
    "    )\n",
    "\n",
    "    # Add metrics box\n",
    "    metrics_text = f\"<b>XGBoost Metrics:</b><br>MAE: {xgb_mae:.2f}<br>RMSE: {xgb_rmse:.2f}<br>MSE: {xgb_mse:.2f}\"\n",
    "    if prophet_mae is not None:\n",
    "        metrics_text += f\"<br><br><b>Prophet Metrics:</b><br>MAE: {prophet_mae:.2f}<br>RMSE: {prophet_rmse:.2f}<br>MSE: {prophet_mse:.2f}\"\n",
    "\n",
    "    fig.add_annotation(\n",
    "        text=metrics_text,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        x=0.02, y=0.98,\n",
    "        showarrow=False,\n",
    "        font=dict(size=12),\n",
    "        bgcolor=\"white\",\n",
    "        bordercolor=\"black\",\n",
    "        borderwidth=1,\n",
    "        xanchor=\"left\",\n",
    "        yanchor=\"top\"\n",
    "    )\n",
    "\n",
    "    # Add shaded region for future period\n",
    "    fig.add_vrect(\n",
    "        x0=last_date, x1=xgb_future_times[-1],\n",
    "        fillcolor=\"lightgray\", opacity=0.2,\n",
    "        layer=\"below\", line_width=0\n",
    "    )\n",
    "\n",
    "    # Update layout\n",
    "    title_text = f\"Combined Forecast Comparison for {metric_label}<br>\"\n",
    "    title_text += f\"<sub>{int(confidence_level*100)}% Confidence Intervals | {forecast_days} Days Ahead</sub>\"\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title_text,\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=f\"Number of {metric_label}\",\n",
    "        hovermode='x unified',\n",
    "        template=\"plotly_white\",\n",
    "        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.99),\n",
    "        height=650\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch0JFTuMnSoL",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1757012277246,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "ch0JFTuMnSoL"
   },
   "outputs": [],
   "source": [
    "def train_prophet_with_ci(train_df, test_df, forecast_days, confidence_level):\n",
    "    \"\"\"Train Prophet and return predictions with confidence intervals.\"\"\"\n",
    "    prophet_train = pd.DataFrame({\n",
    "        'ds': train_df.index,\n",
    "        'y': train_df['trips'].values\n",
    "    })\n",
    "\n",
    "    model = Prophet(\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        daily_seasonality=True,\n",
    "        changepoint_prior_scale=0.05,\n",
    "        seasonality_prior_scale=10,\n",
    "        interval_width=confidence_level\n",
    "    )\n",
    "    model.add_seasonality(name='hourly', period=24, fourier_order=8)\n",
    "    model.fit(prophet_train)\n",
    "\n",
    "    # Test predictions\n",
    "    test_dates = pd.DataFrame({'ds': test_df.index})\n",
    "    test_forecast = model.predict(test_dates)\n",
    "\n",
    "    # Future predictions\n",
    "    last_date = test_df.index.max()\n",
    "    future_dates = pd.date_range(\n",
    "        start=last_date + timedelta(hours=1),\n",
    "        periods=forecast_days * 24,\n",
    "        freq='h'\n",
    "    )\n",
    "    future_df = pd.DataFrame({'ds': future_dates})\n",
    "    future_forecast = model.predict(future_df)\n",
    "\n",
    "    return (test_forecast['yhat'].values, future_forecast['yhat'].values, future_dates,\n",
    "            test_forecast['yhat_lower'].values, test_forecast['yhat_upper'].values,\n",
    "            future_forecast['yhat_lower'].values, future_forecast['yhat_upper'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NGONcphzVqHR",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1757012277246,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "NGONcphzVqHR"
   },
   "outputs": [],
   "source": [
    "def detect_anomalies_from_db(taxi_type):\n",
    "    \"\"\"Load and visualize anomalies from pre-computed BigQuery tables.\"\"\"\n",
    "    if not taxi_type:\n",
    "        return px.line(title=\"Please select a taxi type\")\n",
    "\n",
    "    try:\n",
    "        # Query to get anomalies from the pipeline results\n",
    "        query = f\"\"\"\n",
    "        WITH anomaly_data AS (\n",
    "            SELECT\n",
    "                datetime,\n",
    "                trips,\n",
    "                is_anomaly,\n",
    "                anomaly_score,\n",
    "                reconstruction_error,\n",
    "                threshold\n",
    "            FROM `{PROJECT_ID}.{MlDATASET_NAME}.{taxi_type}_anomalies`\n",
    "            ORDER BY datetime\n",
    "        )\n",
    "        SELECT * FROM anomaly_data\n",
    "        \"\"\"\n",
    "\n",
    "        df = client.query(query).to_dataframe()\n",
    "\n",
    "        if df.empty:\n",
    "            return px.line(title=f\"No anomaly data found for {taxi_type} taxi. Run the anomaly detection pipeline first.\")\n",
    "\n",
    "        # Create plot\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # First, get the date range and partitions from anomaly data\n",
    "        date_range_query = f\"\"\"\n",
    "        SELECT\n",
    "            MIN(datetime) as min_date,\n",
    "            MAX(datetime) as max_date\n",
    "        FROM `{PROJECT_ID}.{MlDATASET_NAME}.{taxi_type}_anomalies`\n",
    "        \"\"\"\n",
    "\n",
    "        date_range_result = list(client.query(date_range_query).result())[0]\n",
    "        min_date = date_range_result.min_date\n",
    "        max_date = date_range_result.max_date\n",
    "\n",
    "        # Get all available partitions for this taxi type\n",
    "        all_partitions = get_available_partitions(taxi_type)\n",
    "\n",
    "        # Filter partitions that could contain data in our date range\n",
    "        min_year_month = f\"{min_date.year}_{min_date.strftime('%m')}\"\n",
    "        max_year_month = f\"{max_date.year}_{max_date.strftime('%m')}\"\n",
    "\n",
    "        relevant_partitions = [p for p in all_partitions\n",
    "                              if min_year_month <= p <= max_year_month]\n",
    "\n",
    "        # Build UNION query for relevant partitions\n",
    "        union_queries = []\n",
    "        for partition in relevant_partitions:\n",
    "            union_queries.append(f\"\"\"\n",
    "                SELECT\n",
    "                    DATETIME(pickup_date, TIME(pickup_hour, 0, 0)) as datetime,\n",
    "                    trips\n",
    "                FROM `{PROJECT_ID}.{DATASET_NAME}.{taxi_type}_{partition}_hourly`\n",
    "                WHERE DATETIME(pickup_date, TIME(pickup_hour, 0, 0)) >= DATETIME(TIMESTAMP('{min_date}'))\n",
    "                AND DATETIME(pickup_date, TIME(pickup_hour, 0, 0)) <= DATETIME(TIMESTAMP('{max_date}'))\n",
    "            \"\"\")\n",
    "\n",
    "        if union_queries:\n",
    "            hourly_query = \" UNION ALL \".join(union_queries) + \" ORDER BY datetime\"\n",
    "            hourly_df = client.query(hourly_query).to_dataframe()\n",
    "\n",
    "            # Plot all hourly data\n",
    "            if not hourly_df.empty:\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=hourly_df['datetime'],\n",
    "                    y=hourly_df['trips'],\n",
    "                    mode='lines',\n",
    "                    name='Hourly Traffic',\n",
    "                    line=dict(color='lightblue', width=1),\n",
    "                    opacity=0.7,\n",
    "                    hovertemplate='<b>%{x}</b><br>Trips: %{y}<extra></extra>'\n",
    "                ))\n",
    "\n",
    "        # Anomalies with severity coloring\n",
    "        anomalies = df[df['is_anomaly']]\n",
    "\n",
    "        # Categorize anomalies by severity\n",
    "        mild_anomalies = anomalies[anomalies['anomaly_score'] <= 1.5]\n",
    "        moderate_anomalies = anomalies[(anomalies['anomaly_score'] > 1.5) & (anomalies['anomaly_score'] <= 3.0)]\n",
    "        severe_anomalies = anomalies[anomalies['anomaly_score'] > 3.0]\n",
    "\n",
    "        # Add mild anomalies\n",
    "        if not mild_anomalies.empty:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=mild_anomalies['datetime'],\n",
    "                y=mild_anomalies['trips'],\n",
    "                mode='markers',\n",
    "                name='Mild Anomalies (Score ≤ 1.5)',\n",
    "                marker=dict(color='yellow', size=6, symbol='circle'),\n",
    "                hovertemplate='<b>%{x}</b><br>Trips: %{y}<br>Score: %{customdata:.2f}<extra></extra>',\n",
    "                customdata=mild_anomalies['anomaly_score']\n",
    "            ))\n",
    "\n",
    "        # Add moderate anomalies\n",
    "        if not moderate_anomalies.empty:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=moderate_anomalies['datetime'],\n",
    "                y=moderate_anomalies['trips'],\n",
    "                mode='markers',\n",
    "                name='Moderate Anomalies (1.5 < Score ≤ 3.0)',\n",
    "                marker=dict(color='orange', size=8, symbol='diamond'),\n",
    "                hovertemplate='<b>%{x}</b><br>Trips: %{y}<br>Score: %{customdata:.2f}<extra></extra>',\n",
    "                customdata=moderate_anomalies['anomaly_score']\n",
    "            ))\n",
    "\n",
    "        # Add severe anomalies\n",
    "        if not severe_anomalies.empty:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=severe_anomalies['datetime'],\n",
    "                y=severe_anomalies['trips'],\n",
    "                mode='markers',\n",
    "                name='Severe Anomalies (Score > 3.0)',\n",
    "                marker=dict(color='red', size=10, symbol='x'),\n",
    "                hovertemplate='<b>%{x}</b><br>Trips: %{y}<br>Score: %{customdata:.2f}<extra></extra>',\n",
    "                customdata=severe_anomalies['anomaly_score']\n",
    "            ))\n",
    "\n",
    "        # Add holiday annotations\n",
    "        date_range = pd.to_datetime(df['datetime'])\n",
    "        min_date = date_range.min()\n",
    "        max_date = date_range.max()\n",
    "\n",
    "        for holiday_date, holiday_name in US_HOLIDAYS:\n",
    "            holiday_dt = pd.to_datetime(holiday_date).tz_localize(None)\n",
    "            if min_date.tz_localize(None) <= holiday_dt <= max_date.tz_localize(None):\n",
    "                # Check if this holiday coincides with an anomaly\n",
    "                holiday_anomalies = df[(pd.to_datetime(df['datetime']).dt.date == holiday_dt.date()) & df['is_anomaly']]\n",
    "\n",
    "                # Find y-value for annotation\n",
    "                holiday_data = df[pd.to_datetime(df['datetime']).dt.date == holiday_dt.date()]\n",
    "                if not holiday_data.empty:\n",
    "                    y_val = holiday_data['trips'].max()\n",
    "\n",
    "                    # Different styling if holiday has anomalies\n",
    "                    if not holiday_anomalies.empty:\n",
    "                        max_score = holiday_anomalies['anomaly_score'].max()\n",
    "                        annotation_text = f\"{holiday_name}<br>Max Score: {max_score:.2f}\"\n",
    "                        border_color = \"red\"\n",
    "                    else:\n",
    "                        annotation_text = holiday_name\n",
    "                        border_color = \"green\"\n",
    "\n",
    "                    fig.add_annotation(\n",
    "                        x=holiday_dt,\n",
    "                        y=y_val,\n",
    "                        text=annotation_text,\n",
    "                        showarrow=True,\n",
    "                        arrowhead=2,\n",
    "                        arrowsize=1,\n",
    "                        arrowwidth=2,\n",
    "                        arrowcolor=border_color,\n",
    "                        ax=0,\n",
    "                        ay=-40,\n",
    "                        bgcolor=\"white\",\n",
    "                        bordercolor=border_color,\n",
    "                        borderwidth=2,\n",
    "                        font=dict(size=10)\n",
    "                    )\n",
    "\n",
    "        # Update layout\n",
    "        total_anomalies = df['is_anomaly'].sum()\n",
    "        avg_score = df[df['is_anomaly']]['anomaly_score'].mean()\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"Anomaly Detection Results for {taxi_type.title()} Taxi<br>\" +\n",
    "                  f\"<sub>Total Anomalies: {total_anomalies} | Average Anomaly Score: {avg_score:.2f}</sub>\",\n",
    "            xaxis_title=\"Date\",\n",
    "            yaxis_title=\"Number of Trips\",\n",
    "            hovermode='x unified',\n",
    "            template=\"plotly_white\",\n",
    "            height=600,\n",
    "            legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01)\n",
    "        )\n",
    "\n",
    "        return fig\n",
    "\n",
    "    except Exception as e:\n",
    "        # print entre error\n",
    "        print(\n",
    "            e\n",
    "        )\n",
    "        return px.line(title=f\"Error loading anomaly data: {str(e)}\")\n",
    "\n",
    "def get_anomaly_summary(taxi_type):\n",
    "    \"\"\"Get summary statistics for anomalies.\"\"\"\n",
    "\n",
    "    if not taxi_type:\n",
    "        return \"Please load data first\"\n",
    "\n",
    "    try:\n",
    "        # Query summary statistics\n",
    "        query = f\"\"\"\n",
    "        WITH summary AS (\n",
    "            SELECT\n",
    "                COUNT(*) as total_hours,\n",
    "                SUM(CAST(is_anomaly AS INT64)) as total_anomalies,\n",
    "                AVG(CASE WHEN is_anomaly THEN anomaly_score END) as avg_anomaly_score,\n",
    "                MAX(CASE WHEN is_anomaly THEN anomaly_score END) as max_anomaly_score,\n",
    "                COUNT(DISTINCT DATE(datetime)) as days_analyzed,\n",
    "                COUNT(DISTINCT FORMAT_DATETIME('%Y-%m', datetime)) as months_analyzed\n",
    "            FROM `{PROJECT_ID}.{MlDATASET_NAME}.{taxi_type}_anomalies`\n",
    "        ),\n",
    "        severity_breakdown AS (\n",
    "            SELECT\n",
    "                SUM(CASE WHEN anomaly_score > 3.0 THEN 1 ELSE 0 END) as severe_count,\n",
    "                SUM(CASE WHEN anomaly_score > 1.5 AND anomaly_score <= 3.0 THEN 1 ELSE 0 END) as moderate_count,\n",
    "                SUM(CASE WHEN anomaly_score > 1.0 AND anomaly_score <= 1.5 THEN 1 ELSE 0 END) as mild_count\n",
    "            FROM `{PROJECT_ID}.{MlDATASET_NAME}.{taxi_type}_anomalies`\n",
    "            WHERE is_anomaly = TRUE\n",
    "        )\n",
    "        SELECT * FROM summary CROSS JOIN severity_breakdown\n",
    "        \"\"\"\n",
    "\n",
    "        result = list(client.query(query).result())[0]\n",
    "\n",
    "        summary_text = f\"\"\"\n",
    "**Anomaly Detection Summary for {taxi_type.title()} Taxi**\n",
    "\n",
    "**Coverage:**\n",
    "- Months Analyzed: {result.months_analyzed}\n",
    "- Days Analyzed: {result.days_analyzed}\n",
    "- Total Hours: {result.total_hours:,}\n",
    "\n",
    "**Anomaly Statistics:**\n",
    "- Total Anomalies: {result.total_anomalies:,} ({result.total_anomalies/result.total_hours*100:.2f}%)\n",
    "- Average Anomaly Score: {result.avg_anomaly_score:.2f}\n",
    "- Maximum Anomaly Score: {result.max_anomaly_score:.2f}\n",
    "\n",
    "**Severity Breakdown:**\n",
    "- Severe (Score > 3.0): {result.severe_count}\n",
    "- Moderate (1.5 < Score ≤ 3.0): {result.moderate_count}\n",
    "- Mild (1.0 < Score ≤ 1.5): {result.mild_count}\n",
    "        \"\"\"\n",
    "\n",
    "        return summary_text\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error loading summary: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vpwytre-yYc5",
   "metadata": {
    "id": "vpwytre-yYc5"
   },
   "source": [
    "# ==================== Data Loading Functions ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Z_Byt6Q-yK6X",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1757012277246,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "Z_Byt6Q-yK6X"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def update_partition_choices(taxi_type):\n",
    "    \"\"\"Update partition choices when taxi type changes.\"\"\"\n",
    "    if not taxi_type:\n",
    "        return gr.update(choices=[]), gr.update(choices=[])\n",
    "\n",
    "    try:\n",
    "        partitions = get_available_partitions(taxi_type)\n",
    "        return gr.update(choices=partitions, value=partitions[0] if partitions else None), \\\n",
    "               gr.update(choices=partitions, value=partitions[-1] if partitions else None)\n",
    "    except:\n",
    "        return gr.update(choices=[], value=None), gr.update(choices=[], value=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ht0qouLFyapt",
   "metadata": {
    "id": "Ht0qouLFyapt"
   },
   "source": [
    "# Map Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Da45Gj5YApDo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1719,
     "status": "ok",
     "timestamp": 1757012279401,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "Da45Gj5YApDo",
    "outputId": "d221fa1d-9b58-44d2-bfaf-ca4af350101c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from google.cloud import storage\n",
    "\n",
    "# Create a storage client\n",
    "storage_client = storage.Client()\n",
    "\n",
    "# Specify the bucket and blob name\n",
    "bucket_name = 'nyc_raw_data_bucket'\n",
    "blob_name = 'taxi_zone_lookup.csv'\n",
    "\n",
    "# Get the bucket and blob\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "blob = bucket.blob(blob_name)\n",
    "\n",
    "# Download the blob's content as bytes\n",
    "content = blob.download_as_bytes()\n",
    "\n",
    "# Read the CSV data into a Pandas DataFrame\n",
    "taxi_zone_lookup = pd.read_csv(BytesIO(content))\n",
    "\n",
    "# Display the DataFrame (optional)\n",
    "print(taxi_zone_lookup.head())\n",
    "\n",
    "\n",
    "import geopandas as gpd\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# Create a storage client\n",
    "storage_client = storage.Client()\n",
    "\n",
    "# Specify the bucket and the shapefile prefix\n",
    "bucket_name = 'nyc_raw_data_bucket'\n",
    "shapefile_prefix = 'taxi_zones/taxi_zones'\n",
    "\n",
    "# Define all the required file extensions\n",
    "# .dbf and .prj are also essential for attributes and CRS\n",
    "required_extensions = ['.dbf','.prj','.sbn','.sbx','.shp', '.shp.xml','.shx']\n",
    "\n",
    "# Create a temporary directory to store the downloaded files\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "try:\n",
    "    print(f\"Downloading shapefile components to temporary directory: {temp_dir}\")\n",
    "    # Download each part of the shapefile\n",
    "    for extension in required_extensions:\n",
    "        blob_name = shapefile_prefix + extension\n",
    "        blob = storage_client.bucket(bucket_name).blob(blob_name)\n",
    "\n",
    "        # Construct the local file path\n",
    "        local_path = os.path.join(temp_dir, os.path.basename(blob_name))\n",
    "\n",
    "        try:\n",
    "            blob.download_to_filename(local_path)\n",
    "            print(f\"Downloaded {blob_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {blob_name}: {e}\")\n",
    "            # The .prj file is optional for reading, so we can continue if it's missing\n",
    "            if extension == '.prj':\n",
    "                continue\n",
    "            else:\n",
    "                raise Exception(f\"Failed to download a required file: {blob_name}\")\n",
    "\n",
    "    # Read the shapefile from the temporary directory\n",
    "    shp_path = os.path.join(temp_dir, 'taxi_zones.shp')\n",
    "\n",
    "    print(\"\\nReading shapefile with GeoPandas...\")\n",
    "    taxi_zones_gdf = gpd.read_file(shp_path)\n",
    "\n",
    "    print(\"\\nSuccessfully read the Shapefile. Here is the GeoDataFrame head:\")\n",
    "    print(taxi_zones_gdf.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during shapefile processing: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Clean up the temporary directory and its contents\n",
    "    if os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir)\n",
    "        print(f\"\\nCleaned up temporary directory: {temp_dir}\")\n",
    "\n",
    "import geopandas as gpd\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# Create a storage client\n",
    "storage_client = storage.Client()\n",
    "bucket_name = 'nyc_raw_data_bucket'\n",
    "shapefile_prefix = 'taxi_zones/taxi_zones'\n",
    "required_extensions = ['.shp', '.shx', '.dbf', '.prj']\n",
    "\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "try:\n",
    "    for extension in required_extensions:\n",
    "        blob_name = shapefile_prefix + extension\n",
    "        blob = storage_client.bucket(bucket_name).blob(blob_name)\n",
    "        local_path = os.path.join(temp_dir, os.path.basename(blob_name))\n",
    "        blob.download_to_filename(local_path)\n",
    "\n",
    "    shp_path = os.path.join(temp_dir, 'taxi_zones.shp')\n",
    "\n",
    "    # Read the shapefile\n",
    "    taxi_zones_gdf = gpd.read_file(shp_path)\n",
    "\n",
    "    print(\"Original CRS:\")\n",
    "    print(taxi_zones_gdf.crs)\n",
    "\n",
    "    # Reproject the GeoDataFrame to WGS 84 (EPSG:4326)\n",
    "    # This is the standard CRS for longitude and latitude\n",
    "    print(\"\\nReprojecting to WGS 84 (EPSG:4326)...\")\n",
    "    taxi_zones_gdf_wgs84 = taxi_zones_gdf.to_crs(epsg=4326)\n",
    "\n",
    "    print(\"\\nNew CRS:\")\n",
    "    print(taxi_zones_gdf_wgs84.crs)\n",
    "\n",
    "    # You can now see the coordinates in a longitude/latitude format\n",
    "    print(\"\\nHead of GeoDataFrame with reprojected coordinates:\")\n",
    "    print(taxi_zones_gdf_wgs84.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during shapefile processing: {e}\")\n",
    "\n",
    "finally:\n",
    "    if os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir)\n",
    "\n",
    "taxi_zone_lookup['LocationID'] = taxi_zone_lookup['LocationID'].astype(int)\n",
    "\n",
    "# Merge the two dataframes\n",
    "merged_df = pd.merge(taxi_zone_lookup, taxi_zones_gdf_wgs84, left_on='LocationID', right_on='OBJECTID', how='inner')\n",
    "\n",
    "# Remove rows with missing geometries before plotting\n",
    "merged_df_cleaned = merged_df.dropna(subset=['geometry'])\n",
    "\n",
    "# FIX: Explicitly convert the DataFrame to a GeoDataFrame\n",
    "# and set the CRS to WGS 84 (EPSG:4326) which is the standard for Lon/Lat data.\n",
    "merged_gdf_cleaned = gpd.GeoDataFrame(merged_df_cleaned, geometry='geometry', crs='EPSG:4326')\n",
    "\n",
    "merged_gdf_cleaned[\"LocationID\"] = merged_gdf_cleaned[\"OBJECTID\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wIR00XqzDy--",
   "metadata": {
    "id": "wIR00XqzDy--"
   },
   "source": [
    "# Fleet Insights Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372K5QOl9M01",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1757012279402,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "372K5QOl9M01"
   },
   "outputs": [],
   "source": [
    "def get_fleet_date_range_new(taxi_type):\n",
    "    \"\"\"Get date range from new prediction tables\"\"\"\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        MIN(date) as min_date,\n",
    "        MAX(date) as max_date\n",
    "    FROM `{PROJECT_ID}.{OUTPUT_DATASET}.fleet_recommender_{taxi_type}_predictions_new_xgb`\n",
    "    \"\"\"\n",
    "    df = bq_client.query(query).to_dataframe()\n",
    "\n",
    "    if not df.empty:\n",
    "        min_date = df.iloc[0]['min_date'].strftime('%Y-%m-%d')\n",
    "        max_date = df.iloc[0]['max_date'].strftime('%Y-%m-%d')\n",
    "        range_text = f\"Available: {min_date} to {max_date}\"\n",
    "        return min_date, max_date, range_text\n",
    "    return None, None, \"No data available\"\n",
    "\n",
    "def get_zones_with_predictions(taxi_type):\n",
    "    \"\"\"Get list of zones ordered by trip volume\"\"\"\n",
    "    query = f\"\"\"\n",
    "    SELECT DISTINCT zone_id,\n",
    "           SUM(xgb_pred_y_single + xgb_pred_y_small + xgb_pred_y_medium + xgb_pred_y_large) as total_trips\n",
    "    FROM `{PROJECT_ID}.{OUTPUT_DATASET}.fleet_recommender_{taxi_type}_predictions_new_xgb`\n",
    "    WHERE prediction_type = 'future'\n",
    "    GROUP BY zone_id\n",
    "    ORDER BY total_trips DESC\n",
    "    \"\"\"\n",
    "    df = bq_client.query(query).to_dataframe()\n",
    "\n",
    "    zone_list = [\"All Zones\"]\n",
    "    for _, row in df.iterrows():\n",
    "        zone_list.append(f\"Zone {row['zone_id']} ({row['total_trips']:.0f} trips)\")\n",
    "    return zone_list\n",
    "\n",
    "def load_zone_metrics_summary(taxi_type):\n",
    "    \"\"\"Load and format zone-level metrics\"\"\"\n",
    "    query = f\"\"\"\n",
    "    SELECT zone_id, model_type, target,\n",
    "           ROUND(AVG(smape), 1) as avg_smape,\n",
    "           ROUND(AVG(mae), 2) as avg_mae,\n",
    "           ROUND(AVG(rmse), 2) as avg_rmse\n",
    "    FROM `{PROJECT_ID}.{OUTPUT_DATASET}.fleet_recommender_{taxi_type}_metrics_new`\n",
    "    GROUP BY zone_id, model_type, target\n",
    "    ORDER BY zone_id, model_type, target\n",
    "    \"\"\"\n",
    "    df = bq_client.query(query).to_dataframe()\n",
    "\n",
    "    if df.empty:\n",
    "        return \"No metrics available\"\n",
    "\n",
    "    # Format metrics by zone\n",
    "    summary = []\n",
    "    for zone_id in df['zone_id'].unique():\n",
    "        zone_df = df[df['zone_id'] == zone_id]\n",
    "        summary.append(f\"\\n=== Zone {zone_id} ===\")\n",
    "\n",
    "        for model in ['xgb', 'knn']:\n",
    "            model_df = zone_df[zone_df['model_type'] == model]\n",
    "            if not model_df.empty:\n",
    "                summary.append(f\"\\n{model.upper()} Model:\")\n",
    "                for _, row in model_df.iterrows():\n",
    "                    summary.append(f\"  {row['target']}: SMAPE={row['avg_smape']}%, MAE={row['avg_mae']}\")\n",
    "\n",
    "    return \"\\n\".join(summary)\n",
    "\n",
    "\n",
    "def extract_zone_id(zone_string):\n",
    "    \"\"\"Extract zone ID from dropdown string\"\"\"\n",
    "    if zone_string == \"All Zones\":\n",
    "        return None\n",
    "    return int(zone_string.split()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qsK-DWWt7n74",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1757012279402,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "qsK-DWWt7n74"
   },
   "outputs": [],
   "source": [
    "def plot_fleet_timeseries_new(taxi_type, model_type, start_date, end_date, zone_id, display_type, pred_type):\n",
    "    \"\"\"Create time series plot for new zone-based predictions\"\"\"\n",
    "\n",
    "    # Build the WHERE clause based on filters\n",
    "    where_conditions = [f\"date >= '{start_date}'\", f\"date <= '{end_date}'\"]\n",
    "\n",
    "    if zone_id is not None:\n",
    "        where_conditions.append(f\"zone_id = {zone_id}\")\n",
    "\n",
    "    # Filter by prediction type\n",
    "    if pred_type == \"Test Only\":\n",
    "        where_conditions.append(\"prediction_type = 'test'\")\n",
    "    elif pred_type == \"Future Only\":\n",
    "        where_conditions.append(\"prediction_type = 'future'\")\n",
    "\n",
    "    where_clause = \" AND \".join(where_conditions)\n",
    "\n",
    "    # Query based on model type\n",
    "    if model_type == \"both\":\n",
    "        # Get data from both models\n",
    "        xgb_query = f\"\"\"\n",
    "        SELECT\n",
    "            date,\n",
    "            hr,\n",
    "            {'zone_id,' if zone_id is not None else ''}\n",
    "            prediction_type,\n",
    "            {'SUM' if zone_id is None else ''}(xgb_pred_y_single) as pred_y_single,\n",
    "            {'SUM' if zone_id is None else ''}(xgb_pred_y_small) as pred_y_small,\n",
    "            {'SUM' if zone_id is None else ''}(xgb_pred_y_medium) as pred_y_medium,\n",
    "            {'SUM' if zone_id is None else ''}(xgb_pred_y_large) as pred_y_large,\n",
    "            {'SUM' if zone_id is None else ''}(xgb_pred_y_single_lower) as pred_y_single_lower,\n",
    "            {'SUM' if zone_id is None else ''}(xgb_pred_y_single_upper) as pred_y_single_upper,\n",
    "            {'SUM' if zone_id is None else ''}(actual_y_single) as actual_y_single,\n",
    "            {'SUM' if zone_id is None else ''}(actual_y_small) as actual_y_small,\n",
    "            {'SUM' if zone_id is None else ''}(actual_y_medium) as actual_y_medium,\n",
    "            {'SUM' if zone_id is None else ''}(actual_y_large) as actual_y_large,\n",
    "            'XGBoost' as model\n",
    "        FROM `{PROJECT_ID}.{OUTPUT_DATASET}.fleet_recommender_{taxi_type}_predictions_new_xgb`\n",
    "        WHERE {where_clause}\n",
    "        {'GROUP BY date, hr, prediction_type' if zone_id is None else ''}\n",
    "        ORDER BY date, hr\n",
    "        \"\"\"\n",
    "\n",
    "        knn_query = f\"\"\"\n",
    "        SELECT\n",
    "            date,\n",
    "            hr,\n",
    "            {'zone_id,' if zone_id is not None else ''}\n",
    "            prediction_type,\n",
    "            {'SUM' if zone_id is None else ''}(knn_pred_y_single) as pred_y_single,\n",
    "            {'SUM' if zone_id is None else ''}(knn_pred_y_small) as pred_y_small,\n",
    "            {'SUM' if zone_id is None else ''}(knn_pred_y_medium) as pred_y_medium,\n",
    "            {'SUM' if zone_id is None else ''}(knn_pred_y_large) as pred_y_large,\n",
    "            {'SUM' if zone_id is None else ''}(knn_pred_y_single_lower) as pred_y_single_lower,\n",
    "            {'SUM' if zone_id is None else ''}(knn_pred_y_single_upper) as pred_y_single_upper,\n",
    "            {'SUM' if zone_id is None else ''}(actual_y_single) as actual_y_single,\n",
    "            {'SUM' if zone_id is None else ''}(actual_y_small) as actual_y_small,\n",
    "            {'SUM' if zone_id is None else ''}(actual_y_medium) as actual_y_medium,\n",
    "            {'SUM' if zone_id is None else ''}(actual_y_large) as actual_y_large,\n",
    "            'KNN' as model\n",
    "        FROM `{PROJECT_ID}.{OUTPUT_DATASET}.fleet_recommender_{taxi_type}_predictions_new_knn`\n",
    "        WHERE {where_clause}\n",
    "        {'GROUP BY date, hr, prediction_type' if zone_id is None else ''}\n",
    "        ORDER BY date, hr\n",
    "        \"\"\"\n",
    "\n",
    "        xgb_df = bq_client.query(xgb_query).to_dataframe()\n",
    "        knn_df = bq_client.query(knn_query).to_dataframe()\n",
    "        df = pd.concat([xgb_df, knn_df], ignore_index=True)\n",
    "\n",
    "    else:\n",
    "        # Single model\n",
    "        table_name = f\"fleet_recommender_{taxi_type}_predictions_new_{model_type}\"\n",
    "        prefix = model_type\n",
    "\n",
    "        query = f\"\"\"\n",
    "        SELECT\n",
    "            date,\n",
    "            hr,\n",
    "            {'zone_id,' if zone_id is not None else ''}\n",
    "            prediction_type,\n",
    "            {'SUM' if zone_id is None else ''}({prefix}_pred_y_single) as pred_y_single,\n",
    "            {'SUM' if zone_id is None else ''}({prefix}_pred_y_small) as pred_y_small,\n",
    "            {'SUM' if zone_id is None else ''}({prefix}_pred_y_medium) as pred_y_medium,\n",
    "            {'SUM' if zone_id is None else ''}({prefix}_pred_y_large) as pred_y_large,\n",
    "            {'SUM' if zone_id is None else ''}({prefix}_pred_y_single_lower) as pred_y_single_lower,\n",
    "            {'SUM' if zone_id is None else ''}({prefix}_pred_y_single_upper) as pred_y_single_upper,\n",
    "            {'SUM' if zone_id is None else ''}({prefix}_pred_y_small_lower) as pred_y_small_lower,\n",
    "            {'SUM' if zone_id is None else ''}({prefix}_pred_y_small_upper) as pred_y_small_upper,\n",
    "            {'SUM' if zone_id is None else ''}({prefix}_pred_y_medium_lower) as pred_y_medium_lower,\n",
    "            {'SUM' if zone_id is None else ''}({prefix}_pred_y_medium_upper) as pred_y_medium_upper,\n",
    "            {'SUM' if zone_id is None else ''}({prefix}_pred_y_large_lower) as pred_y_large_lower,\n",
    "            {'SUM' if zone_id is None else ''}({prefix}_pred_y_large_upper) as pred_y_large_upper,\n",
    "            {'SUM' if zone_id is None else ''}(actual_y_single) as actual_y_single,\n",
    "            {'SUM' if zone_id is None else ''}(actual_y_small) as actual_y_small,\n",
    "            {'SUM' if zone_id is None else ''}(actual_y_medium) as actual_y_medium,\n",
    "            {'SUM' if zone_id is None else ''}(actual_y_large) as actual_y_large,\n",
    "            '{model_type.upper()}' as model\n",
    "        FROM `{PROJECT_ID}.{OUTPUT_DATASET}.{table_name}`\n",
    "        WHERE {where_clause}\n",
    "        {'GROUP BY date, hr, prediction_type' if zone_id is None else ''}\n",
    "        ORDER BY date, hr\n",
    "        \"\"\"\n",
    "        df = bq_client.query(query).to_dataframe()\n",
    "\n",
    "    if df.empty:\n",
    "        return create_empty_plot(\"No data available for selected filters\")\n",
    "\n",
    "    # Create datetime column\n",
    "    df['datetime'] = pd.to_datetime(df['date']) + pd.to_timedelta(df['hr'], unit='h')\n",
    "    df = df.sort_values('datetime')\n",
    "\n",
    "    # Calculate total trips\n",
    "    df['pred_total'] = df['pred_y_single'] + df['pred_y_small'] + df['pred_y_medium'] + df['pred_y_large']\n",
    "\n",
    "    # Create the plot based on display type\n",
    "    if display_type == \"All Passengers\":\n",
    "        fig = create_all_passengers_plot(df, zone_id, model_type)\n",
    "    elif display_type == \"By Passenger Type\":\n",
    "        fig = create_by_passenger_type_plot(df, zone_id, model_type)\n",
    "    else:  # Model Comparison\n",
    "        fig = create_model_comparison_plot(df, zone_id)\n",
    "\n",
    "    return fig\n",
    "\n",
    "def create_all_passengers_plot(df, zone_id, model_type):\n",
    "    \"\"\"Create plot showing total passenger predictions\"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Colors for different elements\n",
    "    colors = {\n",
    "        'XGBoost': '#1f77b4',\n",
    "        'KNN': '#ff7f0e',\n",
    "        'actual': '#2ca02c',\n",
    "        'test': 'rgba(255, 0, 0, 0.1)',\n",
    "        'future': 'rgba(0, 0, 255, 0.1)'\n",
    "    }\n",
    "\n",
    "    if model_type == \"both\":\n",
    "        # Plot both models\n",
    "        for model in ['XGBoost', 'KNN']:\n",
    "            model_df = df[df['model'] == model]\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=model_df['datetime'],\n",
    "                y=model_df['pred_total'],\n",
    "                name=f'{model} Prediction',\n",
    "                line=dict(color=colors[model], width=2),\n",
    "                mode='lines'\n",
    "            ))\n",
    "\n",
    "            # Add confidence intervals if available\n",
    "            if 'pred_y_single_upper' in model_df.columns:\n",
    "                # Calculate total CI (simplified - sum of individual CIs)\n",
    "                upper_total = model_df['pred_y_single_upper'].fillna(0) + \\\n",
    "                             model_df['pred_y_small_upper'].fillna(0) + \\\n",
    "                             model_df['pred_y_medium_upper'].fillna(0) + \\\n",
    "                             model_df['pred_y_large_upper'].fillna(0)\n",
    "                lower_total = model_df['pred_y_single_lower'].fillna(0) + \\\n",
    "                             model_df['pred_y_small_lower'].fillna(0) + \\\n",
    "                             model_df['pred_y_medium_lower'].fillna(0) + \\\n",
    "                             model_df['pred_y_large_lower'].fillna(0)\n",
    "\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=model_df['datetime'],\n",
    "                    y=upper_total,\n",
    "                    fill=None,\n",
    "                    mode='lines',\n",
    "                    line_color='rgba(0,0,0,0)',\n",
    "                    showlegend=False,\n",
    "                    hoverinfo='skip'\n",
    "                ))\n",
    "\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=model_df['datetime'],\n",
    "                    y=lower_total,\n",
    "                    fill='tonexty',\n",
    "                    mode='lines',\n",
    "                    line_color='rgba(0,0,0,0)',\n",
    "                    name=f'{model} 95% CI',\n",
    "                    fillcolor=f'rgba{tuple(list(int(colors[model].lstrip(\"#\")[i:i+2], 16) for i in (0, 2, 4)) + [0.2])}'\n",
    "                ))\n",
    "    else:\n",
    "        # Single model\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df['datetime'],\n",
    "            y=df['pred_total'],\n",
    "            name=f'{model_type.upper()} Prediction',\n",
    "            line=dict(color=colors.get(model_type.upper(), '#1f77b4'), width=2),\n",
    "            mode='lines'\n",
    "        ))\n",
    "\n",
    "        # Add confidence intervals if single model\n",
    "        if 'pred_y_single_upper' in df.columns:\n",
    "            upper_total = df['pred_y_single_upper'].fillna(0) + \\\n",
    "                         df['pred_y_small_upper'].fillna(0) + \\\n",
    "                         df['pred_y_medium_upper'].fillna(0) + \\\n",
    "                         df['pred_y_large_upper'].fillna(0)\n",
    "            lower_total = df['pred_y_single_lower'].fillna(0) + \\\n",
    "                         df['pred_y_small_lower'].fillna(0) + \\\n",
    "                         df['pred_y_medium_lower'].fillna(0) + \\\n",
    "                         df['pred_y_large_lower'].fillna(0)\n",
    "\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=df['datetime'],\n",
    "                y=upper_total,\n",
    "                fill=None,\n",
    "                mode='lines',\n",
    "                line_color='rgba(0,0,0,0)',\n",
    "                showlegend=False,\n",
    "                hoverinfo='skip'\n",
    "            ))\n",
    "\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=df['datetime'],\n",
    "                y=lower_total,\n",
    "                fill='tonexty',\n",
    "                mode='lines',\n",
    "                line_color='rgba(0,0,0,0)',\n",
    "                name='95% CI',\n",
    "                fillcolor='rgba(31, 119, 180, 0.2)'\n",
    "            ))\n",
    "\n",
    "    # Add actual values if present\n",
    "    test_df = df[df['prediction_type'] == 'test'].copy()\n",
    "    if not test_df.empty and 'actual_y_single' in test_df.columns:\n",
    "        test_df['actual_total'] = test_df['actual_y_single'].fillna(0) + \\\n",
    "                                  test_df['actual_y_small'].fillna(0) + \\\n",
    "                                  test_df['actual_y_medium'].fillna(0) + \\\n",
    "                                  test_df['actual_y_large'].fillna(0)\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=test_df['datetime'],\n",
    "            y=test_df['actual_total'],\n",
    "            name='Actual',\n",
    "            line=dict(color=colors['actual'], width=2, dash='dot'),\n",
    "            mode='lines'\n",
    "        ))\n",
    "\n",
    "    # Add shaded regions for test/future periods\n",
    "    test_periods = df[df['prediction_type'] == 'test']['datetime']\n",
    "    future_periods = df[df['prediction_type'] == 'future']['datetime']\n",
    "\n",
    "    if not test_periods.empty:\n",
    "        fig.add_vrect(\n",
    "            x0=test_periods.min(), x1=test_periods.max(),\n",
    "            fillcolor=colors['test'], layer=\"below\", line_width=0,\n",
    "            annotation_text=\"Test Period\", annotation_position=\"top left\"\n",
    "        )\n",
    "\n",
    "    if not future_periods.empty:\n",
    "        fig.add_vrect(\n",
    "            x0=future_periods.min(), x1=future_periods.max(),\n",
    "            fillcolor=colors['future'], layer=\"below\", line_width=0,\n",
    "            annotation_text=\"Future Predictions\", annotation_position=\"top left\"\n",
    "        )\n",
    "\n",
    "    # Update layout\n",
    "    zone_text = f\"Zone {zone_id}\" if zone_id else \"All Zones\"\n",
    "    fig.update_layout(\n",
    "        title=f\"Passenger Predictions - {zone_text}\",\n",
    "        xaxis_title=\"Date/Time\",\n",
    "        yaxis_title=\"Total Passengers\",\n",
    "        hovermode='x unified',\n",
    "        height=600\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "def create_by_passenger_type_plot(df, zone_id, model_type):\n",
    "    \"\"\"Create subplots for each passenger type\"\"\"\n",
    "    from plotly.subplots import make_subplots\n",
    "\n",
    "    passenger_types = ['y_single', 'y_small', 'y_medium', 'y_large']\n",
    "    passenger_labels = ['Single', 'Small Groups', 'Medium Groups', 'Large Groups']\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=passenger_labels,\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.1\n",
    "    )\n",
    "\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "    for idx, (ptype, label) in enumerate(zip(passenger_types, passenger_labels)):\n",
    "        row = idx // 2 + 1\n",
    "        col = idx % 2 + 1\n",
    "\n",
    "        if model_type == \"both\":\n",
    "            for model in ['XGBoost', 'KNN']:\n",
    "                model_df = df[df['model'] == model]\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=model_df['datetime'],\n",
    "                        y=model_df[f'pred_{ptype}'],\n",
    "                        name=f'{model}',\n",
    "                        line=dict(width=2),\n",
    "                        showlegend=(idx == 0),  # Only show legend for first subplot\n",
    "                    ),\n",
    "                    row=row, col=col\n",
    "                )\n",
    "        else:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df['datetime'],\n",
    "                    y=df[f'pred_{ptype}'],\n",
    "                    name=label,\n",
    "                    line=dict(color=colors[idx], width=2),\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "\n",
    "            # Add confidence intervals\n",
    "            if f'pred_{ptype}_upper' in df.columns:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=df['datetime'],\n",
    "                        y=df[f'pred_{ptype}_upper'],\n",
    "                        fill=None,\n",
    "                        mode='lines',\n",
    "                        line_color='rgba(0,0,0,0)',\n",
    "                        showlegend=False,\n",
    "                        hoverinfo='skip'\n",
    "                    ),\n",
    "                    row=row, col=col\n",
    "                )\n",
    "\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=df['datetime'],\n",
    "                        y=df[f'pred_{ptype}_lower'],\n",
    "                        fill='tonexty',\n",
    "                        mode='lines',\n",
    "                        line_color='rgba(0,0,0,0)',\n",
    "                        showlegend=False,\n",
    "                        fillcolor=f'rgba{tuple(list(int(colors[idx].lstrip(\"#\")[i:i+2], 16) for i in (0, 2, 4)) + [0.2])}'\n",
    "                    ),\n",
    "                    row=row, col=col\n",
    "                )\n",
    "\n",
    "        # Add actuals if available\n",
    "        test_df = df[df['prediction_type'] == 'test']\n",
    "        if not test_df.empty and f'actual_{ptype}' in test_df.columns:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=test_df['datetime'],\n",
    "                    y=test_df[f'actual_{ptype}'],\n",
    "                    name='Actual' if idx == 0 else '',\n",
    "                    line=dict(color='black', width=1, dash='dot'),\n",
    "                    showlegend=(idx == 0),\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "\n",
    "    zone_text = f\"Zone {zone_id}\" if zone_id else \"All Zones\"\n",
    "    fig.update_layout(\n",
    "        title=f\"Predictions by Passenger Type - {zone_text}\",\n",
    "        height=800,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "def create_model_comparison_plot(df, zone_id):\n",
    "    \"\"\"Create plot comparing XGBoost vs KNN predictions\"\"\"\n",
    "    if 'XGBoost' not in df['model'].values or 'KNN' not in df['model'].values:\n",
    "        return create_empty_plot(\"Both models needed for comparison\")\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    xgb_df = df[df['model'] == 'XGBoost']\n",
    "    knn_df = df[df['model'] == 'KNN']\n",
    "\n",
    "    # Merge on datetime to ensure alignment\n",
    "    merged = pd.merge(\n",
    "        xgb_df[['datetime', 'pred_total']].rename(columns={'pred_total': 'xgb_total'}),\n",
    "        knn_df[['datetime', 'pred_total']].rename(columns={'pred_total': 'knn_total'}),\n",
    "        on='datetime'\n",
    "    )\n",
    "\n",
    "    # Add model predictions\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=merged['datetime'],\n",
    "        y=merged['xgb_total'],\n",
    "        name='XGBoost',\n",
    "        line=dict(color='#1f77b4', width=2),\n",
    "        mode='lines'\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=merged['datetime'],\n",
    "        y=merged['knn_total'],\n",
    "        name='KNN',\n",
    "        line=dict(color='#ff7f0e', width=2),\n",
    "        mode='lines'\n",
    "    ))\n",
    "\n",
    "\n",
    "\n",
    "    # Add actual values if present\n",
    "    test_df = df[df['prediction_type'] == 'test'].groupby('datetime').first()\n",
    "    if not test_df.empty and 'actual_y_single' in test_df.columns:\n",
    "        test_df['actual_total'] = test_df['actual_y_single'].fillna(0) + \\\n",
    "                                  test_df['actual_y_small'].fillna(0) + \\\n",
    "                                  test_df['actual_y_medium'].fillna(0) + \\\n",
    "                                  test_df['actual_y_large'].fillna(0)\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=test_df.index,\n",
    "            y=test_df['actual_total'],\n",
    "            name='Actual',\n",
    "            line=dict(color='#2ca02c', width=2, dash='dot'),\n",
    "            mode='lines'\n",
    "        ))\n",
    "\n",
    "    zone_text = f\"Zone {zone_id}\" if zone_id else \"All Zones\"\n",
    "    fig.update_layout(\n",
    "        title=f\"Model Comparison - {zone_text}\",\n",
    "        xaxis_title=\"Date/Time\",\n",
    "        yaxis_title=\"Total Passengers\",\n",
    "        # yaxis2=dict(\n",
    "        #     title=\"Difference\",\n",
    "        #     overlaying='y',\n",
    "        #     side='right'\n",
    "        # ),\n",
    "        hovermode='x unified',\n",
    "        height=600\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "def create_empty_plot(message):\n",
    "    \"\"\"Create an empty plot with a message\"\"\"\n",
    "    fig = go.Figure()\n",
    "    fig.add_annotation(\n",
    "        x=0.5, y=0.5,\n",
    "        text=message,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        showarrow=False,\n",
    "        font=dict(size=20)\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(visible=False),\n",
    "        yaxis=dict(visible=False),\n",
    "        height=400\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wb7favZWtoQE",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1757012279402,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "Wb7favZWtoQE"
   },
   "outputs": [],
   "source": [
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caJ1jMeGcON",
   "metadata": {
    "id": "9caJ1jMeGcON"
   },
   "source": [
    "## fleet detailed predictions table utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VHZ4rHc1-VKY",
   "metadata": {
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1757013139165,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "VHZ4rHc1-VKY"
   },
   "outputs": [],
   "source": [
    "def load_zone_predictions_table(taxi_type, zone_id, date, pred_type):\n",
    "    \"\"\"Load detailed predictions table for a specific zone and date\"\"\"\n",
    "\n",
    "    # Parse the date\n",
    "    try:\n",
    "        selected_date = pd.to_datetime(date).date()\n",
    "    except:\n",
    "        return pd.DataFrame({\"Error\": [\"Invalid date format. Please use YYYY-MM-DD\"]})\n",
    "\n",
    "    # Build query based on prediction type\n",
    "    pred_type_condition = f\"prediction_type = '{pred_type.lower()}'\"\n",
    "\n",
    "    # Build WHERE clause\n",
    "    where_conditions = [\n",
    "        f\"date = '{selected_date}'\",\n",
    "        pred_type_condition\n",
    "    ]\n",
    "\n",
    "    if zone_id is not None:\n",
    "        where_conditions.append(f\"zone_id = {zone_id}\")\n",
    "\n",
    "    where_clause = \" AND \".join(where_conditions)\n",
    "\n",
    "    # Query both models\n",
    "    query = f\"\"\"\n",
    "    WITH xgb_data AS (\n",
    "        SELECT\n",
    "            zone_id,\n",
    "            hr as hour,\n",
    "            xgb_pred_y_single as xgb_single,\n",
    "            xgb_pred_y_single_lower as xgb_single_lower,\n",
    "            xgb_pred_y_single_upper as xgb_single_upper,\n",
    "            xgb_pred_y_small as xgb_small,\n",
    "            xgb_pred_y_small_lower as xgb_small_lower,\n",
    "            xgb_pred_y_small_upper as xgb_small_upper,\n",
    "            xgb_pred_y_medium as xgb_medium,\n",
    "            xgb_pred_y_medium_lower as xgb_medium_lower,\n",
    "            xgb_pred_y_medium_upper as xgb_medium_upper,\n",
    "            xgb_pred_y_large as xgb_large,\n",
    "            xgb_pred_y_large_lower as xgb_large_lower,\n",
    "            xgb_pred_y_large_upper as xgb_large_upper,\n",
    "            actual_y_single,\n",
    "            actual_y_small,\n",
    "            actual_y_medium,\n",
    "            actual_y_large\n",
    "        FROM `{PROJECT_ID}.{OUTPUT_DATASET}.fleet_recommender_{taxi_type}_predictions_new_xgb`\n",
    "        WHERE {where_clause}\n",
    "    ),\n",
    "    knn_data AS (\n",
    "        SELECT\n",
    "            zone_id,\n",
    "            hr as hour,\n",
    "            knn_pred_y_single as knn_single,\n",
    "            knn_pred_y_single_lower as knn_single_lower,\n",
    "            knn_pred_y_single_upper as knn_single_upper,\n",
    "            knn_pred_y_small as knn_small,\n",
    "            knn_pred_y_small_lower as knn_small_lower,\n",
    "            knn_pred_y_small_upper as knn_small_upper,\n",
    "            knn_pred_y_medium as knn_medium,\n",
    "            knn_pred_y_medium_lower as knn_medium_lower,\n",
    "            knn_pred_y_medium_upper as knn_medium_upper,\n",
    "            knn_pred_y_large as knn_large,\n",
    "            knn_pred_y_large_lower as knn_large_lower,\n",
    "            knn_pred_y_large_upper as knn_large_upper\n",
    "        FROM `{PROJECT_ID}.{OUTPUT_DATASET}.fleet_recommender_{taxi_type}_predictions_new_knn`\n",
    "        WHERE {where_clause}\n",
    "    )\n",
    "    SELECT\n",
    "        COALESCE(x.zone_id, k.zone_id) as zone_id,\n",
    "        COALESCE(x.hour, k.hour) as hour,\n",
    "        -- XGBoost predictions with CIs\n",
    "        ROUND(x.xgb_single, 2) as XGB_Single,\n",
    "        CONCAT('[', ROUND(x.xgb_single_lower, 2), '-', ROUND(x.xgb_single_upper, 2), ']') as XGB_Single_CI,\n",
    "        ROUND(x.xgb_small, 2) as XGB_Small,\n",
    "        CONCAT('[', ROUND(x.xgb_small_lower, 2), '-', ROUND(x.xgb_small_upper, 2), ']') as XGB_Small_CI,\n",
    "        ROUND(x.xgb_medium, 2) as XGB_Medium,\n",
    "        CONCAT('[', ROUND(x.xgb_medium_lower, 2), '-', ROUND(x.xgb_medium_upper, 2), ']') as XGB_Medium_CI,\n",
    "        ROUND(x.xgb_large, 2) as XGB_Large,\n",
    "        CONCAT('[', ROUND(x.xgb_large_lower, 2), '-', ROUND(x.xgb_large_upper, 2), ']') as XGB_Large_CI,\n",
    "        -- KNN predictions with CIs\n",
    "        ROUND(k.knn_single, 2) as KNN_Single,\n",
    "        CONCAT('[', ROUND(k.knn_single_lower, 2), '-', ROUND(k.knn_single_upper, 2), ']') as KNN_Single_CI,\n",
    "        ROUND(k.knn_small, 2) as KNN_Small,\n",
    "        CONCAT('[', ROUND(k.knn_small_lower, 2), '-', ROUND(k.knn_small_upper, 2), ']') as KNN_Small_CI,\n",
    "        ROUND(k.knn_medium, 2) as KNN_Medium,\n",
    "        CONCAT('[', ROUND(k.knn_medium_lower, 2), '-', ROUND(k.knn_medium_upper, 2), ']') as KNN_Medium_CI,\n",
    "        ROUND(k.knn_large, 2) as KNN_Large,\n",
    "        CONCAT('[', ROUND(k.knn_large_lower, 2), '-', ROUND(k.knn_large_upper, 2), ']') as KNN_Large_CI,\n",
    "        -- Totals\n",
    "        ROUND(x.xgb_single + x.xgb_small + x.xgb_medium + x.xgb_large, 2) as XGB_Total,\n",
    "        ROUND(k.knn_single + k.knn_small + k.knn_medium + k.knn_large, 2) as KNN_Total\n",
    "        {', ROUND(x.actual_y_single, 2) as Actual_Single' if pred_type.lower() == 'test' else ''}\n",
    "        {', ROUND(x.actual_y_small, 2) as Actual_Small' if pred_type.lower() == 'test' else ''}\n",
    "        {', ROUND(x.actual_y_medium, 2) as Actual_Medium' if pred_type.lower() == 'test' else ''}\n",
    "        {', ROUND(x.actual_y_large, 2) as Actual_Large' if pred_type.lower() == 'test' else ''}\n",
    "        {', ROUND(x.actual_y_single + x.actual_y_small + x.actual_y_medium + x.actual_y_large, 2) as Actual_Total' if pred_type.lower() == 'test' else ''}\n",
    "    FROM xgb_data x\n",
    "    FULL OUTER JOIN knn_data k ON x.zone_id = k.zone_id AND x.hour = k.hour\n",
    "    ORDER BY zone_id, hour\n",
    "    \"\"\"\n",
    "\n",
    "    df = bq_client.query(query).to_dataframe()\n",
    "\n",
    "    if df.empty:\n",
    "        return pd.DataFrame({\"Info\": [f\"No {pred_type.lower()} predictions found for {date}\"]})\n",
    "\n",
    "    # Convert hour and zone_id to string type before adding summary row\n",
    "    df['hour'] = df['hour'].astype(str)\n",
    "    df['zone_id'] = df['zone_id'].astype(str)\n",
    "\n",
    "    # Create summary row\n",
    "    summary = {}\n",
    "\n",
    "    # Add numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        summary[col] = df[col].sum()\n",
    "\n",
    "    # Add non-numeric columns\n",
    "    summary['hour'] = 'TOTAL'\n",
    "    summary['zone_id'] = '-'\n",
    "\n",
    "    # Add CI columns\n",
    "    for col in df.columns:\n",
    "        if '_CI' in col and col not in summary:\n",
    "            summary[col] = '-'\n",
    "\n",
    "    # Append summary row\n",
    "    df = pd.concat([df, pd.DataFrame([summary])], ignore_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6E4PqT9aGiJo",
   "metadata": {
    "id": "6E4PqT9aGiJo"
   },
   "source": [
    "## plot_zone_performance_comparison and other utilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruLL7GNHFf-q",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1757012279402,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "ruLL7GNHFf-q"
   },
   "outputs": [],
   "source": [
    "def plot_zone_performance_comparison(taxi_type, perf_metric, perf_passenger_type):\n",
    "    \"\"\"Create bar chart comparing zone performance across models\"\"\"\n",
    "\n",
    "    # Map display metric names to column names\n",
    "    metric_map = {\n",
    "        'MAE': 'mae',\n",
    "        'RMSE': 'rmse'\n",
    "    }\n",
    "\n",
    "    metric_col = metric_map[perf_metric]\n",
    "\n",
    "    # Build query\n",
    "    if perf_passenger_type == \"All\":\n",
    "        # Average across all passenger types\n",
    "        query = f\"\"\"\n",
    "        WITH zone_metrics AS (\n",
    "            SELECT\n",
    "                zone_id,\n",
    "                model_type,\n",
    "                AVG({metric_col}) as avg_metric\n",
    "            FROM `{PROJECT_ID}.{OUTPUT_DATASET}.fleet_recommender_{taxi_type}_metrics_new`\n",
    "            GROUP BY zone_id, model_type\n",
    "        )\n",
    "        SELECT\n",
    "            zone_id,\n",
    "            MAX(CASE WHEN model_type = 'xgb' THEN avg_metric END) as xgb_metric,\n",
    "            MAX(CASE WHEN model_type = 'knn' THEN avg_metric END) as knn_metric\n",
    "        FROM zone_metrics\n",
    "        GROUP BY zone_id\n",
    "        ORDER BY zone_id\n",
    "        \"\"\"\n",
    "    else:\n",
    "        # Specific passenger type\n",
    "        query = f\"\"\"\n",
    "        SELECT\n",
    "            zone_id,\n",
    "            MAX(CASE WHEN model_type = 'xgb' THEN {metric_col} END) as xgb_metric,\n",
    "            MAX(CASE WHEN model_type = 'knn' THEN {metric_col} END) as knn_metric\n",
    "        FROM `{PROJECT_ID}.{OUTPUT_DATASET}.fleet_recommender_{taxi_type}_metrics_new`\n",
    "        WHERE target = '{perf_passenger_type}'\n",
    "        GROUP BY zone_id\n",
    "        ORDER BY zone_id\n",
    "        \"\"\"\n",
    "\n",
    "    df = bq_client.query(query).to_dataframe()\n",
    "\n",
    "    if df.empty:\n",
    "        return create_empty_plot(\"No performance metrics available\")\n",
    "\n",
    "    # Calculate which model is better for each zone\n",
    "    df['better_model'] = df.apply(lambda row: 'XGBoost' if row['xgb_metric'] < row['knn_metric'] else 'KNN', axis=1)\n",
    "    df['improvement'] = abs(df['xgb_metric'] - df['knn_metric'])\n",
    "\n",
    "    # Sort by the better metric value\n",
    "    df['best_metric'] = df[['xgb_metric', 'knn_metric']].min(axis=1)\n",
    "    df = df.sort_values('best_metric')\n",
    "\n",
    "    # Calculate average metrics\n",
    "    xgb_avg = df['xgb_metric'].mean()\n",
    "    knn_avg = df['knn_metric'].mean()\n",
    "    overall_avg = df[['xgb_metric', 'knn_metric']].mean().mean()\n",
    "\n",
    "    # Create grouped bar chart\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add XGBoost bars\n",
    "    fig.add_trace(go.Bar(\n",
    "        name='XGBoost',\n",
    "        x=df['zone_id'].astype(str),\n",
    "        y=df['xgb_metric'],\n",
    "        marker_color='#1f77b4',\n",
    "        text=df['xgb_metric'].round(2),\n",
    "        textposition='auto',\n",
    "    ))\n",
    "\n",
    "    # Add KNN bars\n",
    "    fig.add_trace(go.Bar(\n",
    "        name='KNN',\n",
    "        x=df['zone_id'].astype(str),\n",
    "        y=df['knn_metric'],\n",
    "        marker_color='#ff7f0e',\n",
    "        text=df['knn_metric'].round(2),\n",
    "        textposition='auto',\n",
    "    ))\n",
    "\n",
    "    # Add average lines\n",
    "    fig.add_hline(y=xgb_avg, line_dash=\"dash\", line_color=\"#1f77b4\",\n",
    "                  annotation_text=f\"XGBoost Avg: {xgb_avg:.2f}\",\n",
    "                  annotation_position=\"right\")\n",
    "\n",
    "    fig.add_hline(y=knn_avg, line_dash=\"dash\", line_color=\"#ff7f0e\",\n",
    "                  annotation_text=f\"KNN Avg: {knn_avg:.2f}\",\n",
    "                  annotation_position=\"right\")\n",
    "\n",
    "    # Update layout\n",
    "    passenger_label = \"All Types\" if perf_passenger_type == \"All\" else perf_passenger_type.replace('y_', '').title()\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f'{perf_metric} by Zone - {passenger_label} Passengers',\n",
    "        xaxis_title='Zone ID',\n",
    "        yaxis_title=perf_metric,\n",
    "        barmode='group',\n",
    "        height=600,\n",
    "        showlegend=True,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "\n",
    "    # Add annotations for best performing zones and average metrics\n",
    "    best_zones = df.nsmallest(3, 'best_metric')\n",
    "    annotation_text = f\"<b>Average {perf_metric}:</b><br>\"\n",
    "    annotation_text += f\"XGBoost: {xgb_avg:.3f}<br>\"\n",
    "    annotation_text += f\"KNN: {knn_avg:.3f}<br>\"\n",
    "    annotation_text += f\"Overall: {overall_avg:.3f}<br><br>\"\n",
    "    annotation_text += \"<b>Top 3 Zones:</b><br>\"\n",
    "    for _, row in best_zones.iterrows():\n",
    "        annotation_text += f\"Zone {row['zone_id']}: {row['better_model']} ({row['best_metric']:.2f})<br>\"\n",
    "\n",
    "    fig.add_annotation(\n",
    "        text=annotation_text,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        x=0.02, y=0.98,\n",
    "        showarrow=False,\n",
    "        bgcolor=\"white\",\n",
    "        bordercolor=\"gray\",\n",
    "        borderwidth=1,\n",
    "        align=\"left\"\n",
    "    )\n",
    "\n",
    "    # If there are many zones, only show every nth label\n",
    "    if len(df) > 20:\n",
    "        tickvals = list(range(0, len(df), len(df)//20))\n",
    "        ticktext = [df.iloc[i]['zone_id'] for i in tickvals]\n",
    "        fig.update_xaxes(tickvals=tickvals, ticktext=ticktext)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tFIidGpQUrSM",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1757012279402,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "tFIidGpQUrSM"
   },
   "outputs": [],
   "source": [
    "# First, add this function at the top level to load and cache geometry data\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_cached_zone_geometry():\n",
    "    \"\"\"Load and cache zone geometry data\"\"\"\n",
    "    import geopandas as gpd\n",
    "    from google.cloud import storage\n",
    "    import os\n",
    "    import tempfile\n",
    "    import shutil\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket_name = 'nyc_raw_data_bucket'\n",
    "\n",
    "    # Load the lookup table\n",
    "    blob = storage_client.bucket(bucket_name).blob('taxi_zone_lookup.csv')\n",
    "    content = blob.download_as_bytes()\n",
    "    taxi_zone_lookup = pd.read_csv(BytesIO(content))\n",
    "    taxi_zone_lookup['LocationID'] = taxi_zone_lookup['LocationID'].astype(int)\n",
    "\n",
    "    # Load the shapefile\n",
    "    shapefile_prefix = 'taxi_zones/taxi_zones'\n",
    "    required_extensions = ['.shp', '.shx', '.dbf', '.prj']\n",
    "\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    try:\n",
    "        for extension in required_extensions:\n",
    "            blob_name = shapefile_prefix + extension\n",
    "            blob = storage_client.bucket(bucket_name).blob(blob_name)\n",
    "            local_path = os.path.join(temp_dir, os.path.basename(blob_name))\n",
    "            blob.download_to_filename(local_path)\n",
    "\n",
    "        shp_path = os.path.join(temp_dir, 'taxi_zones.shp')\n",
    "        taxi_zones_gdf = gpd.read_file(shp_path)\n",
    "        taxi_zones_gdf_wgs84 = taxi_zones_gdf.to_crs(epsg=4326)\n",
    "\n",
    "        # Merge with lookup\n",
    "        merged_df = pd.merge(taxi_zone_lookup, taxi_zones_gdf_wgs84,\n",
    "                            left_on='LocationID', right_on='OBJECTID', how='inner')\n",
    "        merged_df_cleaned = merged_df.dropna(subset=['geometry'])\n",
    "        merged_gdf_cleaned = gpd.GeoDataFrame(merged_df_cleaned,\n",
    "                                              geometry='geometry',\n",
    "                                              crs='EPSG:4326')\n",
    "        merged_gdf_cleaned[\"LocationID\"] = merged_gdf_cleaned[\"OBJECTID\"]\n",
    "\n",
    "        return merged_gdf_cleaned\n",
    "\n",
    "    finally:\n",
    "        if os.path.exists(temp_dir):\n",
    "            shutil.rmtree(temp_dir)\n",
    "\n",
    "# The plotting function\n",
    "def plot_zone_demand_map(taxi_type, date_type, single_date, hour, start_date, end_date,\n",
    "                         passenger_type, model, pred_type):\n",
    "    \"\"\"Create geographic visualization of zone demand predictions\"\"\"\n",
    "    import plotly.express as px\n",
    "    import numpy as np\n",
    "\n",
    "    # Build query based on date type\n",
    "    if date_type == \"Single Date\":\n",
    "        date_condition = f\"date = '{single_date}' AND hr = {hour}\"\n",
    "        title_suffix = f\"on {single_date} at {hour}:00\"\n",
    "    else:\n",
    "        date_condition = f\"date >= '{start_date}' AND date <= '{end_date}'\"\n",
    "        title_suffix = f\"from {start_date} to {end_date}\"\n",
    "\n",
    "    # Add prediction type filter\n",
    "    if pred_type != \"All\":\n",
    "        date_condition += f\" AND prediction_type = '{pred_type.lower()}'\"\n",
    "\n",
    "    # Build the query\n",
    "    table_name = f\"fleet_recommender_{taxi_type}_predictions_new_{model}\"\n",
    "\n",
    "    if passenger_type == \"Total\":\n",
    "        metric_calc = f\"\"\"\n",
    "        SUM({model}_pred_y_single + {model}_pred_y_small +\n",
    "            {model}_pred_y_medium + {model}_pred_y_large) as total_pred_trips\n",
    "        \"\"\"\n",
    "    else:\n",
    "        metric_calc = f\"SUM({model}_pred_{passenger_type}) as total_pred_trips\"\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        zone_id,\n",
    "        {metric_calc},\n",
    "        COUNT(*) as num_predictions\n",
    "    FROM `{PROJECT_ID}.{OUTPUT_DATASET}.{table_name}`\n",
    "    WHERE {date_condition}\n",
    "    GROUP BY zone_id\n",
    "    ORDER BY zone_id\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute query\n",
    "    df = bq_client.query(query).to_dataframe()\n",
    "\n",
    "    if df.empty:\n",
    "        return create_empty_plot(\"No data available for selected parameters\")\n",
    "\n",
    "    try:\n",
    "        # Load cached geometry data\n",
    "        zone_gdf = get_cached_zone_geometry()\n",
    "\n",
    "        # Create a copy to avoid modifying cached data\n",
    "        zone_gdf = zone_gdf.copy()\n",
    "\n",
    "        # Merge prediction data with geometry\n",
    "        map_df = zone_gdf.merge(df, left_on='LocationID', right_on='zone_id', how='left')\n",
    "\n",
    "        # CRITICAL FIX: Handle all NA/NaN values before passing to Plotly\n",
    "        # Replace NA values in all columns\n",
    "        map_df = map_df.fillna({\n",
    "            'total_pred_trips': 0,\n",
    "            'zone_id': 0,\n",
    "            'num_predictions': 0\n",
    "        })\n",
    "\n",
    "        # For string columns, replace any remaining NA with empty string\n",
    "        for col in ['Zone', 'Borough', 'zone', 'borough']:\n",
    "            if col in map_df.columns:\n",
    "                map_df[col] = map_df[col].fillna('').astype(str)\n",
    "\n",
    "        # Ensure zone and borough columns exist\n",
    "        if 'zone' not in map_df.columns:\n",
    "            map_df['zone'] = map_df['Zone'] if 'Zone' in map_df.columns else ''\n",
    "        if 'borough' not in map_df.columns:\n",
    "            map_df['borough'] = map_df['Borough'] if 'Borough' in map_df.columns else ''\n",
    "\n",
    "        # Convert to proper types and handle any pandas NA\n",
    "        map_df['total_pred_trips'] = pd.to_numeric(map_df['total_pred_trips'], errors='coerce').fillna(0)\n",
    "        map_df['zone_id'] = pd.to_numeric(map_df['zone_id'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "        # Create the choropleth map\n",
    "        fig = px.choropleth_mapbox(\n",
    "            map_df,\n",
    "            geojson=map_df.geometry,\n",
    "            locations=map_df.index,\n",
    "            color='total_pred_trips',\n",
    "            color_continuous_scale='YlOrRd',\n",
    "            mapbox_style=\"carto-positron\",\n",
    "            center={\"lat\": 40.7128, \"lon\": -74.0060},\n",
    "            zoom=9,\n",
    "            opacity=0.7,\n",
    "            labels={'total_pred_trips': 'Predicted Trips'},\n",
    "            hover_data={\n",
    "                'zone_id': True,\n",
    "                'zone': True,\n",
    "                'borough': True,\n",
    "                'total_pred_trips': ':.1f'\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Update layout\n",
    "        passenger_label = \"Total\" if passenger_type == \"Total\" else passenger_type.replace('y_', '').title()\n",
    "        fig.update_layout(\n",
    "            title=f\"{passenger_label} Passenger Predictions - {model.upper()} Model<br>{title_suffix}\",\n",
    "            height=700,\n",
    "            margin={\"r\":0,\"t\":60,\"l\":0,\"b\":0}\n",
    "        )\n",
    "\n",
    "        # Add color bar formatting\n",
    "        fig.update_coloraxes(\n",
    "            colorbar_title_text=\"Predicted<br>Trips\",\n",
    "            colorbar_thickness=15\n",
    "        )\n",
    "\n",
    "        return fig\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating map: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        # Fallback to bar chart\n",
    "        return create_zone_demand_bar_chart(df, passenger_type, model, title_suffix)\n",
    "\n",
    "def create_zone_demand_bar_chart(df, passenger_type, model, title_suffix):\n",
    "    \"\"\"Fallback visualization using bar chart when map fails\"\"\"\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "    # Sort by demand and show top 30 zones\n",
    "    df_sorted = df.sort_values('total_pred_trips', ascending=False).head(30)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=df_sorted['zone_id'].astype(str),\n",
    "        y=df_sorted['total_pred_trips'],\n",
    "        marker_color='lightcoral',\n",
    "        text=df_sorted['total_pred_trips'].round(1),\n",
    "        textposition='auto',\n",
    "        hovertemplate='Zone %{x}<br>Predicted Trips: %{y:.1f}<extra></extra>'\n",
    "    ))\n",
    "\n",
    "    passenger_label = \"Total\" if passenger_type == \"Total\" else passenger_type.replace('y_', '').title()\n",
    "    fig.update_layout(\n",
    "        title=f\"Top 30 Zones by {passenger_label} Passenger Demand<br>{model.upper()} Model - {title_suffix}\",\n",
    "        xaxis_title=\"Zone ID\",\n",
    "        yaxis_title=\"Predicted Trips\",\n",
    "        height=600,\n",
    "        showlegend=False,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n0HoDyWl8myQ",
   "metadata": {
    "id": "n0HoDyWl8myQ"
   },
   "source": [
    "# Gradio Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GbBX9oFX9PqD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 685
    },
    "executionInfo": {
     "elapsed": 92138,
     "status": "ok",
     "timestamp": 1757013234001,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "GbBX9oFX9PqD",
    "outputId": "5ca64e80-6feb-40b6-8803-70c4389c01d1"
   },
   "outputs": [],
   "source": [
    "with gr.Blocks(title=\"NYC Taxi Dashboard\") as demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "    # NYC Taxi Interactive Dashboard\n",
    "\n",
    "    Analyze NYC taxi trip patterns, forecast demand, and detect anomalies using BigQuery data.\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Tab(\"Data Explorer\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                explore_taxi_type = gr.Dropdown(\n",
    "                    choices=TAXI_TYPES,\n",
    "                    label=\"Select Taxi Type\",\n",
    "                    value=None\n",
    "                )\n",
    "                explore_partitions = gr.CheckboxGroup(\n",
    "                    choices=[],\n",
    "                    label=\"Select Time Periods (YYYY_MM)\",\n",
    "                    value=[]\n",
    "                )\n",
    "\n",
    "            with gr.Column(scale=1):\n",
    "                metric = gr.Radio(\n",
    "                    [\"Trips\", \"Revenue\"],\n",
    "                    value=\"Trips\",\n",
    "                    label=\"Metric to Display\"\n",
    "                )\n",
    "                granularity = gr.Radio(\n",
    "                    [\"Daily\", \"Hourly\"],\n",
    "                    value=\"Daily\",\n",
    "                    label=\"Time Granularity\"\n",
    "                )\n",
    "\n",
    "        explore_plot = gr.Plot(label=\"Time Series Visualization\")\n",
    "\n",
    "        # Update partitions when taxi type changes\n",
    "        explore_taxi_type.change(\n",
    "            lambda x: gr.update(choices=get_available_partitions(x) if x else []),\n",
    "            inputs=[explore_taxi_type],\n",
    "            outputs=[explore_partitions]\n",
    "        )\n",
    "\n",
    "        # Update plot when any parameter changes\n",
    "        for component in [explore_taxi_type, explore_partitions, metric, granularity]:\n",
    "            component.change(\n",
    "                plot_time_series,\n",
    "                inputs=[explore_taxi_type, explore_partitions, metric, granularity],\n",
    "                outputs=[explore_plot]\n",
    "            )\n",
    "\n",
    "    with gr.Tab(\"ML & Forecasting\"):\n",
    "        gr.Markdown(\"\"\"\n",
    "        ### Load Data and Generate Forecasts\n",
    "        Select data range, load it, then generate forecasts with confidence intervals.\n",
    "        \"\"\")\n",
    "\n",
    "        # Data Loading Section\n",
    "        gr.Markdown(\"#### Step 1: Load Training Data\")\n",
    "        with gr.Row():\n",
    "            ml_taxi_type = gr.Dropdown(\n",
    "                choices=TAXI_TYPES,\n",
    "                label=\"Select Taxi Type\",\n",
    "                value=None\n",
    "            )\n",
    "            forecast_metric = gr.Radio(\n",
    "                [\"trips\", \"revenue\"],\n",
    "                value=\"trips\",\n",
    "                label=\"Metric to Forecast\"\n",
    "            )\n",
    "\n",
    "        with gr.Row():\n",
    "            start_partition = gr.Dropdown(\n",
    "                choices=[],\n",
    "                label=\"Start Period (YYYY_MM)\",\n",
    "                value=None\n",
    "            )\n",
    "            end_partition = gr.Dropdown(\n",
    "                choices=[],\n",
    "                label=\"End Period (YYYY_MM)\",\n",
    "                value=None\n",
    "            )\n",
    "\n",
    "        load_button = gr.Button(\"Load and Prepare Data\", variant=\"primary\")\n",
    "        load_status = gr.Textbox(label=\"Loading Status\", lines=3)\n",
    "\n",
    "        # Forecasting Section\n",
    "        gr.Markdown(\"#### Step 2: Configure and Generate Forecast\")\n",
    "        with gr.Row():\n",
    "            forecast_days = gr.Slider(\n",
    "                minimum=1,\n",
    "                maximum=14,\n",
    "                value=7,\n",
    "                step=1,\n",
    "                label=\"Days to Forecast\",\n",
    "                info=\"Number of days to predict into the future\"\n",
    "            )\n",
    "            confidence_level = gr.Radio(\n",
    "                choices=[0.95, 0.99],\n",
    "                value=0.95,\n",
    "                label=\"Confidence Level\",\n",
    "                info=\"Confidence interval coverage\"\n",
    "            )\n",
    "            show_actual_future = gr.Checkbox(\n",
    "                label=\"Show Actual Future Data (if available)\",\n",
    "                value=False\n",
    "            )\n",
    "\n",
    "        forecast_button = gr.Button(\"Generate Forecast\", variant=\"primary\")\n",
    "        forecast_plot = gr.Plot(label=\"Forecast Results with Confidence Intervals\")\n",
    "\n",
    "        # Event handlers\n",
    "        ml_taxi_type.change(\n",
    "            update_partition_choices,\n",
    "            inputs=[ml_taxi_type],\n",
    "            outputs=[start_partition, end_partition]\n",
    "        )\n",
    "\n",
    "        load_button.click(\n",
    "            lambda taxi, metric, start, end: load_and_prepare_data_with_metric(taxi, start, end, metric),\n",
    "            inputs=[ml_taxi_type, forecast_metric, start_partition, end_partition],\n",
    "            outputs=[load_status]\n",
    "        )\n",
    "\n",
    "        forecast_button.click(\n",
    "            lambda days, conf, actual, metric: train_combined_forecast_with_ci(days, conf, actual, metric),\n",
    "            inputs=[forecast_days, confidence_level, show_actual_future, forecast_metric],\n",
    "            outputs=[forecast_plot]\n",
    "        )\n",
    "\n",
    "    with gr.Tab(\"Anomaly Detection\"):\n",
    "        gr.Markdown(\"\"\"\n",
    "        ### Pre-computed Anomaly Detection Results\n",
    "        View anomalies detected by the pipeline, categorized by severity with holiday annotations.\n",
    "        \"\"\")\n",
    "\n",
    "        # Taxi type selector\n",
    "        anomaly_taxi_type = gr.Dropdown(\n",
    "            choices=TAXI_TYPES,\n",
    "            label=\"Select Taxi Type\",\n",
    "            value=None\n",
    "        )\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "                anomaly_plot = gr.Plot(label=\"Anomaly Detection Results\")\n",
    "            with gr.Column(scale=1):\n",
    "                anomaly_summary = gr.Textbox(\n",
    "                    label=\"Anomaly Summary\",\n",
    "                    lines=15,\n",
    "                    value=\"Select a taxi type to see summary\"\n",
    "                )\n",
    "        anomaly_button = gr.Button(\"Load Anomaly Results\", variant=\"primary\")\n",
    "\n",
    "        anomaly_button.click(\n",
    "            detect_anomalies_from_db,\n",
    "            inputs=[anomaly_taxi_type],\n",
    "            outputs=[anomaly_plot]\n",
    "        )\n",
    "\n",
    "        anomaly_button.click(\n",
    "            get_anomaly_summary,\n",
    "            inputs=[anomaly_taxi_type],\n",
    "            outputs=[anomaly_summary]\n",
    "        )\n",
    "\n",
    "    # Updated Fleet Recommender tab\n",
    "    with gr.Tab(\"Fleet Recommendations\"):\n",
    "        gr.Markdown(\"\"\"\n",
    "        ### Zone-Based Fleet Optimization Predictions\n",
    "        View passenger demand predictions by zone using XGBoost and KNN models.\n",
    "        Compare test performance with future predictions.\n",
    "        \"\"\")\n",
    "\n",
    "        with gr.Row():\n",
    "            fleet_taxi_type = gr.Dropdown(\n",
    "                choices=[\"yellow\", \"green\"],\n",
    "                label=\"Taxi Type\",\n",
    "                value=\"yellow\"\n",
    "            )\n",
    "            fleet_model_type = gr.Dropdown(\n",
    "                choices=[\"xgb\", \"knn\", \"both\"],\n",
    "                label=\"Model Type\",\n",
    "                value=\"both\"\n",
    "            )\n",
    "            fleet_date_range = gr.Textbox(\n",
    "                label=\"Available Date Range\",\n",
    "                value=\"Loading...\",\n",
    "                interactive=False\n",
    "            )\n",
    "\n",
    "        # Hidden components\n",
    "        fleet_min_date = gr.State(value=None)\n",
    "        fleet_max_date = gr.State(value=None)\n",
    "        zone_df_state = gr.State(value=pd.DataFrame())\n",
    "\n",
    "        # Model Performance Section\n",
    "        # with gr.Row():\n",
    "        #     with gr.Column():\n",
    "        #         gr.Markdown(\"### Model Performance Summary\")\n",
    "        #         metrics_display = gr.Textbox(\n",
    "        #             label=\"Zone-Level Performance Metrics (SMAPE)\",\n",
    "        #             lines=15,\n",
    "        #             value=\"Select taxi type to load metrics\"\n",
    "        #         )\n",
    "\n",
    "        with gr.Tabs():\n",
    "            with gr.Tab(\"Time Series View\"):\n",
    "                with gr.Row():\n",
    "                    zone_select = gr.Dropdown(\n",
    "                        choices=[\"All Zones\"],\n",
    "                        label=\"Zone ID (Ordered by Trip Volume)\",\n",
    "                        value=\"All Zones\"\n",
    "                    )\n",
    "                    ts_display_type = gr.Radio(\n",
    "                        choices=[ \"By Passenger Type\", \"Model Comparison\"],\n",
    "                        value=\"By Passenger Type\",\n",
    "                        label=\"Display Type\"\n",
    "                    )\n",
    "                    prediction_type_filter = gr.Radio(\n",
    "                        choices=[\"All\"], #  \"Test Only\", \"Future Only\"\n",
    "                        value=\"All\",\n",
    "                        label=\"Data (All : Test+Future)\"\n",
    "                    )\n",
    "\n",
    "                with gr.Row():\n",
    "                    ts_start_date = gr.Textbox(\n",
    "                        label=\"Start Date (YYYY-MM-DD)\",\n",
    "                        value=\"2025-06-24\"\n",
    "                    )\n",
    "                    ts_end_date = gr.Textbox(\n",
    "                        label=\"End Date (YYYY-MM-DD)\",\n",
    "                        value=\"2025-07-15\"\n",
    "                    )\n",
    "\n",
    "                fleet_ts_button = gr.Button(\"Generate Time Series\", variant=\"primary\")\n",
    "\n",
    "                with gr.Row():\n",
    "                    fleet_ts_plot = gr.Plot(label=\"Zone Predictions Time Series\")\n",
    "\n",
    "                # Updated plot function\n",
    "                def plot_zone_timeseries(taxi_type, model_type, start_date, end_date, zone_string, display_type, pred_type):\n",
    "                    zone_id = extract_zone_id(zone_string)\n",
    "                    return plot_fleet_timeseries_new(taxi_type, model_type, start_date, end_date, zone_id, display_type, pred_type)\n",
    "\n",
    "                fleet_ts_button.click(\n",
    "                    plot_zone_timeseries,\n",
    "                    inputs=[fleet_taxi_type, fleet_model_type, ts_start_date, ts_end_date,\n",
    "                          zone_select, ts_display_type, prediction_type_filter],\n",
    "                    outputs=[fleet_ts_plot]\n",
    "                )\n",
    "\n",
    "            with gr.Tab(\"Zone Performance Comparison\"):\n",
    "                with gr.Row():\n",
    "                    perf_metric = gr.Radio(\n",
    "                        choices=[ \"MAE\", \"RMSE\"],\n",
    "                        value=\"MAE\",\n",
    "                        label=\"Performance Metric\"\n",
    "                    )\n",
    "                    perf_passenger_type = gr.Dropdown(\n",
    "                        choices=[\"All\", \"y_single\", \"y_small\", \"y_medium\", \"y_large\"],\n",
    "                        value=\"All\",\n",
    "                        label=\"Passenger Type\"\n",
    "                    )\n",
    "\n",
    "                perf_button = gr.Button(\"Generate Performance Comparison\", variant=\"primary\")\n",
    "                perf_plot = gr.Plot(label=\"Zone Performance Comparison\")\n",
    "\n",
    "                perf_button.click(\n",
    "                    plot_zone_performance_comparison,\n",
    "                    inputs=[fleet_taxi_type, perf_metric, perf_passenger_type],\n",
    "                    outputs=[perf_plot]\n",
    "                )\n",
    "\n",
    "\n",
    "            with gr.Tab(\"Geographic View\"):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ### Geographic Distribution of Predicted Demand\n",
    "                Visualize passenger predictions by zone on an interactive map.\n",
    "                \"\"\")\n",
    "\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        geo_date_type = gr.Radio(\n",
    "                            choices=[\"Single Date\", \"Date Range\"],\n",
    "                            value=\"Single Date\",\n",
    "                            label=\"Date Selection Type\"\n",
    "                        )\n",
    "\n",
    "                        # Single date inputs\n",
    "                        geo_single_date = gr.Textbox(\n",
    "                            label=\"Select Date (YYYY-MM-DD)\",\n",
    "                            value=\"\",\n",
    "                            visible=True\n",
    "                        )\n",
    "                        geo_hour = gr.Slider(\n",
    "                            minimum=0,\n",
    "                            maximum=23,\n",
    "                            value=12,\n",
    "                            step=1,\n",
    "                            label=\"Hour of Day (for single date)\",\n",
    "                            visible=True\n",
    "                        )\n",
    "\n",
    "                        # Date range inputs\n",
    "                        geo_start_date = gr.Textbox(\n",
    "                            label=\"Start Date (YYYY-MM-DD)\",\n",
    "                            value=\"\",\n",
    "                            visible=False\n",
    "                        )\n",
    "                        geo_end_date = gr.Textbox(\n",
    "                            label=\"End Date (YYYY-MM-DD)\",\n",
    "                            value=\"\",\n",
    "                            visible=False\n",
    "                        )\n",
    "\n",
    "                    with gr.Column(scale=1):\n",
    "                        geo_passenger_type = gr.Dropdown(\n",
    "                            choices=[\"Total\", \"y_single\", \"y_small\", \"y_medium\", \"y_large\"],\n",
    "                            value=\"Total\",\n",
    "                            label=\"Passenger Type\"\n",
    "                        )\n",
    "                        geo_model_select = gr.Radio(\n",
    "                            choices=[\"xgb\", \"knn\"],\n",
    "                            value=\"xgb\",\n",
    "                            label=\"Model\"\n",
    "                        )\n",
    "                        geo_pred_type = gr.Radio(\n",
    "                            choices=[\"All\"],\n",
    "                            value=\"All\",\n",
    "                            label=\"Prediction Type\"\n",
    "                        )\n",
    "\n",
    "                geo_map_button = gr.Button(\"Generate Map\", variant=\"primary\")\n",
    "                geo_map = gr.Plot(label=\"Zone Demand Heatmap\")\n",
    "\n",
    "                # Toggle visibility based on date type selection\n",
    "                def toggle_date_inputs(date_type):\n",
    "                    if date_type == \"Single Date\":\n",
    "                        return (\n",
    "                            gr.update(visible=True),   # geo_single_date\n",
    "                            gr.update(visible=True),   # geo_hour\n",
    "                            gr.update(visible=False),  # geo_start_date\n",
    "                            gr.update(visible=False)   # geo_end_date\n",
    "                        )\n",
    "                    else:\n",
    "                        return (\n",
    "                            gr.update(visible=False),  # geo_single_date\n",
    "                            gr.update(visible=False),  # geo_hour\n",
    "                            gr.update(visible=True),   # geo_start_date\n",
    "                            gr.update(visible=True)    # geo_end_date\n",
    "                        )\n",
    "\n",
    "                geo_date_type.change(\n",
    "                    toggle_date_inputs,\n",
    "                    inputs=[geo_date_type],\n",
    "                    outputs=[geo_single_date, geo_hour, geo_start_date, geo_end_date]\n",
    "                )\n",
    "\n",
    "                # Plot function\n",
    "                geo_map_button.click(\n",
    "                    plot_zone_demand_map,\n",
    "                    inputs=[fleet_taxi_type, geo_date_type, geo_single_date, geo_hour,\n",
    "                            geo_start_date, geo_end_date, geo_passenger_type,\n",
    "                            geo_model_select, geo_pred_type],\n",
    "                    outputs=[geo_map]\n",
    "                )\n",
    "\n",
    "                # Auto-populate dates when taxi type changes\n",
    "                fleet_taxi_type.change(\n",
    "                    lambda max_d: max_d if max_d else \"\",\n",
    "                    inputs=[fleet_max_date],\n",
    "                    outputs=[geo_single_date]\n",
    "                )\n",
    "\n",
    "                fleet_taxi_type.change(\n",
    "                    lambda min_d, max_d: (min_d, max_d) if min_d and max_d else (\"\", \"\"),\n",
    "                    inputs=[fleet_min_date, fleet_max_date],\n",
    "                    outputs=[geo_start_date, geo_end_date]\n",
    "                )\n",
    "\n",
    "            with gr.Tab(\"Detailed Predictions Table\"):\n",
    "                with gr.Row():\n",
    "                    table_zone = gr.Dropdown(\n",
    "                        choices=[\"All Zones\"],\n",
    "                        label=\"Zone ID\",\n",
    "                        value=\"All Zones\"\n",
    "                    )\n",
    "                    table_date = gr.Textbox(\n",
    "                        label=\"Date (YYYY-MM-DD)\",\n",
    "                        value=\"\"\n",
    "                    )\n",
    "                    table_pred_type = gr.Radio(\n",
    "                        choices=[\"Test\", \"Future\"],\n",
    "                        value=\"Future\",\n",
    "                        label=\"Prediction Type\"\n",
    "                    )\n",
    "\n",
    "                table_button = gr.Button(\"Load Predictions Table\", variant=\"primary\")\n",
    "\n",
    "                with gr.Row():\n",
    "                    predictions_table = gr.Dataframe(\n",
    "                        label=\"Hourly Predictions with Confidence Intervals\"\n",
    "                    )\n",
    "\n",
    "                def load_predictions_table(taxi_type, zone_string, date, pred_type):\n",
    "                    zone_id = extract_zone_id(zone_string)\n",
    "                    return load_zone_predictions_table(taxi_type, zone_id, date, pred_type)\n",
    "\n",
    "                table_button.click(\n",
    "                    load_predictions_table,\n",
    "                    inputs=[fleet_taxi_type, table_zone, table_date, table_pred_type],\n",
    "                    outputs=[predictions_table]\n",
    "                )\n",
    "\n",
    "        # Update functions\n",
    "        def update_fleet_interface(taxi_type):\n",
    "            min_date, max_date, range_text = get_fleet_date_range_new(taxi_type)\n",
    "            zone_list = get_zones_with_predictions(taxi_type)\n",
    "            metrics_text = load_zone_metrics_summary(taxi_type)\n",
    "\n",
    "            return (\n",
    "                range_text,\n",
    "                min_date,\n",
    "                max_date,\n",
    "                gr.update(choices=zone_list, value=zone_list[0] if zone_list else \"All Zones\"),\n",
    "                metrics_text,\n",
    "                gr.update(choices=zone_list, value=zone_list[0] if zone_list else \"All Zones\"),\n",
    "                gr.update(choices=zone_list, value=zone_list[0] if zone_list else \"All Zones\"),\n",
    "                gr.update(choices=zone_list, value=zone_list[0] if zone_list else \"All Zones\"),\n",
    "            )\n",
    "\n",
    "        # Event handlers\n",
    "        metrics_display = gr.State(value = None)\n",
    "        fleet_taxi_type.change(\n",
    "            update_fleet_interface,\n",
    "            inputs=[fleet_taxi_type],\n",
    "            outputs=[fleet_date_range, fleet_min_date, fleet_max_date,\n",
    "                    zone_select, metrics_display, metrics_display, table_zone, zone_df_state]\n",
    "        )\n",
    "\n",
    "        # outputs=[fleet_date_range, fleet_min_date, fleet_max_date,\n",
    "        #             zone_select, metrics_display, test_zone, table_zone, zone_df_state]\n",
    "\n",
    "        # Auto-populate dates\n",
    "        def populate_dates(min_date, max_date):\n",
    "            if min_date and max_date:\n",
    "                # Show test period by default\n",
    "                test_start = pd.to_datetime(max_date) - pd.Timedelta(days=21)\n",
    "                test_end = max_date\n",
    "                return test_start.strftime('%Y-%m-%d'), test_end\n",
    "            return \"\", \"\"\n",
    "\n",
    "        fleet_taxi_type.change(\n",
    "            lambda min_d, max_d: populate_dates(min_d, max_d),\n",
    "            inputs=[fleet_min_date, fleet_max_date],\n",
    "            outputs=[ts_start_date, ts_end_date]\n",
    "        )\n",
    "\n",
    "        fleet_taxi_type.change(\n",
    "            lambda max_d: max_d if max_d else \"\",\n",
    "            inputs=[fleet_max_date],\n",
    "            outputs=[table_date]\n",
    "        )\n",
    "\n",
    "        # Add initial load event\n",
    "        demo.load(\n",
    "            update_fleet_interface,\n",
    "            inputs=[fleet_taxi_type],\n",
    "            outputs=[fleet_date_range, fleet_min_date, fleet_max_date,\n",
    "                    zone_select, metrics_display, metrics_display, table_zone, zone_df_state]\n",
    "        )\n",
    "\n",
    "        # Also trigger date population on initial load\n",
    "        demo.load(\n",
    "            lambda: populate_dates(*get_fleet_date_range_new(\"yellow\")[:2]),\n",
    "            inputs=[],\n",
    "            outputs=[ts_start_date, ts_end_date]\n",
    "        )\n",
    "\n",
    "        # Set initial table date\n",
    "        demo.load(\n",
    "            lambda: get_fleet_date_range_new(\"yellow\")[1],  # max_date\n",
    "            inputs=[],\n",
    "            outputs=[table_date]\n",
    "        )\n",
    "\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saEPGDiu-V2c",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1757012368865,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "saEPGDiu-V2c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "FinalGradioDashboard",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
