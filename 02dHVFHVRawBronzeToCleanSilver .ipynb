{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "executionInfo": {
     "elapsed": 111764,
     "status": "ok",
     "timestamp": 1756448970564,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "p2NN8wwF7YRa",
    "outputId": "6f038ca0-a520-4499-9311-3741a16df7a2"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import re\n",
    "import grpc\n",
    "from google.api_core.exceptions import GoogleAPICallError, RetryError, ServiceUnavailable\n",
    "from google.cloud.dataproc_spark_connect import DataprocSparkSession\n",
    "from pyspark.errors import PySparkValueError\n",
    "\n",
    "_QUOTA_PAT = re.compile(r\"insufficient.*cpus.*quota\", re.IGNORECASE)\n",
    "\n",
    "def _is_quota_error(exc: Exception) -> bool:\n",
    "    msg = str(exc) or \"\"\n",
    "    return bool(_QUOTA_PAT.search(msg))\n",
    "\n",
    "def get_spark_with_retry(\n",
    "    gentle_delay: float = 10.0,     # fixed delay for the first few tries\n",
    "    gentle_retries: int = 3,        # how many tries use gentle delay\n",
    "    max_backoff: float = 60.0,      # cap for exponential backoff\n",
    "    quota_wait: float = 90.0,       # wait longer when we hit a QUOTA error\n",
    "    jitter: bool = True,\n",
    "    verify_ready: bool = True,\n",
    "    max_attempts: int | None = None # None = infinite\n",
    "):\n",
    "    \"\"\"\n",
    "    Keep retrying until DataprocSparkSession is usable.\n",
    "    - QUOTA errors: wait `quota_wait` seconds and retry (lets other jobs finish).\n",
    "    - Other errors: gentle linear delay for first `gentle_retries`, then exponential backoff.\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    backoff = gentle_delay\n",
    "\n",
    "    while True:\n",
    "        attempt += 1\n",
    "        try:\n",
    "            spark = DataprocSparkSession.builder.getOrCreate()\n",
    "            # Let backend settle a moment before the test query\n",
    "            time.sleep(3)\n",
    "\n",
    "            if verify_ready:\n",
    "                _ = spark.range(1).count()\n",
    "\n",
    "            print(f\" Spark connected on attempt {attempt}\")\n",
    "            return spark\n",
    "\n",
    "        except (ServiceUnavailable, GoogleAPICallError, RetryError,\n",
    "                grpc.RpcError, PySparkValueError, RuntimeError) as e:\n",
    "            # QUOTA-specific path: someone else might be using CPUs. Wait longer, then retry.\n",
    "            if _is_quota_error(e):\n",
    "                wait = quota_wait + (random.uniform(0, quota_wait/3) if jitter else 0.0)\n",
    "                print(f\"⏳ Quota limited (likely other sessions using CPUs). \"\n",
    "                      f\"Sleeping {wait:.1f}s, then retrying…\\nDetails: {e}\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                # Non-quota transient errors\n",
    "                if attempt <= gentle_retries:\n",
    "                    wait = gentle_delay\n",
    "                else:\n",
    "                    # exponential backoff capped\n",
    "                    backoff = min(backoff * 2, max_backoff)\n",
    "                    wait = backoff\n",
    "                if jitter:\n",
    "                    wait += random.uniform(0, wait/2)\n",
    "                print(f\"  Attempt {attempt} failed: {e!r}\\n   Sleeping {wait:.1f}s before retry…\")\n",
    "                time.sleep(wait)\n",
    "\n",
    "            if max_attempts and attempt >= max_attempts:\n",
    "                raise RuntimeError(f\" Failed after {attempt} attempts\") from e\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\" Stopped by user.\")\n",
    "            raise\n",
    "\n",
    "# --- Usage ---\n",
    "spark = get_spark_with_retry()\n",
    "\n",
    "# Safe to use:\n",
    "df = spark.range(5)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4HjXvQFnRII"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StringType, IntegerType, DoubleType, TimestampType,\n",
    "    StructType, StructField\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZeiMTJxu65rx"
   },
   "source": [
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xzTJ9Aua63yu"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"nyctaxi-467111\"\n",
    "BUCKET_NAME = \"nyc_raw_data_bucket\"\n",
    "BRONZE_DATASET_NAME = \"RawBronze\"\n",
    "SILVER_DATASET_NAME = \"CleanSilver\"\n",
    "TABLE_PREFIX = \"fhvhv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxcNzFkv7O9s"
   },
   "source": [
    "# -----------------------------\n",
    "# SCHEMA\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKUtMkvW7Q-7"
   },
   "outputs": [],
   "source": [
    "hvfhv_schema = StructType([\n",
    "    StructField(\"hvfhs_license_num\", StringType(), True),\n",
    "    StructField(\"dispatching_base_num\", StringType(), True),\n",
    "    StructField(\"originating_base_num\", StringType(), True),\n",
    "\n",
    "    StructField(\"request_datetime\", TimestampType(), True),\n",
    "    StructField(\"on_scene_datetime\", TimestampType(), True),\n",
    "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "\n",
    "    StructField(\"PULocationID\", IntegerType(), True),\n",
    "    StructField(\"DOLocationID\", IntegerType(), True),\n",
    "\n",
    "    StructField(\"trip_miles\", DoubleType(), True),\n",
    "    StructField(\"trip_time\", IntegerType(), True),\n",
    "\n",
    "    StructField(\"base_passenger_fare\", DoubleType(), True),\n",
    "    StructField(\"tolls\", DoubleType(), True),\n",
    "    StructField(\"bcf\", DoubleType(), True),\n",
    "    StructField(\"sales_tax\", DoubleType(), True),\n",
    "    StructField(\"congestion_surcharge\", DoubleType(), True),\n",
    "    StructField(\"airport_fee\", DoubleType(), True),\n",
    "    StructField(\"tips\", DoubleType(), True),\n",
    "    StructField(\"driver_pay\", DoubleType(), True),\n",
    "\n",
    "    StructField(\"shared_request_flag\", StringType(), True),\n",
    "    StructField(\"shared_match_flag\", StringType(), True),\n",
    "    StructField(\"access_a_ride_flag\", StringType(), True),\n",
    "    StructField(\"wav_request_flag\", StringType(), True),\n",
    "    StructField(\"wav_match_flag\", StringType(), True),\n",
    "\n",
    "    StructField(\"cbd_congestion_fee\", DoubleType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCDBkdVj7RuW"
   },
   "source": [
    "# -----------------------------\n",
    "# LOGGER (single per run)\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZeDijwW87WJG"
   },
   "outputs": [],
   "source": [
    "\n",
    "run_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_filename = f\"02d{TABLE_PREFIX}_cleaning_{run_time}.log\"\n",
    "local_log_path = f\"/tmp/{log_filename}\"\n",
    "gcs_log_path = f\"logs/{log_filename}\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "fh = logging.FileHandler(local_log_path)\n",
    "fh.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "def log_and_upload(msg: str):\n",
    "    logger.info(msg)\n",
    "    try:\n",
    "        import subprocess\n",
    "        subprocess.run(\n",
    "            [\"gsutil\", \"cp\", local_log_path, f\"gs://{BUCKET_NAME}/{gcs_log_path}\"],\n",
    "            check=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\" Failed to upload log: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-714UlL7Zj7"
   },
   "source": [
    "# -----------------------------\n",
    "# HELPER: LIST TABLES\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 299,
     "status": "ok",
     "timestamp": 1756448993475,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "2BuoXZWn8YFB",
    "outputId": "f9c24462-b93d-4c85-8dd0-654a35d99636"
   },
   "outputs": [],
   "source": [
    "TABLE_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "executionInfo": {
     "elapsed": 39146,
     "status": "ok",
     "timestamp": 1756449033009,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "RqToZFhp7XOB",
    "outputId": "cc2b8ed7-f05d-4f29-b087-527553d408fa"
   },
   "outputs": [],
   "source": [
    "def list_tables(dataset):\n",
    "    query = f\"SELECT table_name FROM `{PROJECT_ID}.{dataset}.INFORMATION_SCHEMA.TABLES`\"\n",
    "    return [row.table_name for row in spark.read.format(\"bigquery\").load(query).collect()]\n",
    "\n",
    "bronze_tables = list_tables(BRONZE_DATASET_NAME)\n",
    "silver_tables = list_tables(SILVER_DATASET_NAME)\n",
    "\n",
    "bronze_months = {t.replace(f\"{TABLE_PREFIX}_\", \"\") for t in bronze_tables if t.startswith(TABLE_PREFIX)}\n",
    "silver_months = {t.replace(f\"{TABLE_PREFIX}_\", \"\") for t in silver_tables if t.startswith(TABLE_PREFIX)}\n",
    "\n",
    "missing_months = sorted(bronze_months - silver_months)\n",
    "log_and_upload(f\"Missing months to process: {missing_months}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 151,
     "status": "ok",
     "timestamp": 1756449041933,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "NkbfTp3G8v6D",
    "outputId": "7bbb8566-c9a2-4515-ea2a-0e33009d0373"
   },
   "outputs": [],
   "source": [
    "missing_months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xT8qWt-Z7nOh"
   },
   "source": [
    "# -----------------------------\n",
    "# CLEANING LOOP\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 332146,
     "status": "ok",
     "timestamp": 1756328015932,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "VSQoFgYZ67Eb",
    "outputId": "8330931d-77ff-4263-b76e-15024dbb3ed5"
   },
   "outputs": [],
   "source": [
    "for year_month in missing_months:\n",
    "    year, month = map(int, year_month.split(\"_\"))\n",
    "    table_name = f\"{TABLE_PREFIX}_{year_month.replace('-', '_')}\"\n",
    "\n",
    "    try:\n",
    "        log_and_upload(f\" Starting processing for {year_month}\")\n",
    "\n",
    "        # --- Load from Raw Bronze ---\n",
    "        df = spark.read.format(\"bigquery\").option(\n",
    "            \"table\", f\"{PROJECT_ID}.{BRONZE_DATASET_NAME}.{table_name}\"\n",
    "        ).load()\n",
    "        log_and_upload(f\"Loaded {df.count()} rows from {BRONZE_DATASET_NAME}.{table_name}\")\n",
    "\n",
    "        # --- Cleaning Rules ---\n",
    "        before = df.count()\n",
    "        df = df.filter(F.col(\"pickup_datetime\").isNotNull() & F.col(\"dropoff_datetime\").isNotNull())\n",
    "        log_and_upload(f\"Dropped {before - df.count()} rows with null pickup/dropoff\")\n",
    "\n",
    "        before = df.count()\n",
    "        df = df.filter(F.col(\"pickup_datetime\") < F.col(\"dropoff_datetime\"))\n",
    "        log_and_upload(f\"Dropped {before - df.count()} rows with invalid pickup/dropoff order\")\n",
    "\n",
    "        for loc_col in [\"PULocationID\", \"DOLocationID\"]:\n",
    "            if loc_col in df.columns:\n",
    "                before = df.count()\n",
    "                df = df.filter(F.col(loc_col).isNotNull() & (F.col(loc_col) >= 0))\n",
    "                log_and_upload(f\"Dropped {before - df.count()} rows with invalid {loc_col}\")\n",
    "\n",
    "        # trip_time sanity check (hvfhv provides directly)\n",
    "        before = df.count()\n",
    "        df = df.filter((F.col(\"trip_time\") >= 60) & (F.col(\"trip_time\") <= 86400))\n",
    "        log_and_upload(f\"Dropped {before - df.count()} rows with invalid trip_time\")\n",
    "\n",
    "        # trip_miles > 0\n",
    "        before = df.count()\n",
    "        df = df.filter(F.col(\"trip_miles\") > 0)\n",
    "        log_and_upload(f\"Dropped {before - df.count()} rows with invalid trip_miles\")\n",
    "\n",
    "        # Flags normalization: force to Y/N\n",
    "        flag_cols = [\"shared_request_flag\", \"shared_match_flag\",\n",
    "                     \"access_a_ride_flag\", \"wav_request_flag\", \"wav_match_flag\"]\n",
    "        for col in flag_cols:\n",
    "            if col in df.columns:\n",
    "                df = df.withColumn(col, F.when(F.col(col).isin(\"Y\", \"N\"), F.col(col)).otherwise(None))\n",
    "\n",
    "        # Filter to target year/month\n",
    "        df = df.withColumn(\"year\", F.year(\"pickup_datetime\")).withColumn(\"pickup_month\", F.month(\"pickup_datetime\"))\n",
    "        before = df.count()\n",
    "        df = df.filter((F.col(\"year\") == year) & (F.col(\"pickup_month\") == month))\n",
    "        log_and_upload(f\"Filtered to {year_month}, remaining {df.count()} rows\")\n",
    "\n",
    "        # --- Write to Clean Silver ---\n",
    "        df.write.format(\"bigquery\").option(\n",
    "            \"table\", f\"{PROJECT_ID}.{SILVER_DATASET_NAME}.{table_name}\"\n",
    "        ).mode(\"overwrite\").save()\n",
    "        log_and_upload(f\" Written to {SILVER_DATASET_NAME}.{table_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log_and_upload(f\" ERROR processing {year_month}: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2rwvsrw8rzC"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Stop the Spark session gracefully\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\" Spark session stopped\")\n",
    "except Exception as e:\n",
    "    print(f\" Error while stopping Spark session: {e}\")\n",
    "\n",
    "# Sleep for 60 seconds to allow quota/resources to free up\n",
    "print(\" Waiting 60s for resources to be released...\")\n",
    "time.sleep(60)\n",
    "print(\" Done waiting. You can safely start a new session now.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "cell_execution_strategy": "setup",
   "name": "02dHVFHVRawBronzeToCleanSilver",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
